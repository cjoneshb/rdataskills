---
title: "First steps"
description: |
  Data types
toc: TRUE
toc_float: TRUE
site: distill::distill_website
---

```{r, echo = FALSE, eval = TRUE}
library(tidyverse)
```

# Vectors

In R the name for a variable that can store multiple values is a vector. So let’s create one.

## Character vectors

Pretend we’re designing a survey, and we want to keep track of the responses that a participant has given. This time, let’s imagine that we’ve finished running the survey and we’re examining the data. Suppose we’ve administered the Depression, Anxiety and Stress Scale (DASS) and as a consequence every participant has scores for on the depression, anxiety and stress scales provided by the DASS. One thing we might want to do is create a single variable called scale_name that identifies the three scales. The simplest way to do this in R is to use the combine function, c.16 To do so, all we have to do is type the values we want to store in a comma separated list, like this

```{r, echo = TRUE, eval = TRUE}
scale_name <- c("depression","anxpiety","stress")
scale_name
```

To use the correct terminology here, we have a single variable here called `scale_name`: this variable is a vector that has three elements. Because the vector contains text, it is a character vector. You can use the length function to check the `length`, and the `class` function to check what kind of vector it is:

```{r, echo = TRUE, eval = TRUE}
length(scale_name)
```
  
```{r, echo = TRUE, eval = TRUE}
class(scale_name)
```

## Numeric vectors

As you might expect, we can define numeric or logical variables in the same way. For instance, we could define the raw scores on the three DASS scales like so:

```{r, echo = TRUE, eval = TRUE}
raw_score <- c(12, 3, 8)
raw_score
```

We’ll talk about logical vectors in a bit.

## Extracting an element

If I want to extract the first element from the vector, all I have to do is refer to the relevant numerical index, using square brackets to do so. For example, to get the first element of scale_name I would type this

```{r, echo = TRUE, eval = TRUE}
scale_name[1]
```

The second element of the vector is

```{r, echo = TRUE, eval = TRUE}
scale_name[2]
```

You get the idea...

## Extracting multiple elements

There are a few ways to extract multiple elements of a vector. The first way is to specify a vector that contains the indices of the variables that you want to keep. To extract the first two scale names:

```{r, echo = TRUE, eval = TRUE}
scale_name[c(1,2)]
```

Alternatively, R provides a convenient shorthand notation in which 1:2 is a vector containing the numbers from 1 to 2, and similarly 1:10 is a vector containing the numbers from 1 to 10. So this is also the same:

```{r, echo = TRUE, eval = TRUE}
scale_name[1:2]
```

Notice that order matters here. So if I do this

```{r, echo = TRUE, eval = TRUE}
scale_name[c(2,1)]
```

I get the same numbers, but in the reverse order.

## Removing elements

Finally, when working with vectors, R allows us to use negative numbers to indicate which elements to remove. So this is yet another way of doing the same thing:

```{r, echo = TRUE, eval = TRUE}
scale_name[-3]
```

Notice that done of this has changed the original variable. The scale_name itself has remained completely untouched.

```{r, echo = TRUE, eval = TRUE}
scale_name
```

## Editing vectors

Sometimes you’ll want to change the values stored in a vector. Imagine my surprise if someone pointed out to me that "anxiety" is not in fact a real thing. I should probably fix that! 
One possibility would be to assign the whole vector again from the beginning, using c. But that’s a lot of typing. Also, it’s a little wasteful: why should R have to redefine the names for all three scales, when only the second one is wrong? Fortunately, we can tell R to change only the second element, using this trick:

```{r, echo = TRUE, eval = TRUE}
scale_name[2] <- "anxiety"
scale_name
```

That’s better.

## Naming elements

One very handy thing in R is that it lets you assign meaningful names to the different elements in a vector. For example, the `raw_scores` vector that we introduced earlier contains the actual data from a study but when you print it out on its own

```{r, echo = TRUE, eval = TRUE}
raw_score
```

its not obvious what each of the scores corresponds to. There are several different ways of making this a little more meaningful (and we’ll talk about them later) but for now I want to show one simple trick. Ideally, what we’d like to do is have R remember that the first element of the raw_score is the “depression” score, the second is “anxiety” and the third is “stress”. We can do that like this:

```{r, echo = TRUE, eval = TRUE}
names(raw_score) <- scale_name
```

This is a bit of an unusual looking assignment statement. Usually, whenever we use `<-` the thing on the left hand side is the variable itself (i.e., `raw_score`) but this time around the left hand side refers to the names. To see what this command has done, let’s get R to print out the `raw_score` variable now:

```{r, echo = TRUE, eval = TRUE}
raw_score
```

That’s a little nicer. Element names don’t just look nice, they’re functional too. You can refer to the elements of a vector using their names, like so:

```{r, echo = TRUE, eval = TRUE}
raw_score["anxiety"]
```

## Vector operations

One really nice thing about vectors is that a lot of R functions and operators will work on the whole vector at once. For instance, suppose I want to normalize the raw scores from the DASS. Each scale of the DASS is constructed from 14 questions that are rated on a 0-3 scale, so the minimum possible score is 0 and the maximum is 42. Suppose I wanted to re-scale the raw scores to lie on a scale from 0 to 1. I can create the `scaled_score` variable like this:

```{r, echo = TRUE, eval = TRUE}
scaled_score <- raw_score / 42
scaled_score
```

In other words, when you divide a vector by a single number, all elements in the vector get divided. The same is true for addition, subtraction, multiplication and taking powers. So that’s neat.

Suppose it later turned out that I’d made a mistake. I hadn’t in fact administered the complete DASS, only the first page. As noted in the DASS website, it’s possible to fix this mistake (sort of). First, I have to recognize that my scores are actually out of 21 not 42, so the calculation I should have done is this:

```{r, echo = TRUE, eval = TRUE}
scaled_score <- raw_score / 21
scaled_score
```

Then, it turns out that page 1 of the full DASS is almost the same as the short form of the DASS, but there’s a correction factor you have to apply. The depression score needs to be multiplied by 1.04645, the anxiety score by 1.02284, and stress by 0.98617

```{r, echo = TRUE, eval = TRUE}
correction_factor <- c(1.04645, 1.02284, 0.98617)
corrected_score <- scaled_score * correction_factor
corrected_score
```

What this has done is multiply the first element of `scaled_score` by the first element of `correction_factor`, multiply the second element of scaled_score by the second element of `correction_factor`, and so on.

I’ll talk more about calculations involving vectors later, because they come up a lot. In particular R has a thing called the recycling rule that is worth knowing about. But that’s enough detail for now.

## Logical vectors

I mentioned earlier that we can define vectors of logical values in the same way that we can store vectors of numbers and vectors of text, again using the c function to combine multiple values. Logical vectors can be useful as data in their own right, but the thing that they’re especially useful for is extracting elements of another vector, which is referred to as logical indexing.

Here’s a simple example. Suppose I decide that the stress scale is not very useful for my study, and I only want to keep the first two elements, depression and anxiety. One way to do this is to define a logical vector that indicates which values to keep:

```{r, echo = TRUE, eval = TRUE}
keep <- c(TRUE, TRUE, FALSE) 
keep
```

In this instance the keep vector indicates that it is TRUE that I want to retain the first two elements, and FALSE that I want to keep the third. So if I type this

```{r, echo = TRUE, eval = TRUE}
corrected_score[keep]
```

R prints out the corrected scores for the two variables only. As usual, note that this hasn’t changed the original variable. If I print out the original vector…

```{r, echo = TRUE, eval = TRUE}
corrected_score
```

… all three values are still there. If I do want to create a new variable, I need to explicitly assign the results of my previous command to a variable.

Let’s suppose that I want to call the new variable `short_score`, indicating that I’ve only retained some of the scales. Here’s how I do that:

```{r, echo = TRUE, eval = TRUE}
short_score <- corrected_score[keep]
short_score
```

## Comment

At this point, I hope you can see why logical indexing is such a useful thing. It’s a very basic, yet very powerful way to manipulate data. For instance, I might want to extract the scores of the adult participants in a study, which would probably involve a command like `scores[age > 18]`. The operation `age > 18` would return a vector of `TRUE` and `FALSE` values, and so the the full command `scores[age > 18]` would return only the scores for participants with `age > 18`. It does take practice to become completely comfortable using logical indexing, so it’s a good idea to play around with these sorts of commands. Practice makes perfect, and it’s only by practicing logical indexing that you’ll perfect the art of cursing your computer.

# Factors

Research methodology classes are at pains to point out: data we analyze come in different kinds! Some variables are inherently quantitative in nature: response time (RT) for instance, has a natural interpretation in units of time. So when I define a response time variable, I use a numeric vector. To keep my variable names concise, I’ll define the same variable again using the conventional RT abbreviation:

```{r, echo = TRUE, eval = TRUE}
RT <- c(420, 619, 550, 521, 1003, 486, 512, 560, 495, 610)
RT
```

A response time of 1500 milliseconds is indeed 400 milliseconds slower than a response time of 1100 milliseconds, so addition and subtraction are meaningful operations. Similarly, 1500 milliseconds is twice as long as 750 milliseconds, so multiplication and division are also meaningful. That’s not the case for other kinds of data, and this is where factors can be useful…

## Unordered factors

Some variables are inherently nominal in nature. If I recruit participants in an online experiment I might see that their place of residence falls in one of several different regions. For simplicity, let’s imagine we ran a study on MTurk designed to sample people from one of four distinct geographical regions: the United States, India, China or the European Union, which I’ll represent using the codes "us", "in", "ch" and "eu". My first thought would be to represent the data using a character vector:

```{r, echo = TRUE, eval = TRUE}
region_raw <- c("us","us","us","eu","in","eu","in","in","us","in")
```

This seems quite reasonable, but there’s a problem: as it happens there is nobody from China in this sample. So if I try to construct a frequency table of these data – which I can do using the table() function in R – the answer I get omits China entirely:

```{r, echo = TRUE, eval = TRUE}
table(region_raw)
```

Intuitively it feels like there should be a fourth entry here, indicating that we have 0 participants from China. R has a natural tool for representing this idea, called a factor. First, we’ll create a new variable using the `factor()` function that contains the same information but represents it as a factor:

```{r, echo = TRUE, eval = TRUE}
region <- factor(region_raw)
region
```

This looks much the same, and not surprisingly R still doesn’t know anything about the possibility of participants from China. However, notice that the bottom of the output lists the levels of the factor. The levels of a factor specify the set of values that variable could have taken. By default, `factor()` tries to guess the levels using the raw data, but we can override that manually, like this:

```{r, echo = TRUE, eval = TRUE}
region <- factor(region_raw, levels = c("ch","eu","in","us"))
region
```

Now when we tabulate the region variable, we obtain the right answer:

```{r, echo = TRUE, eval = TRUE}
table(region)
```

Much better.

## Ordered factors

There are two different types of factor in R. Until now we have been discussing un-ordered factors, in which the categories are purely nominal and there is no notion that the categories are arranged in any particular order. However, many psychologically important variables are inherently ordinal. Questionnaire responses often take this form, where participants might be asked to endorse a proposition using verbal categories such as “strongly agree”, “agree”, “neutral”, “disagree” and “strongly disagree”. The five response categories can’t be given any sensible numerical values but they can be ordered in a sensible fashion. In this situation we may want to represent the responses as an ordered factor.

To give you a sense of how these work in R, suppose we’ve been unfortunate enough to be given a data set that encodes ordinal responses numerically. Let’s suppose the original survey asked people how strongly they supported a politicial policy. Here we have a variable consisting of Likert-scale data, where (let’s suppose) in the original questionnaire 1 = “strongly agree” and 7 = “strongly disagree”,

```{r, echo = TRUE, eval = TRUE}
support_raw <- c(1, 7, 3, 4, 4, 4, 2, 6, 5, 5)
```

We can convert this to an ordered factor by specifying `ordered = TRUE` when we call the `factor()` function, like so:

```{r, echo = TRUE, eval = TRUE}
support <- factor( 
  x = support_raw,            # the raw data
  levels = c(7,6,5,4,3,2,1),  # strongest agreement is 1, weakest is 7
  ordered = TRUE              # and it’s ordered
)
support
```

Notice that when we print out the ordered factor, R explicitly tells us what order the levels come in.

Because I wanted to order my levels in terms of increasing strength of endorsement, and because a response of 1 corresponded to the strongest agreement and 7 to the strongest disagreement, it was important that I tell R to encode 7 as the lowest value and 1 as the largest. Always check this when creating an ordered factor: it’s very easy to accidentally encode your data with the levels reversed if you’re not paying attention. In any case, note that we can (and should) attach meaningful names to these factor levels by using the levels function, like this:

```{r, echo = TRUE, eval = TRUE}
levels(support) <- c( 
  "strong disagree", "disagree", "weak disagree",
  "neutral", "weak agree", "agree", "strong agree" 
)
support
```

A nice thing about ordered factors is that some analyses in R automatically treat ordered factors differently to un-ordered factors, and generally in a way that is more appropriate for ordinal data.

# Data frames / tibbles

We now have three variables that we might plausibly have encountered as the result of some study, region, support and RT.20 At the moment, R has no understanding of how these variables are related to each other. Quite likely they’re ordered the same way, so that the data stored in `region[1]`, `support[1]` and `RT[1]` all come from the same person. That would be sensible, but R is a robot and does not possess common sense. To help a poor little robot out (and to make our own lives easier), it’s nice to organize these three variable into a tabular format. This is where data frames – and the tidyverse analog tibbles – are very useful.

## Making a data frame

So how do we create a data frame (or tibble)? One way: if we import our data from a CSV file, R will create one for you. A second method is to create a data frame directly from some existing variables using the `data.frame` function. In real world data analysis this method is less common, but it’s very helpful for understanding what a data frame actually is, so that’s what we’ll do in this section.

Manually constructing a data frame is simple. All you have to do when calling `data.frame` is type a list of variables that you want to include in the data frame. If I want to store the variables from my experiment in a data frame called dat I can do so like this:

```{r, echo = TRUE, eval = TRUE}
dat <- data.frame(region, support, RT)
dat
```

Note that `dat` is a self-contained variable. Once created, it no longer depends on the variables from which it was constructed. If we make changes to the original `RT` variable, these will not influence the copy in `dat` (or vice versa). So for the sake of my sanity I’m going to remove all the originals:

```{r, echo = TRUE, eval = TRUE}
rm(region_raw, region, support_raw, support, RT)  
```

As you can see, our workspace has only a single variable, a data frame called `dat`. 

In this example I constructed the data frame manually so that you can see how a data frame is built from a set of variables, but in most real life situations you’d probably load your data frame directly from a CSV file or similar.

## Making a tibble

Constructing a tibble from raw variables is essentially the same as constructing a data frame, and the function we use to do this is `tibble`. If I hadn’t deleted all the raw variables in the previous section, this command would work:

```{r, echo = TRUE, eval = FALSE}
tib <- tibble(region, support, RT)
```

Alas they are gone, and I will have to try a different method. Fortunately, I can `coerce` my existing data frame `dat` into a `tibble` using the `as_tibble()` function, and use it to create a tibble called `tib`.

```{r, echo = TRUE, eval = TRUE}
tib <- as_tibble(dat)
tib
```

`Coercion` is an important R concept, and one that we’ll talk about again. In the meantime, there are some nice things to note about the output when we print `tib`. It states that the variable is a `tibble` with 10 rows and 3 columns. Underneath the variable names it tells you what type of data they store: region is a `factor` (<fct>), support is an `ordered factor` (<ord>) and RT is `numeric` (<dbl>, short for “double”)21.

## Tibbles are data frames

Under the hood, tibbles are essentially the same thing as data frames and are designed to behave the same way. In fact, if we use the `class()` function to see what R thinks `tib` really is…

```{r, echo = TRUE, eval = TRUE}
class(tib)
```

… it agrees that in addition to being a tibble, tib is also a data frame! We can check this more directly using the `is.data.frame()` function:

```{r, echo = TRUE, eval = TRUE}
is.data.frame(tib)
```

That being said, there are one or two differences between tibbles and pure data frames. For the most part, my impression has been that whenever they differ, the behaviour of tibbles tends to be more intuitive. With this in mind, although I’ll tend to use the terms “data frame” and “tibble” interchangeably, for the rest of these notes I’m going to work with tibbles like `tib` rather than pure data frames like `dat`.

## Using the $ operator

At this point our workspace contains a data frame called `dat`, a tibble called `tib`, but no longer contains the original variables. That’s okay because the tibble (data frame) is acting as a container that keeps them in a nice tidy rectangular shape. Conceptually this is very nice, but now we have a practical question … how do we get information out again? There are two qualitatively different ways to do this, reflecting two different ways to think about your data:

* Your data set is a list of variables (…use `$`)

* Your data set is a table of values (…use `[ ]`)

Both perspectives are valid, and R allows you to work with your data both ways.

To start with, let’s think of `tib` as a list of variables. This was the perspective we took when constructing `dat` in the first place: we took three different vectors (`region`, `support`, `RT`) and bound them together into a data frame, which we later coerced into the tibble `tib`. From this perspective, what we want is an *operator* that will extract one of those variables for us. This is the role played by `$`. If I want to refer to the region variable contained within the tib tibble, I would use this command:

```{r, echo = TRUE, eval = TRUE}
tib$region
```

As you can see, the output looks exactly the same as it did for the original variable: `tib$region` is a vector (an un-ordered factor in this case), and we can refer to an element of that vector in the same way we normally would:

```{r, echo = TRUE, eval = TRUE}
tib$region[1]
```

Conceptually, the metaphor here is `dataset$variable[value]`. The table below illustrates this by showing what type of output you get with different commands:

data frame command |	data frame output |	tibble command |	tibble output
-------------------|--------------------|----------------|---------------
dat                |	data frame        |	tib            |	tibble
dat`$`RT           |	vector            |	tib`$`RT       |	vector
dat`$`RT[1]        |	element           |	tib`$`RT[1]    |	element

As you can see, the `$` operator works the same way for pure data frames as for tibbles. This is not quite the case for when using square brackets [ ], as the next section demonstrates…

## Using square brackets

The second way to think about a tibble is to treat it as a fancy table. There is something appealing about this, because it emphasizes the fact that the data set has a case by variable structure:

```{r, echo = TRUE, eval = TRUE}
tib
```

In this structure each row is a person, and each column is a variable. The square bracket notation allows you to refer to entries in the data set by their row and column number (or name). As such, the reference looks like this:

```{r, echo = TRUE, eval = FALSE}
dataset[row,column]
```

R allows you to select multiple rows and columns. For instance if you set row to be 1:3 then R will return the first three cases. Here is an example where we select the first three rows and the first two columns:

```{r, echo = TRUE, eval = TRUE}
tib[1:3, 1:2]
```

If we omit values for the rows (or columns) while keeping the comma then R will assume you want all rows (or columns). So this returns every row in `tib` but only the first two columns:

```{r, echo = TRUE, eval = TRUE}
tib[, 1:2]
```

An important thing to recognize here is that – for tibbles – the metaphor underpinning the square bracket system is that your data have a rectangular shape that is imposed by the fact that your variable is a tibble, and no matter what you do with the square brackets the result will always remain a tibble. If I select just one row…

```{r, echo = TRUE, eval = TRUE}
tib[5,]
```

```{r, echo = TRUE, eval = TRUE}
tib[,3]
```

the result is a tibble. Even if I select a single value…

```{r, echo = TRUE, eval = TRUE}
tib[5,3]
```

the result is a tibble. For the square bracket system the rule is very simple: *tibbles stay tibbles*

Annoyingly, this is not the case for a pure data frame like `dat`. For a pure data frame, any time it is possible for R to treat the result as something else, it does: if I were to use the same commands for the data frame `dat`, the results would be different in some cases. This has caused no end of frustration over the years because everyone forgets about this particular property of data frames and stuff breaks. For what it’s worth, if you are working with pure data frames, here’s a summary of what to expect:

data frame command	| data frame output |	tibble command |	tibble output
------------------- |-------------------|----------------|----------------
dat[1,1]            |	element	          | tib[1,1]       |	tibble
dat[1,]             |	data frame        |	tib[1,]	       | tibble
dat[,1]             |	vector            |	tib[,1]	       | tibble
dat[2:3,]           |	data frame        |	tib[2:3,]      |	tibble
dat[,2:3]           |	data frame        |	tib[,2:3]      |	tibble

*Use tibbles.*

# Matrices

Data frames and tibbles are mostly used to describe data that take the form of a case by variable structure: each row is a case (e.g., a participant) and each column is a variable (e.g., measurement). Case by variable structures are fundamentally asymmetric because the rows and columns have qualitatively different meaning. Two participants who provide data will always provide data in the same format (if they don’t then you can’t organize the data this way), but two variables can be different in many different ways: one column might be numeric, another is a factor, yet another might contains dates. Many data sets have this characteristic. Others do not, so it is worth talking about a few other data structures that arise quite frequently!

Much like a data frame, a matrix is basically a big rectangular table of data, and there are similarities between the two. However, matrices treat columns and rows in the same fashion, and as a consequence every entry in a matrix has to be of the same type (e.g. all numeric, all character, etc). Let’s create a matrix using the row bind function, `rbind`, which combines multiple vectors in a row-wise fashion:

```{r, echo = TRUE, eval = TRUE}
row1 <- c(2, 3, 1)          # create data for row 1
row2 <- c(5, 6, 7)          # create data for row 2
mattie <- rbind(row1, row2) # row bind them into a matrix
mattie
```

Notice that when we bound the two vectors together R turned the names of the original variables into row names. To keep things fair, let’s add some exciting column names as well:

```{r, echo = TRUE, eval = TRUE}
colnames(mattie) <- c("col1", "col2", "col3")
mattie
```

## Matrix indexing

You can use square brackets to subset a matrix in much the same way that you can for data frames, again specifying a row index and then a column index. For instance, `mattie[2,3]` pulls out the entry in the 2nd row and 3rd column of the matrix (i.e., 7), whereas `mattie[2,]` pulls out the entire 2nd row, and `mattie[,3]` pulls out the entire 3rd column. However, it’s worth noting that when you pull out a column, R will print the results horizontally, not vertically.

```{r, echo = TRUE, eval = TRUE}
mattie[2,]
```

```{r, echo = TRUE, eval = TRUE}
mattie[,3]
```

This can be a little confusing for novice users: because it is no longer a two dimensional object R treats the output as a regular vector.

## Matrices vs data frames

As mentioned above difference between a data frame and a matrix is that, at a fundamental level, a matrix really is just one variable: it just happens that this one variable is formatted into rows and columns. If you want a matrix of numeric data, every single element in the matrix must be a number. If you want a matrix of character strings, every single element in the matrix must be a character string. If you try to mix data of different types together, then R will either complain or try to transform the matrix into something unexpected. To give you a sense of this, let’s do something silly and convert one element of `mattie` from the number 5 to the character string "five"…

```{r, echo = TRUE, eval = TRUE}
mattie[2,2] <- "five" 
mattie
```

Oh no I broke `mattie` – she’s all text now! 

## Other ways to make a matrix

When I created `mattie` I used the `rbind` command. Not surprisingly there is also a `cbind` command that combines vectors column-wise rather than row-wise. There is also a matrix command that you can use to specify a matrix directly:

```{r, echo = TRUE, eval = TRUE}
matrix(
  data = 1:12, # the values to include in the matrix
  nrow = 3,    # number of rows
  ncol = 4     # number of columns
)
```

The result is a *3 × 4* matrix of the numbers 1 to 12, listed column-wise. If you need to create a matrix row-wise, you can specify `byrow = TRUE` when calling `matrix()`.

# Arrays

When doing data analysis, we often have reasons to want to use higher dimensional tables (e.g., sometimes you need to cross-tabulate three variables against each other). You can’t do this with matrices, but you can do it with arrays. An array is just like a matrix, except it can have more than two dimensions if you need it to. In fact, as far as R is concerned a matrix is just a special kind of array, in much the same way that a data frame is a special kind of list. I´ll keep it short here, but will very briefly show you an example of what a three dimensional array looks like.

```{r, echo = TRUE, eval = TRUE}
arr <- array(
  data = 1:24, 
  dim = c(3,4,2)
  )
arr
```

Of course, calling an array `arr` ... no. no.

![](https://media.giphy.com/media/3orifb3X9DEIgl7hzG/giphy.gif)

# Dates

Dates (and time) are very annoying types of data. To a first approximation we can say that there are 365 days in a year, 24 hours in a day, 60 minutes in an hour and 60 seconds in a minute, but that’s not quite correct. The length of the solar day is not exactly 24 hours, and the length of solar year is not exactly 365 days, so we have a complicated system of corrections that have to be made to keep the time and date system working. On top of that, the measurement of time is usually taken relative to a local time zone, and most (but not all) time zones have both a standard time and a daylight savings time, though the date at which the switch occurs is not at all standardized. 

So, as a form of data, times and dates are just *awful* to work with. 

Unfortunately, they’re also important. Sometimes it’s possible to avoid having to use any complicated system for dealing with times and dates. Often you just want to know what year something happened in, so you can just use numeric data: in quite a lot of situations something as simple as declaring that this_year is 2019, and it works just fine. If you can get away with that for your application, this is probably the best thing to do. However, sometimes you really do need to know the actual date. Or, even worse, the actual time. In this section, I’ll very briefly introduce you to the basics of how R deals with date and time data. As with a lot of things in this chapter, I won’t go into details: the goal here is to show you the basics of what you need to do if you ever encounter this kind of data in real life.

To start with, let’s talk about the date. As it happens, modern operating systems are very good at keeping track of the time and date, and can even handle all those annoying timezone issues and daylight savings pretty well. So R takes the quite sensible view that it can just ask the operating system what the date is. We can pull the date using the `Sys.Date` function:

```{r, echo = TRUE, eval = TRUE}
today <- Sys.Date()  # ask the operating system for the date
print(today)         # display the date
```

Okay, that seems straightforward. But, it does rather look like today is just a character string, doesn’t it? That would be a problem, because dates really do have a quasi-numeric character to them, and it would be nice to be able to do basic addition and subtraction with them. Well, fear not. If you type in `class(today)`, R will tell you that the today variable is a "Date" object. What this means is that, hidden underneath this text string, R has a numeric representation. What that means is that you can in fact add and subtract days. For instance, if we add 1 to today, R will print out the date for tomorrow:

```{r, echo = TRUE, eval = TRUE}
today + 1
```

Let’s see what happens when we add 365 days:

```{r, echo = TRUE, eval = TRUE}
today + 365
```

R provides a number of functions for working with dates, but I don’t want to talk about them in any detail, other than to say that the *lubridate package* (part of the tidyverse) makes things a lot easier than they used to be. Some time (in the not too distant future) I´ll write something about working with `lubridate`, EMA data and doing... stuff...

# Coercion

Sometimes you want to change the variable class. Sometimes when you import data from files, it can come to you in the wrong format: numbers sometimes get imported as text, dates usually get imported as text, and many other possibilities besides. Sometimes you might want to convert a data frame to a tibble or vice versa. Changing the variable in this way is called coercion, and the functions to coerce variables are usually given names like `as.numeric()`, `as.factor()`, `as_tibble()` and so on.

* Coercing a data frame to a tibble

* Coercing a character vector to a factor

There are many other possibilities. A common situation requiring coercion arises when you have been given a variable x that is supposed to be representing a number, but the data file that you’ve been given has encoded it as text.

```{r, echo = TRUE, eval = TRUE}
x <- c("15","19")  # the variable
class(x)           # what class is it?
```

Obviously, if I want to do mathematical calculations using x in its current state R wil get very sad. It thinks x is text and it won’t allow me to do mathematics with text! To coerce x from “character” to “numeric”, we use the as.numeric function:

```{r, echo = TRUE, eval = TRUE}
x <- as.numeric(x)  # coerce the variable
class(x)            # what class is it?
```

```{r, echo = TRUE, eval = TRUE}
x + 1               # hey, addition works!
```

Not surprisingly, we can also convert it back again if we need to. The function that we use to do this is the `as.character` function:

```{r, echo = TRUE, eval = TRUE}
x <- as.character(x)   # coerce back to text
class(x)               # check the class
```

There are of course some limitations: you can’t coerce "hello world" into a number because there isn’t a number that corresponds to it. If you try, R metaphorically shrugs its shoulders and declares it to be missing:

```{r, echo = TRUE, eval = TRUE, error = TRUE, warning = TRUE}
x <- c("51", "hello world")
as.numeric(x)
```

Makes sense I suppose!

Another case worth talking about is how R handles coercion with logical variables. Coercing text to logical data using `as.logical()` is mostly intuitive. The strings "T", "TRUE", "True" and "true" all convert to TRUE, whereas "F", "FALSE", "False", and "false" all become FALSE. 

All other strings convert to NA. When coercing from logical to test using `as.character`, TRUE converts to "TRUE" and FALSE converts to "FALSE".

Converting numeric values to logical data – again using `as.logical` – is similarly straightforward. Following the standard convention in the study of Boolean logic `0` coerces to FALSE. Everything else is TRUE. When coercing logical to numeric, FALSE converts to 0 and TRUE converts to 1.