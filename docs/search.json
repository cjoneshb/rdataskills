{
  "articles": [
    {
      "path": "about.html",
      "title": "About",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n\n\n\n",
      "last_modified": "2022-06-28T10:18:00+02:00"
    },
    {
      "path": "activities.html",
      "title": "Activities",
      "description": "Go ahead, don´t be shy!\n",
      "author": [],
      "contents": "\n\nContents\n\n\nActivities\nActivity 1\nOpen up R Studio and in the console, type the following code:\n\n\n?rnorm\n\n\n\nThe help documentation for rnorm() should appear in the\nbottom right help panel. In the usage section, we see that\nrnorm() takes the following form:\n\n\nrnorm(n, mean = 0, sd = 1)\n\n\n\nIn the arguments section, there are explanations for each of the\narguments. n is the number of observations we\nwant to create, mean is the mean of the data points we will\ncreate and sd is the standard deviation of the set. In the\ndetails section it notes that if no values are entered for\nmean and sd it will use a default of 0 and 1\nfor these values. Because there is no default value for n\nit must be specified otherwise the code won’t run.\nLet’s try an example and just change the required argument\nn to ask R to produce 5 random numbers.\nActivity 2\nCopy and paste the following code into the console.\n\n\nset.seed(12042016)\nrnorm(n = 5, mean = 10, sd = 2)\n\n\n\nThis time R has still produced 5 random numbers, but now this set of\nnumbers has a mean of 10 and an sd of 2 as specified.\nTry to find out which functions allow you to calculate\nthe mean or the standard deviation of a column (or “variable”).\nAlways remember to use the help documentation to help you understand\nwhat arguments a function requires.\nActivity 3\nOpen up RStudio and create a new project file.\nAfter creating your new project, create a new R script and copy and\npaste the following code into the script. Save the script.\n\n\nset.seed(12042016)\nnorm_distb <- rnorm(n = 600, mean = 10, sd = 2)\n\nhist(norm_distb)\n\n\n\nActivity 4\nCreate a new RMarkdown file in your project folder. Change all the\nsetting in the YAML-header (i.e., your name.)\nWe always the date to be up-to-date: Try to replace the set date\nby a function we have used before (hint: your computer like to tell\nyou)\nAdd the following options to the YAML-header\n\n\noutput: \n  bookdown::html_document2:\n    toc: TRUE\n    toc_float: TRUE\n\n\n\nAdd a new headline to the RMarkdown file (indicated by the\n#) and copy and paste the following code beneath (set\neval = to TRUE)\n\n\nset.seed(12042016)\nnorm_distb <- rnorm(n = 600, mean = 10, sd = 2)\n\nhist(norm_distb)\n\n\n\nRender! (knit)\nActivity 5\nRe-open your RMarkdown file and add a new code chunk.\nTry to import one of the datasets you can find via the\ndatasets tab.\nCreate a histogram (as in Activity 4) for one of the columns.\nActivity 6\nIs part of the introductory post on grammar of ggplot2.\nActivity 7\nIs part of the introductory post on grammar of ggplot2.\nActivity 8\n\n\n\n",
      "last_modified": "2022-06-28T10:18:03+02:00"
    },
    {
      "path": "analysis_basics.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n\n\n\nAfter exploring our example data sets, on this page, I´ll briefly\ncover different statistical methods to e.g., compare two means or\nseveral means. For this purpose, we will again use the positive\npsychology intervention dataset (and a brief summary subset of it).\n\n\ndat <- read_csv (\"datasets/positive_psychology/ahi-cesd.csv\") # import intervention data\npinfo <- read_csv(\"datasets/positive_psychology/participant-info.csv\") # import participant data\n\nall_dat <- inner_join(x = dat,                      # the first table you want to join\n                      y = pinfo,                    # the second table you want to join\n                      by = c(\"id\", \"intervention\")) # columns the two tables have in common\n\nsummarydata <- select(.data = all_dat, \n                      ahiTotal, cesdTotal, sex, age, educ, income, occasion,elapsed.days)\n\n\n\nDescriptive statistics\nMeasures of central tendency\nDrawing pictures of the data, as I did in Figure 5.1 is an excellent\nway to convey the “gist” of what the data is trying to tell you, it’s\noften extremely useful to try to condense the data into a few simple\n“summary” statistics. In most situations, the first thing that you’ll\nwant to calculate is a measure of central tendency. That is, you’d like\nto know something about the “average” or “middle” of your data lies. The\ntwo most commonly used measures are the mean, median and mode;\noccasionally people will also report a trimmed mean.\nMean\nThe mean is calculated with the mean function. Keep in\nmind to specify how to handle missing values by setting the\nna.rm argument to TRUE.\n\n\nmean(summarydata$ahiTotal)\n\n\n[1] 72.78831\n\n# The quite not-so-easy way\nsummarydata %>%\n  summarise(mean = mean(ahiTotal))\n\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1  72.8\n\nMedian\nSimilar to the mean, the median is calculated with the\nmedian function. Again, keep in mind to specify how to\nhandle missing values by setting the na.rm argument to\nTRUE.\n\n\nmedian(summarydata$ahiTotal)\n\n\n[1] 74\n\nTrimmed mean\nAs the mean is very sensitive to the influence of potential outliers\n(or just extreme values), you might be interested in providing a trimmed\nversion of the mean. To calculate such an adjusted mean (e.g., 10%\ntrim), you can expand the mean function:\n\n\nmean(summarydata$ahiTotal, trim = .1)\n\n\n[1] 73.22292\n\nMeasures of variability\nThe statistics that we’ve discussed so far all relate to central\ntendency. That is, they all talk about which values are “in the middle”\nor “popular” in the data. However, central tendency is not the only type\nof summary statistic that we want to calculate. The second thing that we\nreally want is a measure of the variability of the data.\nMinimum, maximum and range\n\n\nmin(summarydata$ahiTotal)\n\n\n[1] 32\n\nmax(summarydata$ahiTotal)\n\n\n[1] 114\n\nrange(summarydata$ahiTotal)\n\n\n[1]  32 114\n\nQuantiles and IQR\n\n\nquantile(summarydata$ahiTotal, probs = c(0.25, 0.75))\n\n\n25% 75% \n 63  83 \n\nIQR(summarydata$ahiTotal)\n\n\n[1] 20\n\nVariance and Standard\nDeviation\n\n\nvar(summarydata$ahiTotal)\n\n\n[1] 201.6565\n\nsd(summarydata$ahiTotal)\n\n\n[1] 14.20058\n\nsqrt(var(summarydata$ahiTotal))\n\n\n[1] 14.20058\n\nSkew and Kurtosis\n\n\nlibrary(psych)\n\nskew(summarydata$ahiTotal)\n\n\n[1] -0.2278947\n\nkurtosi(summarydata$ahiTotal)\n\n\n[1] -0.1450682\n\nhist(summarydata$ahiTotal)\n\n\n\n\nCorrelations\nUp to this point we have focused entirely on how to construct\ndescriptive statistics for a single variable. What we haven’t done is\ntalked about how to describe the relationships between variables in the\ndata. To do that, we want to talk mostly about the correlation between\nvariables.\nPearson´s correlation\ncoefficient\n\n\ncor.test(x = summarydata$ahiTotal, y = summarydata$cesdTotal, method = 'pearson')\n\n\n\n    Pearson's product-moment correlation\n\ndata:  summarydata$ahiTotal and summarydata$cesdTotal\nt = -37.684, df = 990, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7920134 -0.7407612\nsample estimates:\n       cor \n-0.7676117 \n\nSpearman´s rank correlations\nThe Pearson correlation coefficient is useful for a lot of things,\nbut it does have shortcomings. One issue in particular stands out: what\nit actually measures is the strength of the linear relationship between\ntwo variables. In other words, what it gives you is a measure of the\nextent to which the data all tend to fall on a single, perfectly\nstraight line. Often, this is a pretty good approximation to what we\nmean when we say “relationship”, and so the Pearson correlation is a\ngood thing to calculation. Sometimes, it isn’t. An alternative in such\ncases can be Spearman´s rank order correlation.\n\n\ncor.test(x = summarydata$ahiTotal, y = summarydata$cesdTotal, method = 'spearman', exact = FALSE)\n\n\n\n    Spearman's rank correlation rho\n\ndata:  summarydata$ahiTotal and summarydata$cesdTotal\nS = 285349440, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.7538551 \n\nPlotting\nIf you would like to not only provide readers with raw numbers or a\nhuge table, but instead would like to plot a set of correlations, the\nggcorrplot package offers some nice functionality.\n\n\nlibrary(ggcorrplot)\n\n# Create a subset of variables to be included in correaltion matrix\ncorr_subset <- summarydata %>%\n  select(ahiTotal,\n         cesdTotal,\n         age,\n         educ,\n         income)\n\n# Create a correlation matrix\ncorr <- round(cor(corr_subset), 1)\nhead(corr)\n\n\n          ahiTotal cesdTotal age educ income\nahiTotal       1.0      -0.8   0  0.1    0.1\ncesdTotal     -0.8       1.0   0  0.0    0.0\nage            0.0       0.0   1  0.0    0.0\neduc           0.1       0.0   0  1.0    0.3\nincome         0.1       0.0   0  0.3    1.0\n\n# Compute a matrix of p-values (for correlations)\np.mat <- cor_pmat(corr_subset)\n\n# Visualize the matrix\n## Boxes\nggcorrplot(corr)\n\n\n\n## Fined tuned\nggcorrplot(corr, \n           method = \"circle\",\n           hc.order = TRUE, \n           type = \"lower\",\n           outline.col = \"white\",\n           ggtheme = ggplot2::theme_minimal,\n           colors = c(\"#6D9EC1\", \"white\", \"#E46726\"))\n\n\n\n\nComparing means\nt-test\nThere are three different kinds of t-test: the one sample test, the\nindependent samples test (Student’s and Welch’s), and the paired samples\ntest. R allows you to run all three by specific different arguments in\nthe t.test function.\n\n\n# One sample t-test\nt.test(x = summarydata$ahiTotal, mu = 70)\n\n\n\n    One Sample t-test\n\ndata:  summarydata$ahiTotal\nt = 6.1843, df = 991, p-value = 9.108e-10\nalternative hypothesis: true mean is not equal to 70\n95 percent confidence interval:\n 71.90354 73.67307\nsample estimates:\nmean of x \n 72.78831 \n\n# Two sample t-test\nt.test(formula = ahiTotal ~ sex, data = summarydata)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ahiTotal by sex\nt = 2.1577, df = 192.67, p-value = 0.03219\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 0.2513483 5.6001409\nsample estimates:\nmean in group 1 mean in group 2 \n       73.22776        70.30201 \n\n## Find Cohen´s d\nlibrary(psych)\ncohen.d(x = summarydata$ahiTotal, group = summarydata$sex)\n\n\nCall: cohen.d(x = summarydata$ahiTotal, group = summarydata$sex)\nCohen d statistic of difference between two means\n     lower effect upper\n[1,] -0.38  -0.21 -0.03\n\nMultivariate (Mahalanobis) distance between groups\n[1] 0.21\nr equivalent of difference between two means\n data \n-0.07 \n\n\n\n# Paired samples t-test (example)\nt.test(x = data$t1, y = data$t2, paired = TRUE)\n\n\n\nChecking the normality of a\nsample\nAll of the tests that we have discussed so far have assumed that the\ndata are normally distributed, or at least close enough to normal that\nyou can get away with using t-tests. Okay, so if normality is assumed by\nall the tests, and is mostly but not always satisfied (at least\napproximately) by real world data, how can we check the normality of a\nsample?\nQ-Q-Plots\nTo draw a Q-Q-Plot to check normality of your sample distribution,\nyou can use the qqnorm function.\n\n\nqqnorm(y = summarydata$ahiTotal) \n\n\n\n\nShapiro-Wilk test\nAlthough Q-Q-plots provide a nice way to informally check the\nnormality of your data, sometimes you’ll want to do something a bit more\nformal. And when that moment comes, the Shapiro-Wilk test is probably\nwhat you’re looking for. As you’d expect, the null hypothesis being\ntested is that a set of \\(N\\)\nobservations is normally distributed.\n\n\nshapiro.test(x = summarydata$ahiTotal)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  summarydata$ahiTotal\nW = 0.99038, p-value = 4.393e-06\n\nFortunately, at least for me, as I now have to(!) introduce a\nnon-parametric alternative, we find evidence that the distribution in\nour sample indeed deviates from normality. So what can we do?\nTwo sample Wilcoxon test\n\n\n# One sample\nwilcox.test(x = summarydata$ahiTotal, mu = 70)\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  summarydata$ahiTotal\nV = 289206, p-value = 8.82e-11\nalternative hypothesis: true location is not equal to 70\n\n# Two samples\nwilcox.test(formula = ahiTotal ~ sex, data = summarydata)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  ahiTotal by sex\nW = 69434, p-value = 0.03969\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n# Paired samples (example)\nwilcox.test(x = data$t1, y = data$t2, paired = TRUE)\n\n\n\n\n\n\n",
      "last_modified": "2022-06-28T10:18:10+02:00"
    },
    {
      "path": "analysis_explore.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:12+02:00"
    },
    {
      "path": "analysis_mlm.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:14+02:00"
    },
    {
      "path": "analysis_power.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:16+02:00"
    },
    {
      "path": "analysis_regression.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:18+02:00"
    },
    {
      "path": "analysis_sem.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:20+02:00"
    },
    {
      "path": "analysis.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:20+02:00"
    },
    {
      "path": "firststeps_datatypes.html",
      "title": "First steps",
      "description": "Data types\n",
      "author": [],
      "contents": "\n\nContents\n\n\n\n\n\nVectors\nIn R the name for a variable that can store multiple values is a\nvector. So let’s create one.\nCharacter vectors\nPretend we’re designing a survey, and we want to keep track of the\nresponses that a participant has given. This time, let’s imagine that\nwe’ve finished running the survey and we’re examining the data. Suppose\nwe’ve administered the Depression, Anxiety and Stress Scale (DASS) and\nas a consequence every participant has scores for on the depression,\nanxiety and stress scales provided by the DASS. One thing we might want\nto do is create a single variable called scale_name that identifies the\nthree scales. The simplest way to do this in R is to use the combine\nfunction, c.16 To do so, all we have to do is type the values we want to\nstore in a comma separated list, like this\n\n\nscale_name <- c(\"depression\",\"anxpiety\",\"stress\")\nscale_name\n\n\n[1] \"depression\" \"anxpiety\"   \"stress\"    \n\nTo use the correct terminology here, we have a single variable here\ncalled scale_name: this variable is a vector that has three\nelements. Because the vector contains text, it is a character vector.\nYou can use the length function to check the length, and\nthe class function to check what kind of vector it is:\n\n\nlength(scale_name)\n\n\n[1] 3\n\n\n\nclass(scale_name)\n\n\n[1] \"character\"\n\nNumeric vectors\nAs you might expect, we can define numeric or logical variables in\nthe same way. For instance, we could define the raw scores on the three\nDASS scales like so:\n\n\nraw_score <- c(12, 3, 8)\nraw_score\n\n\n[1] 12  3  8\n\nWe’ll talk about logical vectors in a bit.\nExtracting an element\nIf I want to extract the first element from the vector, all I have to\ndo is refer to the relevant numerical index, using square brackets to do\nso. For example, to get the first element of scale_name I would type\nthis\n\n\nscale_name[1]\n\n\n[1] \"depression\"\n\nThe second element of the vector is\n\n\nscale_name[2]\n\n\n[1] \"anxpiety\"\n\nYou get the idea…\nExtracting multiple elements\nThere are a few ways to extract multiple elements of a vector. The\nfirst way is to specify a vector that contains the indices of the\nvariables that you want to keep. To extract the first two scale\nnames:\n\n\nscale_name[c(1,2)]\n\n\n[1] \"depression\" \"anxpiety\"  \n\nAlternatively, R provides a convenient shorthand notation in which\n1:2 is a vector containing the numbers from 1 to 2, and similarly 1:10\nis a vector containing the numbers from 1 to 10. So this is also the\nsame:\n\n\nscale_name[1:2]\n\n\n[1] \"depression\" \"anxpiety\"  \n\nNotice that order matters here. So if I do this\n\n\nscale_name[c(2,1)]\n\n\n[1] \"anxpiety\"   \"depression\"\n\nI get the same numbers, but in the reverse order.\nRemoving elements\nFinally, when working with vectors, R allows us to use negative\nnumbers to indicate which elements to remove. So this is yet another way\nof doing the same thing:\n\n\nscale_name[-3]\n\n\n[1] \"depression\" \"anxpiety\"  \n\nNotice that done of this has changed the original variable. The\nscale_name itself has remained completely untouched.\n\n\nscale_name\n\n\n[1] \"depression\" \"anxpiety\"   \"stress\"    \n\nEditing vectors\nSometimes you’ll want to change the values stored in a vector.\nImagine my surprise if someone pointed out to me that “anxiety” is not\nin fact a real thing. I should probably fix that! One possibility would\nbe to assign the whole vector again from the beginning, using c. But\nthat’s a lot of typing. Also, it’s a little wasteful: why should R have\nto redefine the names for all three scales, when only the second one is\nwrong? Fortunately, we can tell R to change only the second element,\nusing this trick:\n\n\nscale_name[2] <- \"anxiety\"\nscale_name\n\n\n[1] \"depression\" \"anxiety\"    \"stress\"    \n\nThat’s better.\nNaming elements\nOne very handy thing in R is that it lets you assign meaningful names\nto the different elements in a vector. For example, the\nraw_scores vector that we introduced earlier contains the\nactual data from a study but when you print it out on its own\n\n\nraw_score\n\n\n[1] 12  3  8\n\nits not obvious what each of the scores corresponds to. There are\nseveral different ways of making this a little more meaningful (and\nwe’ll talk about them later) but for now I want to show one simple\ntrick. Ideally, what we’d like to do is have R remember that the first\nelement of the raw_score is the “depression” score, the second is\n“anxiety” and the third is “stress”. We can do that like this:\n\n\nnames(raw_score) <- scale_name\n\n\n\nThis is a bit of an unusual looking assignment statement. Usually,\nwhenever we use <- the thing on the left hand side is\nthe variable itself (i.e., raw_score) but this time around\nthe left hand side refers to the names. To see what this command has\ndone, let’s get R to print out the raw_score variable\nnow:\n\n\nraw_score\n\n\ndepression    anxiety     stress \n        12          3          8 \n\nThat’s a little nicer. Element names don’t just look nice, they’re\nfunctional too. You can refer to the elements of a vector using their\nnames, like so:\n\n\nraw_score[\"anxiety\"]\n\n\nanxiety \n      3 \n\nVector operations\nOne really nice thing about vectors is that a lot of R functions and\noperators will work on the whole vector at once. For instance, suppose I\nwant to normalize the raw scores from the DASS. Each scale of the DASS\nis constructed from 14 questions that are rated on a 0-3 scale, so the\nminimum possible score is 0 and the maximum is 42. Suppose I wanted to\nre-scale the raw scores to lie on a scale from 0 to 1. I can create the\nscaled_score variable like this:\n\n\nscaled_score <- raw_score / 42\nscaled_score\n\n\ndepression    anxiety     stress \n0.28571429 0.07142857 0.19047619 \n\nIn other words, when you divide a vector by a single number, all\nelements in the vector get divided. The same is true for addition,\nsubtraction, multiplication and taking powers. So that’s neat.\nSuppose it later turned out that I’d made a mistake. I hadn’t in fact\nadministered the complete DASS, only the first page. As noted in the\nDASS website, it’s possible to fix this mistake (sort of). First, I have\nto recognize that my scores are actually out of 21 not 42, so the\ncalculation I should have done is this:\n\n\nscaled_score <- raw_score / 21\nscaled_score\n\n\ndepression    anxiety     stress \n 0.5714286  0.1428571  0.3809524 \n\nThen, it turns out that page 1 of the full DASS is almost the same as\nthe short form of the DASS, but there’s a correction factor you have to\napply. The depression score needs to be multiplied by 1.04645, the\nanxiety score by 1.02284, and stress by 0.98617\n\n\ncorrection_factor <- c(1.04645, 1.02284, 0.98617)\ncorrected_score <- scaled_score * correction_factor\ncorrected_score\n\n\ndepression    anxiety     stress \n 0.5979714  0.1461200  0.3756838 \n\nWhat this has done is multiply the first element of\nscaled_score by the first element of\ncorrection_factor, multiply the second element of\nscaled_score by the second element of correction_factor,\nand so on.\nI’ll talk more about calculations involving vectors later, because\nthey come up a lot. In particular R has a thing called the recycling\nrule that is worth knowing about. But that’s enough detail for now.\nLogical vectors\nI mentioned earlier that we can define vectors of logical values in\nthe same way that we can store vectors of numbers and vectors of text,\nagain using the c function to combine multiple values. Logical vectors\ncan be useful as data in their own right, but the thing that they’re\nespecially useful for is extracting elements of another vector, which is\nreferred to as logical indexing.\nHere’s a simple example. Suppose I decide that the stress scale is\nnot very useful for my study, and I only want to keep the first two\nelements, depression and anxiety. One way to do this is to define a\nlogical vector that indicates which values to keep:\n\n\nkeep <- c(TRUE, TRUE, FALSE) \nkeep\n\n\n[1]  TRUE  TRUE FALSE\n\nIn this instance the keep vector indicates that it is TRUE that I\nwant to retain the first two elements, and FALSE that I want to keep the\nthird. So if I type this\n\n\ncorrected_score[keep]\n\n\ndepression    anxiety \n 0.5979714  0.1461200 \n\nR prints out the corrected scores for the two variables only. As\nusual, note that this hasn’t changed the original variable. If I print\nout the original vector…\n\n\ncorrected_score\n\n\ndepression    anxiety     stress \n 0.5979714  0.1461200  0.3756838 \n\n… all three values are still there. If I do want to create a new\nvariable, I need to explicitly assign the results of my previous command\nto a variable.\nLet’s suppose that I want to call the new variable\nshort_score, indicating that I’ve only retained some of the\nscales. Here’s how I do that:\n\n\nshort_score <- corrected_score[keep]\nshort_score\n\n\ndepression    anxiety \n 0.5979714  0.1461200 \n\nComment\nAt this point, I hope you can see why logical indexing is such a\nuseful thing. It’s a very basic, yet very powerful way to manipulate\ndata. For instance, I might want to extract the scores of the adult\nparticipants in a study, which would probably involve a command like\nscores[age > 18]. The operation age > 18\nwould return a vector of TRUE and FALSE\nvalues, and so the the full command scores[age > 18]\nwould return only the scores for participants with\nage > 18. It does take practice to become completely\ncomfortable using logical indexing, so it’s a good idea to play around\nwith these sorts of commands. Practice makes perfect, and it’s only by\npracticing logical indexing that you’ll perfect the art of cursing your\ncomputer.\nFactors\nResearch methodology classes are at pains to point out: data we\nanalyze come in different kinds! Some variables are inherently\nquantitative in nature: response time (RT) for instance, has a natural\ninterpretation in units of time. So when I define a response time\nvariable, I use a numeric vector. To keep my variable names concise,\nI’ll define the same variable again using the conventional RT\nabbreviation:\n\n\nRT <- c(420, 619, 550, 521, 1003, 486, 512, 560, 495, 610)\nRT\n\n\n [1]  420  619  550  521 1003  486  512  560  495  610\n\nA response time of 1500 milliseconds is indeed 400 milliseconds\nslower than a response time of 1100 milliseconds, so addition and\nsubtraction are meaningful operations. Similarly, 1500 milliseconds is\ntwice as long as 750 milliseconds, so multiplication and division are\nalso meaningful. That’s not the case for other kinds of data, and this\nis where factors can be useful…\nUnordered factors\nSome variables are inherently nominal in nature. If I recruit\nparticipants in an online experiment I might see that their place of\nresidence falls in one of several different regions. For simplicity,\nlet’s imagine we ran a study on MTurk designed to sample people from one\nof four distinct geographical regions: the United States, India, China\nor the European Union, which I’ll represent using the codes “us”, “in”,\n“ch” and “eu”. My first thought would be to represent the data using a\ncharacter vector:\n\n\nregion_raw <- c(\"us\",\"us\",\"us\",\"eu\",\"in\",\"eu\",\"in\",\"in\",\"us\",\"in\")\n\n\n\nThis seems quite reasonable, but there’s a problem: as it happens\nthere is nobody from China in this sample. So if I try to construct a\nfrequency table of these data – which I can do using the table()\nfunction in R – the answer I get omits China entirely:\n\n\ntable(region_raw)\n\n\nregion_raw\neu in us \n 2  4  4 \n\nIntuitively it feels like there should be a fourth entry here,\nindicating that we have 0 participants from China. R has a natural tool\nfor representing this idea, called a factor. First, we’ll create a new\nvariable using the factor() function that contains the same\ninformation but represents it as a factor:\n\n\nregion <- factor(region_raw)\nregion\n\n\n [1] us us us eu in eu in in us in\nLevels: eu in us\n\nThis looks much the same, and not surprisingly R still doesn’t know\nanything about the possibility of participants from China. However,\nnotice that the bottom of the output lists the levels of the factor. The\nlevels of a factor specify the set of values that variable could have\ntaken. By default, factor() tries to guess the levels using\nthe raw data, but we can override that manually, like this:\n\n\nregion <- factor(region_raw, levels = c(\"ch\",\"eu\",\"in\",\"us\"))\nregion\n\n\n [1] us us us eu in eu in in us in\nLevels: ch eu in us\n\nNow when we tabulate the region variable, we obtain the right\nanswer:\n\n\ntable(region)\n\n\nregion\nch eu in us \n 0  2  4  4 \n\nMuch better.\nOrdered factors\nThere are two different types of factor in R. Until now we have been\ndiscussing un-ordered factors, in which the categories are purely\nnominal and there is no notion that the categories are arranged in any\nparticular order. However, many psychologically important variables are\ninherently ordinal. Questionnaire responses often take this form, where\nparticipants might be asked to endorse a proposition using verbal\ncategories such as “strongly agree”, “agree”, “neutral”, “disagree” and\n“strongly disagree”. The five response categories can’t be given any\nsensible numerical values but they can be ordered in a sensible fashion.\nIn this situation we may want to represent the responses as an ordered\nfactor.\nTo give you a sense of how these work in R, suppose we’ve been\nunfortunate enough to be given a data set that encodes ordinal responses\nnumerically. Let’s suppose the original survey asked people how strongly\nthey supported a politicial policy. Here we have a variable consisting\nof Likert-scale data, where (let’s suppose) in the original\nquestionnaire 1 = “strongly agree” and 7 = “strongly disagree”,\n\n\nsupport_raw <- c(1, 7, 3, 4, 4, 4, 2, 6, 5, 5)\n\n\n\nWe can convert this to an ordered factor by specifying\nordered = TRUE when we call the factor()\nfunction, like so:\n\n\nsupport <- factor( \n  x = support_raw,            # the raw data\n  levels = c(7,6,5,4,3,2,1),  # strongest agreement is 1, weakest is 7\n  ordered = TRUE              # and it’s ordered\n)\nsupport\n\n\n [1] 1 7 3 4 4 4 2 6 5 5\nLevels: 7 < 6 < 5 < 4 < 3 < 2 < 1\n\nNotice that when we print out the ordered factor, R explicitly tells\nus what order the levels come in.\nBecause I wanted to order my levels in terms of increasing strength\nof endorsement, and because a response of 1 corresponded to the\nstrongest agreement and 7 to the strongest disagreement, it was\nimportant that I tell R to encode 7 as the lowest value and 1 as the\nlargest. Always check this when creating an ordered factor: it’s very\neasy to accidentally encode your data with the levels reversed if you’re\nnot paying attention. In any case, note that we can (and should) attach\nmeaningful names to these factor levels by using the levels function,\nlike this:\n\n\nlevels(support) <- c( \n  \"strong disagree\", \"disagree\", \"weak disagree\",\n  \"neutral\", \"weak agree\", \"agree\", \"strong agree\" \n)\nsupport\n\n\n [1] strong agree    strong disagree weak agree      neutral        \n [5] neutral         neutral         agree           disagree       \n [9] weak disagree   weak disagree  \n7 Levels: strong disagree < disagree < weak disagree < ... < strong agree\n\nA nice thing about ordered factors is that some analyses in R\nautomatically treat ordered factors differently to un-ordered factors,\nand generally in a way that is more appropriate for ordinal data.\nData frames / tibbles\nWe now have three variables that we might plausibly have encountered\nas the result of some study, region, support and RT.20 At the moment, R\nhas no understanding of how these variables are related to each other.\nQuite likely they’re ordered the same way, so that the data stored in\nregion[1], support[1] and RT[1]\nall come from the same person. That would be sensible, but R is a robot\nand does not possess common sense. To help a poor little robot out (and\nto make our own lives easier), it’s nice to organize these three\nvariable into a tabular format. This is where data frames – and the\ntidyverse analog tibbles – are very useful.\nMaking a data frame\nSo how do we create a data frame (or tibble)? One way: if we import\nour data from a CSV file, R will create one for you. A second method is\nto create a data frame directly from some existing variables using the\ndata.frame function. In real world data analysis this\nmethod is less common, but it’s very helpful for understanding what a\ndata frame actually is, so that’s what we’ll do in this section.\nManually constructing a data frame is simple. All you have to do when\ncalling data.frame is type a list of variables that you\nwant to include in the data frame. If I want to store the variables from\nmy experiment in a data frame called dat I can do so like this:\n\n\ndat <- data.frame(region, support, RT)\ndat\n\n\n   region         support   RT\n1      us    strong agree  420\n2      us strong disagree  619\n3      us      weak agree  550\n4      eu         neutral  521\n5      in         neutral 1003\n6      eu         neutral  486\n7      in           agree  512\n8      in        disagree  560\n9      us   weak disagree  495\n10     in   weak disagree  610\n\nNote that dat is a self-contained variable. Once\ncreated, it no longer depends on the variables from which it was\nconstructed. If we make changes to the original RT\nvariable, these will not influence the copy in dat (or vice\nversa). So for the sake of my sanity I’m going to remove all the\noriginals:\n\n\nrm(region_raw, region, support_raw, support, RT)  \n\n\n\nAs you can see, our workspace has only a single variable, a data\nframe called dat.\nIn this example I constructed the data frame manually so that you can\nsee how a data frame is built from a set of variables, but in most real\nlife situations you’d probably load your data frame directly from a CSV\nfile or similar.\nMaking a tibble\nConstructing a tibble from raw variables is essentially the same as\nconstructing a data frame, and the function we use to do this is\ntibble. If I hadn’t deleted all the raw variables in the\nprevious section, this command would work:\n\n\ntib <- tibble(region, support, RT)\n\n\n\nAlas they are gone, and I will have to try a different method.\nFortunately, I can coerce my existing data frame\ndat into a tibble using the\nas_tibble() function, and use it to create a tibble called\ntib.\n\n\ntib <- as_tibble(dat)\ntib\n\n\n# A tibble: 10 × 3\n   region support            RT\n   <fct>  <ord>           <dbl>\n 1 us     strong agree      420\n 2 us     strong disagree   619\n 3 us     weak agree        550\n 4 eu     neutral           521\n 5 in     neutral          1003\n 6 eu     neutral           486\n 7 in     agree             512\n 8 in     disagree          560\n 9 us     weak disagree     495\n10 in     weak disagree     610\n\nCoercion is an important R concept, and one that we’ll\ntalk about again. In the meantime, there are some nice things to note\nabout the output when we print tib. It states that the\nvariable is a tibble with 10 rows and 3 columns. Underneath\nthe variable names it tells you what type of data they store: region is\na factor (), support is an ordered factor\n() and RT is numeric (, short for\n“double”)21.\nTibbles are data frames\nUnder the hood, tibbles are essentially the same thing as data frames\nand are designed to behave the same way. In fact, if we use the\nclass() function to see what R thinks tib\nreally is…\n\n\nclass(tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n… it agrees that in addition to being a tibble, tib is also a data\nframe! We can check this more directly using the\nis.data.frame() function:\n\n\nis.data.frame(tib)\n\n\n[1] TRUE\n\nThat being said, there are one or two differences between tibbles and\npure data frames. For the most part, my impression has been that\nwhenever they differ, the behaviour of tibbles tends to be more\nintuitive. With this in mind, although I’ll tend to use the terms “data\nframe” and “tibble” interchangeably, for the rest of these notes I’m\ngoing to work with tibbles like tib rather than pure data\nframes like dat.\nUsing the $ operator\nAt this point our workspace contains a data frame called\ndat, a tibble called tib, but no longer\ncontains the original variables. That’s okay because the tibble (data\nframe) is acting as a container that keeps them in a nice tidy\nrectangular shape. Conceptually this is very nice, but now we have a\npractical question … how do we get information out again? There are two\nqualitatively different ways to do this, reflecting two different ways\nto think about your data:\nYour data set is a list of variables (…use\n$)\nYour data set is a table of values (…use\n[ ])\nBoth perspectives are valid, and R allows you to work with your data\nboth ways.\nTo start with, let’s think of tib as a list of\nvariables. This was the perspective we took when constructing\ndat in the first place: we took three different vectors\n(region, support, RT) and bound\nthem together into a data frame, which we later coerced into the tibble\ntib. From this perspective, what we want is an\noperator that will extract one of those variables for us. This\nis the role played by $. If I want to refer to the region\nvariable contained within the tib tibble, I would use this command:\n\n\ntib$region\n\n\n [1] us us us eu in eu in in us in\nLevels: ch eu in us\n\nAs you can see, the output looks exactly the same as it did for the\noriginal variable: tib$region is a vector (an un-ordered\nfactor in this case), and we can refer to an element of that vector in\nthe same way we normally would:\n\n\ntib$region[1]\n\n\n[1] us\nLevels: ch eu in us\n\nConceptually, the metaphor here is\ndataset$variable[value]. The table below illustrates this\nby showing what type of output you get with different commands:\ndata frame command\ndata frame output\ntibble command\ntibble output\ndat\ndata frame\ntib\ntibble\ndat$RT\nvector\ntib$RT\nvector\ndat$RT[1]\nelement\ntib$RT[1]\nelement\nAs you can see, the $ operator works the same way for\npure data frames as for tibbles. This is not quite the case for when\nusing square brackets [ ], as the next section demonstrates…\nUsing square brackets\nThe second way to think about a tibble is to treat it as a fancy\ntable. There is something appealing about this, because it emphasizes\nthe fact that the data set has a case by variable structure:\n\n\ntib\n\n\n# A tibble: 10 × 3\n   region support            RT\n   <fct>  <ord>           <dbl>\n 1 us     strong agree      420\n 2 us     strong disagree   619\n 3 us     weak agree        550\n 4 eu     neutral           521\n 5 in     neutral          1003\n 6 eu     neutral           486\n 7 in     agree             512\n 8 in     disagree          560\n 9 us     weak disagree     495\n10 in     weak disagree     610\n\nIn this structure each row is a person, and each column is a\nvariable. The square bracket notation allows you to refer to entries in\nthe data set by their row and column number (or name). As such, the\nreference looks like this:\n\n\ndataset[row,column]\n\n\n\nR allows you to select multiple rows and columns. For instance if you\nset row to be 1:3 then R will return the first three cases. Here is an\nexample where we select the first three rows and the first two\ncolumns:\n\n\ntib[1:3, 1:2]\n\n\n# A tibble: 3 × 2\n  region support        \n  <fct>  <ord>          \n1 us     strong agree   \n2 us     strong disagree\n3 us     weak agree     \n\nIf we omit values for the rows (or columns) while keeping the comma\nthen R will assume you want all rows (or columns). So this returns every\nrow in tib but only the first two columns:\n\n\ntib[, 1:2]\n\n\n# A tibble: 10 × 2\n   region support        \n   <fct>  <ord>          \n 1 us     strong agree   \n 2 us     strong disagree\n 3 us     weak agree     \n 4 eu     neutral        \n 5 in     neutral        \n 6 eu     neutral        \n 7 in     agree          \n 8 in     disagree       \n 9 us     weak disagree  \n10 in     weak disagree  \n\nAn important thing to recognize here is that – for tibbles – the\nmetaphor underpinning the square bracket system is that your data have a\nrectangular shape that is imposed by the fact that your variable is a\ntibble, and no matter what you do with the square brackets the result\nwill always remain a tibble. If I select just one row…\n\n\ntib[5,]\n\n\n# A tibble: 1 × 3\n  region support    RT\n  <fct>  <ord>   <dbl>\n1 in     neutral  1003\n\n\n\ntib[,3]\n\n\n# A tibble: 10 × 1\n      RT\n   <dbl>\n 1   420\n 2   619\n 3   550\n 4   521\n 5  1003\n 6   486\n 7   512\n 8   560\n 9   495\n10   610\n\nthe result is a tibble. Even if I select a single value…\n\n\ntib[5,3]\n\n\n# A tibble: 1 × 1\n     RT\n  <dbl>\n1  1003\n\nthe result is a tibble. For the square bracket system the rule is\nvery simple: tibbles stay tibbles\nAnnoyingly, this is not the case for a pure data frame like\ndat. For a pure data frame, any time it is possible for R\nto treat the result as something else, it does: if I were to use the\nsame commands for the data frame dat, the results would be\ndifferent in some cases. This has caused no end of frustration over the\nyears because everyone forgets about this particular property of data\nframes and stuff breaks. For what it’s worth, if you are working with\npure data frames, here’s a summary of what to expect:\ndata frame command\ndata frame output\ntibble command\ntibble output\ndat[1,1]\nelement\ntib[1,1]\ntibble\ndat[1,]\ndata frame\ntib[1,]\ntibble\ndat[,1]\nvector\ntib[,1]\ntibble\ndat[2:3,]\ndata frame\ntib[2:3,]\ntibble\ndat[,2:3]\ndata frame\ntib[,2:3]\ntibble\nUse tibbles.\nMatrices\nData frames and tibbles are mostly used to describe data that take\nthe form of a case by variable structure: each row is a case (e.g., a\nparticipant) and each column is a variable (e.g., measurement). Case by\nvariable structures are fundamentally asymmetric because the rows and\ncolumns have qualitatively different meaning. Two participants who\nprovide data will always provide data in the same format (if they don’t\nthen you can’t organize the data this way), but two variables can be\ndifferent in many different ways: one column might be numeric, another\nis a factor, yet another might contains dates. Many data sets have this\ncharacteristic. Others do not, so it is worth talking about a few other\ndata structures that arise quite frequently!\nMuch like a data frame, a matrix is basically a big rectangular table\nof data, and there are similarities between the two. However, matrices\ntreat columns and rows in the same fashion, and as a consequence every\nentry in a matrix has to be of the same type (e.g. all numeric, all\ncharacter, etc). Let’s create a matrix using the row bind function,\nrbind, which combines multiple vectors in a row-wise\nfashion:\n\n\nrow1 <- c(2, 3, 1)          # create data for row 1\nrow2 <- c(5, 6, 7)          # create data for row 2\nmattie <- rbind(row1, row2) # row bind them into a matrix\nmattie\n\n\n     [,1] [,2] [,3]\nrow1    2    3    1\nrow2    5    6    7\n\nNotice that when we bound the two vectors together R turned the names\nof the original variables into row names. To keep things fair, let’s add\nsome exciting column names as well:\n\n\ncolnames(mattie) <- c(\"col1\", \"col2\", \"col3\")\nmattie\n\n\n     col1 col2 col3\nrow1    2    3    1\nrow2    5    6    7\n\nMatrix indexing\nYou can use square brackets to subset a matrix in much the same way\nthat you can for data frames, again specifying a row index and then a\ncolumn index. For instance, mattie[2,3] pulls out the entry\nin the 2nd row and 3rd column of the matrix (i.e., 7), whereas\nmattie[2,] pulls out the entire 2nd row, and\nmattie[,3] pulls out the entire 3rd column. However, it’s\nworth noting that when you pull out a column, R will print the results\nhorizontally, not vertically.\n\n\nmattie[2,]\n\n\ncol1 col2 col3 \n   5    6    7 \n\n\n\nmattie[,3]\n\n\nrow1 row2 \n   1    7 \n\nThis can be a little confusing for novice users: because it is no\nlonger a two dimensional object R treats the output as a regular\nvector.\nMatrices vs data frames\nAs mentioned above difference between a data frame and a matrix is\nthat, at a fundamental level, a matrix really is just one variable: it\njust happens that this one variable is formatted into rows and columns.\nIf you want a matrix of numeric data, every single element in the matrix\nmust be a number. If you want a matrix of character strings, every\nsingle element in the matrix must be a character string. If you try to\nmix data of different types together, then R will either complain or try\nto transform the matrix into something unexpected. To give you a sense\nof this, let’s do something silly and convert one element of\nmattie from the number 5 to the character string\n“five”…\n\n\nmattie[2,2] <- \"five\" \nmattie\n\n\n     col1 col2   col3\nrow1 \"2\"  \"3\"    \"1\" \nrow2 \"5\"  \"five\" \"7\" \n\nOh no I broke mattie – she’s all text now!\nOther ways to make a matrix\nWhen I created mattie I used the rbind\ncommand. Not surprisingly there is also a cbind command\nthat combines vectors column-wise rather than row-wise. There is also a\nmatrix command that you can use to specify a matrix directly:\n\n\nmatrix(\n  data = 1:12, # the values to include in the matrix\n  nrow = 3,    # number of rows\n  ncol = 4     # number of columns\n)\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nThe result is a 3 × 4 matrix of the numbers 1 to 12, listed\ncolumn-wise. If you need to create a matrix row-wise, you can specify\nbyrow = TRUE when calling matrix().\nArrays\nWhen doing data analysis, we often have reasons to want to use higher\ndimensional tables (e.g., sometimes you need to cross-tabulate three\nvariables against each other). You can’t do this with matrices, but you\ncan do it with arrays. An array is just like a matrix, except it can\nhave more than two dimensions if you need it to. In fact, as far as R is\nconcerned a matrix is just a special kind of array, in much the same way\nthat a data frame is a special kind of list. I´ll keep it short here,\nbut will very briefly show you an example of what a three dimensional\narray looks like.\n\n\narr <- array(\n  data = 1:24, \n  dim = c(3,4,2)\n  )\narr\n\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\nOf course, calling an array arr … no. no.\n\nDates\nDates (and time) are very annoying types of data. To a first\napproximation we can say that there are 365 days in a year, 24 hours in\na day, 60 minutes in an hour and 60 seconds in a minute, but that’s not\nquite correct. The length of the solar day is not exactly 24 hours, and\nthe length of solar year is not exactly 365 days, so we have a\ncomplicated system of corrections that have to be made to keep the time\nand date system working. On top of that, the measurement of time is\nusually taken relative to a local time zone, and most (but not all) time\nzones have both a standard time and a daylight savings time, though the\ndate at which the switch occurs is not at all standardized.\nSo, as a form of data, times and dates are just awful to\nwork with.\nUnfortunately, they’re also important. Sometimes it’s possible to\navoid having to use any complicated system for dealing with times and\ndates. Often you just want to know what year something happened in, so\nyou can just use numeric data: in quite a lot of situations something as\nsimple as declaring that this_year is 2019, and it works just fine. If\nyou can get away with that for your application, this is probably the\nbest thing to do. However, sometimes you really do need to know the\nactual date. Or, even worse, the actual time. In this section, I’ll very\nbriefly introduce you to the basics of how R deals with date and time\ndata. As with a lot of things in this chapter, I won’t go into details:\nthe goal here is to show you the basics of what you need to do if you\never encounter this kind of data in real life.\nTo start with, let’s talk about the date. As it happens, modern\noperating systems are very good at keeping track of the time and date,\nand can even handle all those annoying timezone issues and daylight\nsavings pretty well. So R takes the quite sensible view that it can just\nask the operating system what the date is. We can pull the date using\nthe Sys.Date function:\n\n\ntoday <- Sys.Date()  # ask the operating system for the date\nprint(today)         # display the date\n\n\n[1] \"2022-06-28\"\n\nOkay, that seems straightforward. But, it does rather look like today\nis just a character string, doesn’t it? That would be a problem, because\ndates really do have a quasi-numeric character to them, and it would be\nnice to be able to do basic addition and subtraction with them. Well,\nfear not. If you type in class(today), R will tell you that\nthe today variable is a “Date” object. What this means is that, hidden\nunderneath this text string, R has a numeric representation. What that\nmeans is that you can in fact add and subtract days. For instance, if we\nadd 1 to today, R will print out the date for tomorrow:\n\n\ntoday + 1\n\n\n[1] \"2022-06-29\"\n\nLet’s see what happens when we add 365 days:\n\n\ntoday + 365\n\n\n[1] \"2023-06-28\"\n\nR provides a number of functions for working with dates, but I don’t\nwant to talk about them in any detail, other than to say that the\nlubridate package (part of the tidyverse) makes things a lot\neasier than they used to be. Some time (in the not too distant future)\nI´ll write something about working with lubridate, EMA data\nand doing… stuff…\nCoercion\nSometimes you want to change the variable class. Sometimes when you\nimport data from files, it can come to you in the wrong format: numbers\nsometimes get imported as text, dates usually get imported as text, and\nmany other possibilities besides. Sometimes you might want to convert a\ndata frame to a tibble or vice versa. Changing the variable in this way\nis called coercion, and the functions to coerce variables are usually\ngiven names like as.numeric(), as.factor(),\nas_tibble() and so on.\nCoercing a data frame to a tibble\nCoercing a character vector to a factor\nThere are many other possibilities. A common situation requiring\ncoercion arises when you have been given a variable x that is supposed\nto be representing a number, but the data file that you’ve been given\nhas encoded it as text.\n\n\nx <- c(\"15\",\"19\")  # the variable\nclass(x)           # what class is it?\n\n\n[1] \"character\"\n\nObviously, if I want to do mathematical calculations using x in its\ncurrent state R wil get very sad. It thinks x is text and it won’t allow\nme to do mathematics with text! To coerce x from “character” to\n“numeric”, we use the as.numeric function:\n\n\nx <- as.numeric(x)  # coerce the variable\nclass(x)            # what class is it?\n\n\n[1] \"numeric\"\n\n\n\nx + 1               # hey, addition works!\n\n\n[1] 16 20\n\nNot surprisingly, we can also convert it back again if we need to.\nThe function that we use to do this is the as.character\nfunction:\n\n\nx <- as.character(x)   # coerce back to text\nclass(x)               # check the class\n\n\n[1] \"character\"\n\nThere are of course some limitations: you can’t coerce “hello world”\ninto a number because there isn’t a number that corresponds to it. If\nyou try, R metaphorically shrugs its shoulders and declares it to be\nmissing:\n\n\nx <- c(\"51\", \"hello world\")\nas.numeric(x)\n\n\nWarning: NAs introduced by coercion\n[1] 51 NA\n\nMakes sense I suppose!\nAnother case worth talking about is how R handles coercion with\nlogical variables. Coercing text to logical data using\nas.logical() is mostly intuitive. The strings “T”, “TRUE”,\n“True” and “true” all convert to TRUE, whereas “F”, “FALSE”, “False”,\nand “false” all become FALSE.\nAll other strings convert to NA. When coercing from logical to test\nusing as.character, TRUE converts to “TRUE” and FALSE\nconverts to “FALSE”.\nConverting numeric values to logical data – again using\nas.logical – is similarly straightforward. Following the\nstandard convention in the study of Boolean logic 0 coerces\nto FALSE. Everything else is TRUE. When coercing logical to numeric,\nFALSE converts to 0 and TRUE converts to 1.\n\n\n\n",
      "last_modified": "2022-06-28T10:18:24+02:00"
    },
    {
      "path": "firststeps_functions.html",
      "title": "First steps",
      "description": "Functions\n",
      "author": [],
      "contents": "\n\nContents\n\n\nUsing functions\nTo calculate the square root of 255 using the sqrt\nfunction, the command I type is this:\n\n\nsqrt(225)\n\n\n[1] 15\n\nWhen we use a function to do something, we generally refer to this as\ncalling the function, and the values that we type into the function\n(there can be more than one) are referred to as the arguments of that\nfunction.\nObviously, the sqrt function doesn’t really give us any\nnew functionality, since we already knew how to do square root\ncalculations by using the power operator ^. However, there\nare lots of other functions in R: in fact, almost everything of interest\nthat I’ll talk about in this book is an R function of some kind. For\nexample, one function that comes in handy quite often is the absolute\nvalue function. Compared to the square root function, it’s extremely\nsimple: it just converts negative numbers to positive numbers, and\nleaves positive numbers alone. Calculating absolute values in R is\npretty easy, since R provides the abs function that you can use for this\npurpose. For instance:\n\n\nabs(-13)\n\n\n[1] 13\n\nCombining functions\nBefore moving on, it’s worth noting that, in the same way that R\nallows us to put multiple operations together into a longer command\n(like 1 + 2 * 4 for instance), it also lets us put\nfunctions together and even combine functions with operators if we so\ndesire. For example, the following is a perfectly legitimate\ncommand:\n\n\nsqrt(1 + abs(-8))\n\n\n[1] 3\n\nWhen R executes this command, starts out by calculating the value of\nabs(-8), which produces an intermediate value of 8. Having\ndone so, the command simplifies to sqrt(1 + 8). To solve\nthe square root it first needs to add 1 + 8 to get 9, at\nwhich point it evaluates sqrt(9), and so it finally outputs\na value of 3.\nMultiple arguments\nThere’s two more fairly important things that you need to understand\nabout how functions work in R, and that’s the use of “named” arguments,\nand default values” for arguments. Not surprisingly, that’s not to say\nthat this is the last we’ll hear about how functions work, but they are\nthe last things we desperately need to discuss in order to get you\nstarted. To understand what these two concepts are all about, I’ll\nintroduce another function. The round function can be used\nto round some value to the nearest whole number. For example, I could\ntype this:\n\n\nround(3.1415)\n\n\n[1] 3\n\nPretty straightforward, really. However, suppose I only wanted to\nround it to two decimal places: that is, I want to get 3.14 as the\noutput. The round function supports this, by allowing you\nto input a second argument to the function that specifies the number of\ndecimal places that you want to round the number to. In other words, I\ncould do this:\n\n\nround(3.14165, 2)\n\n\n[1] 3.14\n\nWhat’s happening here is that I’ve specified two arguments: the first\nargument is the number that needs to be rounded (i.e., 3.1415), the\nsecond argument is the number of decimal places that it should be\nrounded to (i.e., 2), and the two arguments are separated by a\ncomma.\nArgument names\nIn this simple example, it’s not too hard to remember which argument\ncomes first and which one comes second, but as you might imagine it\nstarts to get very difficult once you start using complicated functions\nthat have lots of arguments. Fortunately, most R functions use argument\nnames to make your life a little easier. For the round\nfunction, for example the number that needs to be rounded is specified\nusing the x argument, and the number of decimal points that you want it\nrounded to is specified using the digits argument. Because\nwe have these names available to us, we can specify the arguments to the\nfunction by name. We do so like this:\n\n\nround(x = 3.1415, digits = 2)\n\n\n[1] 3.14\n\nNotice that this is kind of similar in spirit to variable assignment,\nexcept that I used = here, rather than <-.\nIn both cases we’re specifying specific values to be associated with a\nlabel. However, there are some differences between what I was doing\nearlier on when creating variables, and what I’m doing here when\nspecifying arguments, and so as a consequence it’s important that you\nuse = in this context.\nAs you can see, specifying the arguments by name involves a lot more\ntyping, but it’s also a lot easier to read. Because of this, the\ncommands in this book will usually specify arguments by name, since that\nmakes it clearer to you what I’m doing. However, one important thing to\nnote is that when specifying the arguments using their names, it doesn’t\nmatter what order you type them in. But if you don’t use the argument\nnames, then you have to input the arguments in the correct order. In\nother words, these three commands all produce the same output…\n\n\nround(3.14165, 2)\n\n\n[1] 3.14\n\nround(x = 3.1415, digits = 2)\n\n\n[1] 3.14\n\nround(digits = 2, x = 3.1415)\n\n\n[1] 3.14\n\nbut this one does not…\n\n\nround(2, 3.14165)\n\n\n[1] 2\n\nDefault values\nOkay, so that’s the first thing I said you’d need to know: argument\nnames. The second thing you need to know about is default values. Notice\nthat the first time I called the round function I didn’t\nactually specify the digits argument at all, and yet R\nsomehow knew that this meant it should round to the nearest whole\nnumber. How did that happen? The answer is that the digits\nargument has a default value of 0, meaning that if you decide not to\nspecify a value for digits then R will act as if you had typed\ndigits = 0. This is quite handy: most of the time when you\nwant to round a number you want to round it to the nearest whole number,\nand it would be pretty annoying to have to specify the\ndigits argument every single time. On the other hand,\nsometimes you actually do want to round to something other than the\nnearest whole number, and it would be even more annoying if R didn’t\nallow this! Thus, by having digits = 0 as the default\nvalue, we get the best of both worlds.\nTab autocomplete\nThe first thing I want to call your attention to is the autocomplete\nability in Rstudio. Let’s stick to our example above and assume that\nwhat you want to do is to round a number. This time around, start typing\nthe name of the function that you want, and then hit the “tab” key.\nRstudio will then display a little window like the one shown here:\n\nIn this figure, I’ve typed the letters rou at the\ncommand line, and then hit tab. The window has two panels. On the left,\nthere’s a list of variables and functions that start with the letters\nthat I’ve typed shown in black text, and some grey text that tells you\nwhere that variable/function is stored. Ignore the grey text for now: it\nwon’t make much sense to you until we’ve talked about\npackages. There’s a few options there, and the one we want\nis round, but if you’re typing this yourself you’ll notice that when you\nhit the tab key the window pops up with the top entry highlighted. You\ncan use the up and down arrow keys to select the one that you want. Or,\nif none of the options look right to you, you can hit the escape key\n(“esc”) or the left arrow key to make the window go away.\nIn our case, the thing we want is the round option, and\nthe panel on the right tells you a bit about how the function works.\nThis display is really handy. The very first thing it says is\nround(x, digits = 0) : what this is telling you is that the\nround function has two arguments. The first argument is\ncalled x, and it doesn’t have a default value. The second argument is\ndigits, and it has a default value of 0. In a\nlot of situations, that’s all the information you need. But Rstudio goes\na bit further, and provides some additional information about the\nfunction underneath. Sometimes that additional information is very\nhelpful, sometimes it’s not: Rstudio pulls that text from the R help\ndocumentation, and my experience is that the helpfulness of that\ndocumentation varies wildly. Anyway, if you’ve decided that\nround is the function that you want to use, you can hit the\nenter key and Rstudio will finish typing the rest of the function name\nfor you.\nThe history pane\nOne thing R does is keep track of your “command history”. That is, it\nremembers all the commands that you’ve previously typed. You can access\nthis history in a few different ways. The simplest way is to use the up\nand down arrow keys. If you hit the up key, the R console will show you\nthe most recent command that you’ve typed. Hit it again, and it will\nshow you the command before that. If you want the text on the screen to\ngo away, hit escape. Using the up and down keys can be really handy if\nyou’ve typed a long command that had one typo in it. Rather than having\nto type it all again from scratch, you can use the up key to bring up\nthe command and fix it.\nThe second way to get access to your command history is to look at\nthe history panel in Rstudio. On the upper right hand side of the\nRstudio window you’ll see a tab labelled “History”. Click on that, and\nyou’ll see a list of all your recent commands displayed in that panel:\nit should look something like this:\nIf you double click on one of the commands, it will be copied to the\nR console. You can achieve the same result by selecting the command you\nwant with the mouse and then clicking the “To Console” button.\n\n\n\n",
      "last_modified": "2022-06-28T10:18:28+02:00"
    },
    {
      "path": "firststeps_intro.html",
      "title": "First steps",
      "description": "One big calculator\n",
      "author": [],
      "contents": "\n\nContents\n\n\nIntroduction\nTo get started, we will cover how to use R and\nRStudio as well as some basic programming concepts and\nterminology. We will discuss common pitfalls and, most importantly,\nwhere to get help. Those of you who have no programming experience\nshould find this chapter particularly helpful, however, even if you’ve\nused R (or other languages) before there may be some (I hope)\nhelpful stuff here.\nKeep in mind: You are not expected to memorise all the information -\njust make sure you know what help is available and where you can look up\nthe information you need!\nGetting a feel for R and\nRStudio\nAn important distinction to remember is between the R programming\nlanguage itself, and the software you use to interact with R. You could\nchoose to interact with R directly from the terminal, but that’s\npainful, so most people use an integrated development environment (IDE),\nwhich takes care of a lot of boring tasks for you. For this class, we’ll\nuse the popular Rstudio IDE. To get started, make sure you have both R\nand RStudio installed on your computer. Both are free and open source,\nand for most people they should be straightforward to install.\nInstalling R: Go to the R\nwebsite and download the installer file. Once downloaded, open the\ninstaller file and follow the instructions.\nInstalling RStudio: Go to the RStudio website, and follow the links\nto download RStudio. The version you want is the “RStudio Desktop”. Once\nthe installer is downloaded, open it and follow the\ninstructions.\nR Studio has a console that you can try out code in (appearing as the\nbottom left window in Figure below), there is a script editor (top\nleft), a window showing functions and objects you have created in the\n“environment” tab (top right window in the figure), and a window that\nshows plots, files packages, and help documentation (bottom right).\n\nOne big calculator\nConsole vs. scripts\nWhen you first open up R Studio you won’t see a script like above,\nthere will just be a single pane on the left, which is the console. You\ncan write code in the console to test it out, but it won’t save\nanywhere. For this chapter, we’ll use the console to show you some\nsimple code examples but moving forward you’ll save your code in a\nscript file, and you’ll see the extra pane appear.\nOne of the easiest things you can do with R is use it as a simple\ncalculator, so it’s a good place to start. For instance, try typing\n30, and hitting enter. When you do this, you’ve entered a\ncommand, and R will execute that command. What you see on screen now\nwill be this:\n\n\n10 + 20\n\n\n[1] 30\n\nNot a lot of surprises in this extract. But there’s a few things\nworth talking about, even with such a simple example. Firstly, it’s\nimportant that you understand how to read the extract. In this example,\nwhat I typed was the 10 + 20 part at the top, and the content below is\nwhat R produced.\nBefore we go on to talk about other types of calculations that we can\ndo with R, there’s a few other things I want to point out. The first\nthing is that, while R is good software, it’s still software. It’s\npretty stupid, and because it’s stupid it can’t handle typos. It takes\nit on faith that you meant to type exactly what you did type. For\nexample, suppose that you forgot to hit the shift key when trying to\ntype +, and as a result your command ended up being 10 = 20\nrather than 10 + 20. Here’s what happens:\n\n\n10 = 20\n\n\nError in 10 = 20: invalid (do_set) left-hand side to assignment\n\nWhat’s happened here is that R has attempted to interpret\n10 = 20 as a command, and spits out an error message\nbecause the command doesn’t make any sense to it. When a human looks at\nthis, and then looks down at his or her keyboard and sees that\n+ and = are on the same key, it’s pretty\nobvious that the command was a typo. But R doesn’t know this, so it gets\nupset. And, if you look at it from its perspective, this makes sense.\nAll that R “knows” is that 10 is a legitimate number, 20 is a legitimate\nnumber, and = is a legitimate part of the language too. In\nother words, from its perspective this really does look like the user\nmeant to type 10 = 20, since all the individual parts of\nthat statement are legitimate and it’s too stupid to realize that this\nis probably a typo. Therefore, R takes it on faith that this is exactly\nwhat you meant… it only “discovers” that the command is nonsense when it\ntries to follow your instructions, typo and all. And then it whinges,\nand spits out an error.\nEven more subtle is the fact that some typos won’t produce errors at\nall, because they happen to correspond to “well-formed” R commands. For\ninstance, suppose that not only did I forget to hit the shift key when\ntrying to type 10 + 20, I also managed to press the key\nnext to one I meant do. The resulting typo would produce the command\n10 - 20. Clearly, R has no way of knowing that you meant to\nadd 20 to 10, not subtract 20 from 10, so what happens this time is\nthis:\n\n\n10 - 20\n\n\n[1] -10\n\nIn this case, R produces the right answer, but to the the wrong\nquestion.\nTo some extent, I’m stating the obvious here, but it’s important. The\npeople who wrote R are smart. You, the user, are smart. But R itself is\ndumb. And because it’s dumb, it has to be mindlessly obedient. It does\nexactly what you ask it to do. There is no equivalent to “autocorrect”\nin R, and for good reason. When doing advanced stuff – and even the\nsimplest of statistics is pretty advanced in a lot of ways – it’s\ndangerous to let a mindless automaton like R try to overrule the human\nuser. But because of this, it’s your responsibility to be careful.\nAlways make sure you type exactly what you mean. When dealing with\ncomputers, it’s not enough to type “approximately” the right thing. In\ngeneral, you absolutely must be precise in what you say to R … like all\nmachines it is too stupid to be anything other than absurdly literal in\nits interpretation.\nCommands and spacing\nOf course, now that I’ve been so uptight about the importance of\nalways being precise, I should point out that there are some exceptions.\nOr, more accurately, there are some situations in which R does show a\nbit more flexibility than my previous description suggests. The first\nthing R is smart enough to do is ignore redundant spacing. What I mean\nby this is that, when I typed 10 + 20 before, I could\nequally have done this\n\n\n10        + 20\n\n\n[1] 30\n\nand get exactly the same answer. However, that doesn’t mean that you\ncan insert spaces in any old place. For instance, when you open up R it\nsuggests that you type citation() to get some information\nabout how to cite R:\n\n\ncitation()\n\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for\n  statistical computing. R Foundation for Statistical\n  Computing, Vienna, Austria. URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R,\nplease cite it when using it for data analysis. See also\n'citation(\"pkgname\")' for citing R packages.\n\nOkay, that’s good to know. Let’s see what happens when I try changing\nthe spacing. If I insert spaces in between the word and the parentheses,\nor inside the parentheses themselves, then all is well. But inserting\nspaces in the middle of the commands, not so much. Try these three just\nto see:\ncitation () # works!\ncitation(    ) # works!\ncita tion() # doesn’t work\nR knows you’re not finished?\nOne more thing I should point out. If you hit enter in a situation\nwhere it’s “obvious” to R that you haven’t actually finished typing the\ncommand, R is just smart enough to keep waiting. For example, if you\ntype 10 + and then press enter, even R is smart enough to\nrealize that you probably wanted to type in another number. So here’s\nwhat happens:\n\n10 +\nError: <text>:2:0: unexpected end of input\n1: 10 +\n   ^\n\nand there’s a blinking cursor next to the plus sign. What this means\nis that R is still waiting for you to finish. It “thinks” you’re still\ntyping your command, so it hasn’t tried to execute it yet. In other\nwords, this plus sign is actually another command prompt. It’s different\nfrom the usual one (i.e., the > symbol) to remind you\nthat R is going to “add” whatever you type now to what you typed last\ntime. For example, if I then go on to type 20 and hit enter, what I get\nis this:\n\n\n10 +\n+ 20\n\n\n[1] 30\n\nAnd as far as R is concerned, this is exactly the same as if you had\ntyped 10 + 20. Similarly, consider the citation() command that we talked\nabout in the previous section. Suppose you hit enter after typing\ncitation(. Once again, R is smart enough to realize that there must be\nmore coming – since you need to add the ) character – so it waits. I can\neven hit enter several times and it will keep waiting:\n\n\ncitation( \n\n  \n  )\n\n\n\nSometimes when doing this, you’ll eventually get yourself in trouble\n(it happens to us all). Maybe you start typing a command, and then you\nrealize you’ve screwed up. For example,\n\ncitblation(\n+\n+\n\nYou’d probably prefer R not to try running this command, right? If\nyou want to get out of this situation, just hit the escape key. R will\nreturn you to the normal command prompt (i.e. >) without attempting\nto execute the botched command.\nThat being said, it’s not often the case that R is smart enough to\ntell that there’s more coming. For instance, in the same way that I\ncan’t add a space in the middle of a word, I can’t hit enter in the\nmiddle of a word either. If I hit enter after typing citat\nI get an error, because R thinks I’m interested in something called\ncitat and can’t find it:\n\n\ncitat\n\n\n\nWhat about if I typed citation and hit enter, without the\nparentheses? In this case we get something very odd, something that we\ndefinitely don’t want, at least not at this stage. Here’s what\nhappens:\n\n\ncitation\n\n\nfunction (package = \"base\", lib.loc = NULL, auto = NULL) \n{\n    if (!is.null(auto) && !is.logical(auto) && !anyNA(match(c(\"Package\", \n        \"Version\", \"Title\"), names(meta <- as.list(auto)))) && \n        !all(is.na(match(c(\"Authors@R\", \"Author\"), names(meta))))) {\n        auto_was_meta <- TRUE\n        package <- meta$Package\n    }\n    else {\n        auto_was_meta <- FALSE\n        dir <- system.file(package = package, lib.loc = lib.loc)\n        if (dir == \"\") \n            stop(packageNotFoundError(package, lib.loc, sys.call()))\n        meta <- packageDescription(pkg = package, lib.loc = dirname(dir))\n        citfile <- file.path(dir, \"CITATION\")\n        test <- file_test(\"-f\", citfile)\n        if (!test) {\n            citfile <- file.path(dir, \"inst\", \"CITATION\")\n            test <- file_test(\"-f\", citfile)\n        }\n        if (is.null(auto)) \n            auto <- !test\n        if (!auto) {\n            return(readCitationFile(citfile, meta))\n        }\n    }\n    if ((!is.null(meta$Priority)) && (meta$Priority == \"base\")) {\n        cit <- citation(\"base\", auto = FALSE)\n        attr(cit, \"mheader\")[1L] <- paste0(\"The \", sQuote(package), \n            \" package is part of R.  \", attr(cit, \"mheader\")[1L])\n        return(.citation(cit, package))\n    }\n    year <- sub(\"-.*\", \"\", meta$`Date/Publication`)\n    if (!length(year)) {\n        if (is.null(meta$Date)) {\n            warning(gettextf(\"no date field in DESCRIPTION file of package %s\", \n                sQuote(package)), domain = NA)\n        }\n        else {\n            date <- trimws(as.vector(meta$Date))[1L]\n            date <- strptime(date, \"%Y-%m-%d\", tz = \"GMT\")\n            if (!is.na(date)) \n                year <- format(date, \"%Y\")\n        }\n    }\n    if (!length(year)) {\n        date <- as.POSIXlt(sub(\";.*\", \"\", trimws(meta$Packaged)[1L]))\n        if (!is.na(date)) \n            year <- format(date, \"%Y\")\n    }\n    if (!length(year)) {\n        warning(gettextf(\"could not determine year for %s from package DESCRIPTION file\", \n            sQuote(package)), domain = NA)\n        year <- NA_character_\n    }\n    author <- meta$`Authors@R`\n    if (length(author)) {\n        aar <- .read_authors_at_R_field(author)\n        author <- Filter(function(e) {\n            !(is.null(e$given) && is.null(e$family)) && !is.na(match(\"aut\", \n                e$role))\n        }, aar)\n        if (!length(author)) \n            author <- Filter(function(e) {\n                !(is.null(e$given) && is.null(e$family)) && !is.na(match(\"cre\", \n                  e$role))\n            }, aar)\n    }\n    if (length(author)) {\n        has_authors_at_R_field <- TRUE\n    }\n    else {\n        has_authors_at_R_field <- FALSE\n        author <- as.personList(meta$Author)\n    }\n    z <- list(title = paste0(package, \": \", meta$Title), author = author, \n        year = year, note = paste(\"R package version\", meta$Version))\n    if (identical(meta$Repository, \"CRAN\")) \n        z$url <- sprintf(\"https://CRAN.R-project.org/package=%s\", \n            package)\n    if (identical(meta$Repository, \"R-Forge\")) {\n        z$url <- if (!is.null(rfp <- meta$\"Repository/R-Forge/Project\")) \n            sprintf(\"https://R-Forge.R-project.org/projects/%s/\", \n                rfp)\n        else \"https://R-Forge.R-project.org/\"\n        if (!is.null(rfr <- meta$\"Repository/R-Forge/Revision\")) \n            z$note <- paste(z$note, rfr, sep = \"/r\")\n    }\n    if (!length(z$url) && !is.null(url <- meta$URL)) {\n        if (grepl(\"[, ]\", url)) \n            z$note <- url\n        else z$url <- url\n    }\n    header <- if (!auto_was_meta) {\n        gettextf(\"To cite package %s in publications use:\", sQuote(package))\n    }\n    else NULL\n    footer <- if (!has_authors_at_R_field && !auto_was_meta) {\n        gettextf(\"ATTENTION: This citation information has been auto-generated from the package DESCRIPTION file and may need manual editing, see %s.\", \n            sQuote(\"help(\\\"citation\\\")\"))\n    }\n    else NULL\n    author <- format(z$author, include = c(\"given\", \"family\"))\n    if (length(author) > 1L) \n        author <- paste(paste(head(author, -1L), collapse = \", \"), \n            tail(author, 1L), sep = \" and \")\n    rval <- bibentry(bibtype = \"Manual\", textVersion = paste0(author, \n        \" (\", z$year, \"). \", z$title, \". \", z$note, \". \", z$url), \n        header = header, footer = footer, other = z)\n    .citation(rval, package)\n}\n<bytecode: 0x7f909770dee0>\n<environment: namespace:utils>\n\nwhere the BLAH BLAH BLAH goes on for rather a long time, and you\ndon’t know enough R yet to understand what all this gibberish actually\nmeans. This incomprehensible output can be quite intimidating to novice\nusers, and unfortunately it’s very easy to forget to type the\nparentheses; so almost certainly you’ll do this by accident. Do not\npanic when this happens. Simply ignore the gibberish.\nOperations\nOkay, now that we’ve discussed some of the tedious details associated\nwith typing R commands, let’s get back to learning how to use the most\npowerful piece of statistical software in the world as a $2 calculator.\nSo far, all we know how to do is addition. Clearly, a calculator that\nonly did addition would be a bit stupid, so I should tell you about how\nto perform other simple calculations using R. But first, some more\nterminology. Addition is an example of an “operation” that you can\nperform (specifically, an arithmetic operation), and the operator that\nperforms it is +. To people with a programming or\nmathematics background, this terminology probably feels pretty natural,\nbut to other people it might feel like I’m trying to make something very\nsimple (addition) sound more complicated than it is (by calling it an\narithmetic operation). To some extent, that’s true: if addition was the\nonly operation that we were interested in, it’d be a bit silly to\nintroduce all this extra terminology. However, as we go along, we’ll\nstart using more and more different kinds of operations, so it’s\nprobably a good idea to get the language straight now, while we’re still\ntalking about very familiar concepts like addition!\nArithmetic operations\nSo, now that we have the terminology, let’s learn how to perform some\narithmetic operations. R has operators that correspond to the basic\narithmetic we learned in primary school: addition is +,\nsubtraction is -, multiplication is * and\ndivision is /. As you can see, R uses fairly standard\nsymbols to denote each of the different operations you might want to\nperform: if I wanted to find out what 57 times 61 is (and who\nwouldn’t?), I can use R instead of a calculator, like so:\n\n\n57 * 61\n\n\n[1] 3477\n\nSo that’s handy.\nThere are three other arithmetic operations that I should probably\nmention: taking powers, doing integer division, and calculating a\nmodulus. Of the three, the only one that is of any real importance for\nthe purposes of this book is taking powers, so I’ll discuss that one\nhere: the other two are discussed later.\nType in the complete multiplication?\n\n\n5*5*5*5\n\n\n[1] 625\n\nbut that does seem a bit tedious. It would be very annoying indeed if\nyou wanted to calculate \\(5^{15}\\),\nsince the command would end up being quite long. Therefore, to make our\nlives easier, we use the power operator instead. When we do that, our\ncommand to calculate \\(5^{4}\\) goes\nlike this:\n\n\n5^4\n\n\n[1] 625\n\nMuch easier.\nOrder of operations\nOkay. At this point, you know how to take one of the most powerful\npieces of statistical software in the world, and use it as a calculator.\nAnd as a bonus, you’ve learned a few very basic programming concepts.\nThat’s not nothing (you could argue that you’ve just saved yourself) but\non the other hand, it’s not very much either. In order to use R more\neffectively, we need to introduce more programming concepts.\nIn most situations where you would want to use a calculator, you\nmight want to do multiple calculations. R lets you do this, just by\ntyping in longer commands. In fact, we’ve already seen an example of\nthis earlier, when I typed in 5 * 5 * 5 * 5. However, let’s\ntry a slightly different example:\n\n\n1 + 2 * 4 \n\n\n[1] 9\n\nClearly, this isn’t a problem for R either. However, it’s worth\nstopping for a second, and thinking about what R just did. Clearly,\nsince it gave us an answer of 9 it must have multiplied 2 * 4 (to get an\ninterim answer of 8) and then added 1 to that. But, suppose it had\ndecided to just go from left to right: if R had decided instead to add\n1+2 (to get an interim answer of 3) and then multiplied by 4, it would\nhave come up with an answer of 12\nTo answer this, you need to know the order of operations that R uses.\nIf you remember school maths classes, it’s actually the same order that\nyou got taught when you were at school. That is, first calculate things\ninside Brackets, then calculate Exponents, then Division and\nMultiplication, then Addition and Subtraction. So, to continue the\nexample above, if we want to force R to calculate the 1 + 2 part before\nthe multiplication, all we would have to do is enclose it in\nbrackets:\n\n\n(1 + 2) * 4\n\n\n[1] 12\n\nThis is a fairly useful thing to be able to do. The only other thing\nI should point out about order of operations is what to expect when you\nhave two operations that have the same priority: that is, how does R\nresolve ties? For instance, multiplication and division are actually the\nsame priority, but what should we expect when we give R a problem like\n4 / 2 * 3 to solve? If it evaluates the multiplication\nfirst and then the division, it would calculate a value of two-thirds.\nBut if it evaluates the division first it calculates a value of six. The\nanswer, in this case, is that R goes from left to right, so in this case\nthe division step would come first:\n\n\n4 / 2 * 3\n\n\n[1] 6\n\nAll of the above being said, it’s helpful to remember that brackets\nalways come first. So, if you’re ever unsure about what order R will do\nthings in, an easy solution is to enclose the thing you want it to do\nfirst in brackets. There’s nothing stopping you from typing\n(4 / 2) * 3. By enclosing the division in brackets we make\nit clear which thing is supposed to happen first. In this instance you\nwouldn’t have needed to, since R would have done the division first\nanyway, but when you’re first starting out it’s better to make sure R\ndoes what you want!\nFunctions\nThe symbols +, -, * and so on\nare examples of operators. As we’ve seen, you can do quite a lot of\ncalculations just by using these operators. However, in order to do more\nadvanced calculations (and later on, to do actual statistics), you’re\ngoing to need to start using functions. I’ll talk in more detail about\nfunctions and how they work later, but for now let’s just dive in and\nuse a few. To get started, suppose I wanted to take the square root of\n225. There’s two ways I could do this using R. Firstly, since the square\nroot of 255 is the same thing as raising 225 to the power of 0.5, I\ncould use the power operator ^, just like we did earlier:\n\n\n225 ^ 0.5\n\n\n[1] 15\n\nHowever, there’s a second way to do this by using square root\nfunction sqrt. What´s a function, you ask? Well, there\nactually is a new page for that…\n\n\n\n",
      "last_modified": "2022-06-28T10:18:29+02:00"
    },
    {
      "path": "firststeps_preparations.html",
      "title": "First steps",
      "description": "Preparations\n",
      "author": [],
      "contents": "\n\nContents\n\n\nBase R and packages\nWhen you install R you will have access to a range of functions\nincluding options for data wrangling and statistical analysis. The\nfunctions that are included in the default installation are typically\nreferred to as Base R and there is a useful cheat sheet that shows many\nBase R functions here.\nHowever, the power of R is that it is extendable and open source -\nput simply, if a function doesn’t exist or doesn’t work very well,\nanyone can create a new package that contains data and code to allow you\nto perform new tasks. You may find it useful to think of Base R as the\ndefault apps that come on your phone and packages as additional apps\nthat you need to download separately.\nInstalling the\ntidyverse (or any other package)\nIn order to use a package, you must first install it. The following\ncode installs the package tidyverse, a package we will use very\nfrequently in this course.\nIf you are working on your own computer, use the below code to\ninstall the tidyverse. You do not need to do this if you are working on\nthe server or if you are using the computers in the Boyd Orr\nbuilding.\n\n\ninstall.packages(\"tidyverse\")\n\n\n\nYou only need to install a package once, however, each time you start\nR you need to load the packages you want to use, in a similar way that\nyou need to install an app on your phone once, but you need to open it\nevery time you want to use it.\nIf you get an error message that says something like\nWARNING: Rtools is required to build R packages you may\nneed to download and install an extra bit of software called Rtools.\nTo load packages we use the function library().\nTypically you would start any analysis script by loading all of the\npackages you need, but we will come back to that in the next\nchapter.\nLoading the\ntidyverse (or any other package)\nRun the below code to load the tidyverse. You can do this regardless\nof whether you are using your own computer or the server.\n\n\nlibrary(tidyverse)\n\n\n\nYou will get what looks like an error message - it’s not. It’s just R\ntelling you what it’s done.\nNow that we’ve loaded the tidyverse package we can use any of the\nfunctions it contains but remember, you need to run the\nlibrary() function every time you start R.\nPackage updates\nIn addition to updates to R and R Studio, the creators of packages\nalso sometimes update their code. This can be to add functions to a\npackage, or it can be to fix errors. One thing to avoid is\nunintentionally updating an installed package. When you run\ninstall.packages() it will always install the latest\nversion of the package and it will overwrite any older versions you may\nhave installed. Sometimes this isn’t a problem, however, sometimes you\nwill find that the update means your code no longer works as the package\nhas changed substantially. It is possible to revert back to an older\nversion of a package but try to avoid this anyway.\nTo avoid accidentally overwriting a package with a later version, you\nshould never include install.packages() in your analysis\nscripts in case you, or someone else runs the code by mistake. Remember,\nthe server will already have all of the packages you need for this\ncourse so you only need to install packages if you are using your own\nmachine.\nPackage conflicts\nThere are thousands of different R packages with even more functions.\nUnfortunately, sometimes different packages have the same function\nnames. For example, the packages dplyr and MASS both have a function\nnamed select(). If you load both of these packages, R will\nproduce a warning telling you that there is a conflict.\n\n\nlibrary(dplyr)\nlibrary(MASS)\n\n\n\nIn this case, R is telling you that the function\nselect() in the dplyr package is being hidden (or ‘masked’)\nby another function with the same name. If you were to try and use\nselect(), R would use the function from the package that\nwas loaded most recently - in this case it would use the function from\nMASS.\nIf you want to specify which package you want to use for a particular\nfunction you can use code in the format package::function,\nfor example:\n\n\ndplyr::select()\nMASS::select()\n\n\n\nObjects\nA large part of your coding will involve creating and manipulating\nobjects. Objects contain stuff. That stuff can be numbers, words, or the\nresult of operations and analyses.You assign content to an object using\n<-.\nCreate some objects\nCopy and paste the following code into the console, change the code\nso that it uses your own name and age and run it. You should see that\nname, age, today, new_year, and data appear in the environment pane.\n\n\nname <- \"christopher\"\nage <- 10 + 26 \ntoday <-Sys.Date()\nnew_year <- as.Date(\"2023-01-01\")\ndata <- rnorm(n = 10, mean = 15, sd = 3)\n\n\n\n\nNote that in these examples, name,age, and new_year would always\ncontain the values christopher, 36, and the date of New Year’s Day 2023,\nhowever, today will draw the date from the operating system and data\nwill be a randomly generated set of data so the values of these objects\nwill not be static.\nAs a side note, if you ever have to teach programming and statistics,\ndon’t use your age as an example because every time you have to update\nyour teaching materials you get a reminder of the fragility of existence\nand your advancing age. What about me? I will remain forever 36.\n\nImportantly, objects can be involved in calculations and can interact\nwith each other. For example:\n\n\nage + 10\n\n\n[1] 46\n\nnew_year - today\n\n\nTime difference of 187 days\n\nmean(data)\n\n\n[1] 14.52412\n\nFinally, you can store the result of these operations in a new\nobject:\n\n\ndecade <- age + 10\n\n\n\nYou may also see objects referred to as variables. There is\na difference between the two in programming terms, however, they are\nused synonymously very frequently.\nLooking after the\nenvironment\nIf you’ve been writing a lot of code you may find that the\nenvironment pane (or workspace) has become cluttered with many objects.\nThis can make it difficult to figure out which object you need and\ntherefore you run the risk of using the wrong data frame. If you’re\nworking on a new dataset, or if you’ve tried lots of different code\nbefore getting the final version, it is good practice to remember to\nclear the environment to avoid using the wrong object. You can do this\nin several ways.\nTo remove individual objects, you can type\nrm(object_name) in the console. Try this now to remove one\nof the objects you created in the previous section.\nTo clear all objects from the environment run\nrm(list = ls()) in the console.\nTo clear all objects from the environment you can also click the broom\nicon in the environment pane.\n\nGlobal options\nBy default, when you open R Studio it will show you what you were\nlast working on, including your code and any objects you have created.\nThis might sound helpful, but actually it tends to cause more problems\nthan it’s worth because it means that you risk accidentally using an old\nversion of an object. We recommend changing the settings so that each\ntime you start R Studio, it opens a fresh copy (see next steps).\n\nR sessions\nWhen you open up R and start writing code, loading packages, and\ncreating objects, you’re doing so in a new session. In addition to\nclearing the workspace, it can sometimes be useful to start a new\nsession. This will happen automatically each time you start R on your\ncomputer. If you find that your code isn’t working and you can’t figure\nout why, it might be worth starting a new session. This will clear the\nenvironment and detach all loaded packages - think of it like restarting\nyour phone.\n\nProjects in RStudio\nAs with most things in life, when it comes to dealing with data and\ndata analysis things are so much simpler if you’re organised. Clear\nproject organisation makes it easier for both you (especially the future\nyou) and your collaborators to make sense of what you’ve done. There’s\nnothing more frustrating than coming back to a project months (sometimes\nyears) later and have to spend days (or weeks) figuring out where\neverything is, what you did and why you did it. A well documented\nproject that has a consistent and logical structure increases the\nlikelihood that you can pick up where you left off with minimal fuss no\nmatter how much time has passed. In addition, it’s much easier to write\ncode to automate tasks when files are well organised and are sensibly\nnamed. This is even more relevant nowadays as it’s never been easier to\ncollect vast amounts of data which can be saved across many separate\ndata files. Lastly, having a well organised project reduces the risk of\nintroducing bugs or errors into your workflow and if they do occur\n(which inevitably they will at some point), it makes it easier to track\ndown these errors and deal with them efficiently.\nThankfully, there are some nice features in R and RStudio that make\nit quite easy to manage a project. There are also a few simple steps you\ncan take right at the start of any project to help keep things\nshipshape.\nA great way of keeping things organised is to use RStudio Projects.\nAn RStudio Project keeps all of your R scripts, R markdown documents, R\nfunctions and data together in one place. The nice thing about RStudio\nProjects is that each project has its own directory, workspace, history\nand source documents so different analyses that you are working on are\nkept completely separate from each other. This means that you can have\nmultiple instances of RStudio open at the same time (if that’s your\nthing) or you can switch very easily between projects without fear of\nthem interfering with each other.\nTo create a project, open RStudio and select\nFile -> New Project... from the menu.\n\nYou can also create a new project by clicking on the ‘Project’ button\nin the top right of RStudio and selecting ‘New Project…’\n\nIn the next window select ‘New Project’.\n\nNow enter the name of the directory you want to create in the\nDirectory name: field. If you want to change the location\nof the directory on your computer click the Browse… button\nand navigate to where you would like to create the directory. Always\ntick the Open in new session box as well. Finally, hit the\nCreate Project to create the new project.\n\nOnce your new project has been created you will now have a new folder\non your computer that contains the RStudio project file\n*.Rproj. This .Rproj file contains various\nproject options (but you shouldn’t really interact with it) and can also\nbe used as a shortcut for opening the project directly from the file\nsystem (just double click on it). You can check this out in the ‘Files’\ntab in RStudio (or in Finder if you’re on a Mac or File Explorer in\nWindows).\n\nThe last thing I suggest you do is select\nTools -> Project Options... from the menu. Click on the\nGeneral tab on the left hand side and then change the\nvalues for Restore .RData into workspace at startup and\nSave workspace to .RData on exit from ‘Default’ to ‘No’.\nThis ensures that every time you open your project you start with a\nclean R session. You don’t have to do this (many people don’t) but I\nprefer to start with a completely clean workspace whenever I open my\nprojects to avoid any potential conflicts with things I have done in\nprevious sessions. The downside to this is that you will need to rerun\nyour R code every time you open you project.\n\nNow that you have an RStudio project set up you can start creating R\nscripts (or R markdown documents) or whatever you need to complete your\nproject. All of the R scripts will now be contained within the RStudio\nproject and saved in the project folder. R scripts and RMarkdown files\nare covered on different page (First steps - Writing)\nWorking directories\nThe working directory is the default location where R will look for\nfiles you want to load and where it will put any files you save. One of\nthe great things about using RStudio Projects is that when you open a\nproject it will automatically set your working directory to the\nappropriate location. You can check the file path of your working\ndirectory by looking at bar at the top of the Console pane.\nYou can also use the getwd() function in the Console\nwhich returns the file path of the current working directory.\nIf you weren’t using an RStudio Project then you would have to set\nyour working directory using the setwd() function at the\nstart of every R script.\nHowever, the problem with setwd() is that it uses an\nabsolute file path which is specific to the computer you are working on.\nIf you want to send your script to someone else (or if you’re working on\na different computer) this absolute file path is not going to work on\nyour friend´s / colleague´s computer as their directory configuration\nwill be different. This results in a project that is not self-contained\nand not easily portable. RStudio solves this problem by allowing you to\nuse relative file paths which are relative to the Root project\ndirectory. The Root project directory is just the directory that\ncontains the .Rproj file. If you want to share your analysis with\nsomeone else, all you need to do is copy the entire project directory\nand send to your to your collaborator. They would then just need to open\nthe project file and any R scripts that contain references to relative\nfile paths will just work.\n\n\n\n",
      "last_modified": "2022-06-28T10:18:32+02:00"
    },
    {
      "path": "firststeps_reproduce.html",
      "title": "R Data Skills",
      "description": "Welcome to the website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\n\nContents\n\n\nIntroduction\nTo get started, we will cover how to use R and\nRStudio as well as some basic programming concepts and\nterminology. We will discuss common pitfalls and, most importantly,\nwhere to get help. Those of you who have no programming experience\nshould find this chapter particularly helpful, however, even if you’ve\nused R (or other languages) before there may be some (I hope)\nhelpful stuff here.\nKeep in mind: You are not expected to memorise all the information -\njust make sure you know what help is available and where you can look up\nthe information you need!\nGetting a feel for R and\nRStudio\nAn important distinction to remember is between the R programming\nlanguage itself, and the software you use to interact with R. You could\nchoose to interact with R directly from the terminal, but that’s\npainful, so most people use an integrated development environment (IDE),\nwhich takes care of a lot of boring tasks for you. For this class, we’ll\nuse the popular Rstudio IDE. To get started, make sure you have both R\nand RStudio installed on your computer. Both are free and open source,\nand for most people they should be straightforward to install.\nInstalling R: Go to the R\nwebsite and download the installer file. Once downloaded, open the\ninstaller file and follow the instructions.\nInstalling RStudio: Go to the RStudio website, and follow the links\nto download RStudio. The version you want is the “RStudio Desktop”. Once\nthe installer is downloaded, open it and follow the\ninstructions.\nR Studio has a console that you can try out code in (appearing as the\nbottom left window in Figure below), there is a script editor (top\nleft), a window showing functions and objects you have created in the\n“environment” tab (top right window in the figure), and a window that\nshows plots, files packages, and help documentation (bottom right).\n\nOne big calculator\nConsole vs. scripts\nWhen you first open up R Studio you won’t see a script like above,\nthere will just be a single pane on the left, which is the console. You\ncan write code in the console to test it out, but it won’t save\nanywhere. For this chapter, we’ll use the console to show you some\nsimple code examples but moving forward you’ll save your code in a\nscript file, and you’ll see the extra pane appear.\nOne of the easiest things you can do with R is use it as a simple\ncalculator, so it’s a good place to start. For instance, try typing 30,\nand hitting enter. When you do this, you’ve entered a command, and R\nwill execute that command. What you see on screen now will be this:\n\n\n10 + 20\n\n\n[1] 30\n\nNot a lot of surprises in this extract. But there’s a few things\nworth talking about, even with such a simple example. Firstly, it’s\nimportant that you understand how to read the extract. In this example,\nwhat I typed was the 10 + 20 part at the top, and the content below is\nwhat R produced.\nBefore we go on to talk about other types of calculations that we can\ndo with R, there’s a few other things I want to point out. The first\nthing is that, while R is good software, it’s still software. It’s\npretty stupid, and because it’s stupid it can’t handle typos. It takes\nit on faith that you meant to type exactly what you did type. For\nexample, suppose that you forgot to hit the shift key when trying to\ntype +, and as a result your command ended up being 10 = 20 rather than\n10 + 20. Here’s what happens:\n\n\n10 = 20\n\n\nError in 10 = 20: invalid (do_set) left-hand side to assignment\n\nWhat’s happened here is that R has attempted to interpret 10 = 20 as\na command, and spits out an error message because the command doesn’t\nmake any sense to it. When a human looks at this, and then looks down at\nhis or her keyboard and sees that + and = are on the same key, it’s\npretty obvious that the command was a typo. But R doesn’t know this, so\nit gets upset. And, if you look at it from its perspective, this makes\nsense. All that R “knows” is that 10 is a legitimate number, 20 is a\nlegitimate number, and = is a legitimate part of the language too. In\nother words, from its perspective this really does look like the user\nmeant to type 10 = 20, since all the individual parts of that statement\nare legitimate and it’s too stupid to realise that this is probably a\ntypo. Therefore, R takes it on faith that this is exactly what you\nmeant… it only “discovers” that the command is nonsense when it tries to\nfollow your instructions, typo and all. And then it whinges, and spits\nout an error.\nEven more subtle is the fact that some typos won’t produce errors at\nall, because they happen to correspond to “well-formed” R commands. For\ninstance, suppose that not only did I forget to hit the shift key when\ntrying to type 10 + 20, I also managed to press the key next to one I\nmeant do. The resulting typo would produce the command 10 - 20. Clearly,\nR has no way of knowing that you meant to add 20 to 10, not subtract 20\nfrom 10, so what happens this time is this:\n\n\n10 - 20\n\n\n[1] -10\n\nIn this case, R produces the right answer, but to the the wrong\nquestion.\nTo some extent, I’m stating the obvious here, but it’s important. The\npeople who wrote R are smart. You, the user, are smart. But R itself is\ndumb. And because it’s dumb, it has to be mindlessly obedient. It does\nexactly what you ask it to do. There is no equivalent to “autocorrect”\nin R, and for good reason. When doing advanced stuff – and even the\nsimplest of statistics is pretty advanced in a lot of ways – it’s\ndangerous to let a mindless automaton like R try to overrule the human\nuser. But because of this, it’s your responsibility to be careful.\nAlways make sure you type exactly what you mean. When dealing with\ncomputers, it’s not enough to type “approximately” the right thing. In\ngeneral, you absolutely must be precise in what you say to R … like all\nmachines it is too stupid to be anything other than absurdly literal in\nits interpretation.\nCommands and spacing\nOf course, now that I’ve been so uptight about the importance of\nalways being precise, I should point out that there are some exceptions.\nOr, more accurately, there are some situations in which R does show a\nbit more flexibility than my previous description suggests. The first\nthing R is smart enough to do is ignore redundant spacing. What I mean\nby this is that, when I typed 10 + 20 before, I could equally have done\nthis\n\n\n10        + 20\n\n\n[1] 30\n\nand get exactly the same answer. However, that doesn’t mean that you\ncan insert spaces in any old place. For instance, when you open up R it\nsuggests that you type citation() to get some information about how to\ncite R:\n\n\ncitation()\n\n\n\nTo cite R in publications use:\n\n  R Core Team (2022). R: A language and environment for\n  statistical computing. R Foundation for Statistical\n  Computing, Vienna, Austria. URL https://www.R-project.org/.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2022},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R,\nplease cite it when using it for data analysis. See also\n'citation(\"pkgname\")' for citing R packages.\n\nOkay, that’s good to know. Let’s see what happens when I try changing\nthe spacing. If I insert spaces in between the word and the parentheses,\nor inside the parentheses themselves, then all is well. But insertinhg\nspaces in the middle of the commands, not so much. Try these three just\nto see:\ncitation () # works!\ncitation( ) # works!\ncita tion() # doesn’t work\nR knows you’re not finished?\nOne more thing I should point out. If you hit enter in a situation\nwhere it’s “obvious” to R that you haven’t actually finished typing the\ncommand, R is just smart enough to keep waiting. For example, if you\ntype 10 + and then press enter, even R is smart enough to realise that\nyou probably wanted to type in another number. So here’s what\nhappens:\n\n10 +\nError: <text>:2:0: unexpected end of input\n1: 10 +\n   ^\n\nand there’s a blinking cursor next to the plus sign. What this means\nis that R is still waiting for you to finish. It “thinks” you’re still\ntyping your command, so it hasn’t tried to execute it yet. In other\nwords, this plus sign is actually another command prompt. It’s different\nfrom the usual one (i.e., the > symbol) to remind you that R is going\nto “add” whatever you type now to what you typed last time. For example,\nif I then go on to type 20 and hit enter, what I get is this:\n\n\n10 +\n+ 20\n\n\n[1] 30\n\nAnd as far as R is concerned, this is exactly the same as if you had\ntyped 10 + 20. Similarly, consider the citation() command that we talked\nabout in the previous section. Suppose you hit enter after typing\ncitation(. Once again, R is smart enough to realise that there must be\nmore coming – since you need to add the ) character – so it waits. I can\neven hit enter several times and it will keep waiting:\n\ncitation( \n+\n+\n+)\n\nSometimes when doing this, you’ll eventually get yourself in trouble\n(it happens to us all). Maybe you start typing a command, and then you\nrealise you’ve screwed up. For example,\n\ncitblation(\n+\n+\n\nYou’d probably prefer R not to try running this command, right? If\nyou want to get out of this situation, just hit the escape key. R will\nreturn you to the normal command prompt (i.e. >) without attempting\nto execute the botched command.\nThat being said, it’s not often the case that R is smart enough to\ntell that there’s more coming. For instance, in the same way that I\ncan’t add a space in the middle of a word, I can’t hit enter in the\nmiddle of a word either. If I hit enter after typing citat I\nget an error, because R thinks I’m interested in something called citat\nand can’t find it:\n\n\ncitat\n\n\n\nWhat about if I typed citation and hit enter, without the\nparentheses? In this case we get something very odd, something that we\ndefinitely don’t want, at least not at this stage. Here’s what\nhappens:\n\n\ncitation\n\n\nfunction (package = \"base\", lib.loc = NULL, auto = NULL) \n{\n    if (!is.null(auto) && !is.logical(auto) && !anyNA(match(c(\"Package\", \n        \"Version\", \"Title\"), names(meta <- as.list(auto)))) && \n        !all(is.na(match(c(\"Authors@R\", \"Author\"), names(meta))))) {\n        auto_was_meta <- TRUE\n        package <- meta$Package\n    }\n    else {\n        auto_was_meta <- FALSE\n        dir <- system.file(package = package, lib.loc = lib.loc)\n        if (dir == \"\") \n            stop(packageNotFoundError(package, lib.loc, sys.call()))\n        meta <- packageDescription(pkg = package, lib.loc = dirname(dir))\n        citfile <- file.path(dir, \"CITATION\")\n        test <- file_test(\"-f\", citfile)\n        if (!test) {\n            citfile <- file.path(dir, \"inst\", \"CITATION\")\n            test <- file_test(\"-f\", citfile)\n        }\n        if (is.null(auto)) \n            auto <- !test\n        if (!auto) {\n            return(readCitationFile(citfile, meta))\n        }\n    }\n    if ((!is.null(meta$Priority)) && (meta$Priority == \"base\")) {\n        cit <- citation(\"base\", auto = FALSE)\n        attr(cit, \"mheader\")[1L] <- paste0(\"The \", sQuote(package), \n            \" package is part of R.  \", attr(cit, \"mheader\")[1L])\n        return(.citation(cit, package))\n    }\n    year <- sub(\"-.*\", \"\", meta$`Date/Publication`)\n    if (!length(year)) {\n        if (is.null(meta$Date)) {\n            warning(gettextf(\"no date field in DESCRIPTION file of package %s\", \n                sQuote(package)), domain = NA)\n        }\n        else {\n            date <- trimws(as.vector(meta$Date))[1L]\n            date <- strptime(date, \"%Y-%m-%d\", tz = \"GMT\")\n            if (!is.na(date)) \n                year <- format(date, \"%Y\")\n        }\n    }\n    if (!length(year)) {\n        date <- as.POSIXlt(sub(\";.*\", \"\", trimws(meta$Packaged)[1L]))\n        if (!is.na(date)) \n            year <- format(date, \"%Y\")\n    }\n    if (!length(year)) {\n        warning(gettextf(\"could not determine year for %s from package DESCRIPTION file\", \n            sQuote(package)), domain = NA)\n        year <- NA_character_\n    }\n    author <- meta$`Authors@R`\n    if (length(author)) {\n        aar <- .read_authors_at_R_field(author)\n        author <- Filter(function(e) {\n            !(is.null(e$given) && is.null(e$family)) && !is.na(match(\"aut\", \n                e$role))\n        }, aar)\n        if (!length(author)) \n            author <- Filter(function(e) {\n                !(is.null(e$given) && is.null(e$family)) && !is.na(match(\"cre\", \n                  e$role))\n            }, aar)\n    }\n    if (length(author)) {\n        has_authors_at_R_field <- TRUE\n    }\n    else {\n        has_authors_at_R_field <- FALSE\n        author <- as.personList(meta$Author)\n    }\n    z <- list(title = paste0(package, \": \", meta$Title), author = author, \n        year = year, note = paste(\"R package version\", meta$Version))\n    if (identical(meta$Repository, \"CRAN\")) \n        z$url <- sprintf(\"https://CRAN.R-project.org/package=%s\", \n            package)\n    if (identical(meta$Repository, \"R-Forge\")) {\n        z$url <- if (!is.null(rfp <- meta$\"Repository/R-Forge/Project\")) \n            sprintf(\"https://R-Forge.R-project.org/projects/%s/\", \n                rfp)\n        else \"https://R-Forge.R-project.org/\"\n        if (!is.null(rfr <- meta$\"Repository/R-Forge/Revision\")) \n            z$note <- paste(z$note, rfr, sep = \"/r\")\n    }\n    if (!length(z$url) && !is.null(url <- meta$URL)) {\n        if (grepl(\"[, ]\", url)) \n            z$note <- url\n        else z$url <- url\n    }\n    header <- if (!auto_was_meta) {\n        gettextf(\"To cite package %s in publications use:\", sQuote(package))\n    }\n    else NULL\n    footer <- if (!has_authors_at_R_field && !auto_was_meta) {\n        gettextf(\"ATTENTION: This citation information has been auto-generated from the package DESCRIPTION file and may need manual editing, see %s.\", \n            sQuote(\"help(\\\"citation\\\")\"))\n    }\n    else NULL\n    author <- format(z$author, include = c(\"given\", \"family\"))\n    if (length(author) > 1L) \n        author <- paste(paste(head(author, -1L), collapse = \", \"), \n            tail(author, 1L), sep = \" and \")\n    rval <- bibentry(bibtype = \"Manual\", textVersion = paste0(author, \n        \" (\", z$year, \"). \", z$title, \". \", z$note, \". \", z$url), \n        header = header, footer = footer, other = z)\n    .citation(rval, package)\n}\n<bytecode: 0x7f909770dee0>\n<environment: namespace:utils>\n\nwhere the BLAH BLAH BLAH goes on for rather a long time, and you\ndon’t know enough R yet to understand what all this gibberish actually\nmeans. This incomprehensible output can be quite intimidating to novice\nusers, and unfortunately it’s very easy to forget to type the\nparentheses; so almost certainly you’ll do this by accident. Do not\npanic when this happens. Simply ignore the gibberish.\nOperations\nOkay, now that we’ve discussed some of the tedious details associated\nwith typing R commands, let’s get back to learning how to use the most\npowerful piece of statistical software in the world as a $2 calculator.\nSo far, all we know how to do is addition. Clearly, a calculator that\nonly did addition would be a bit stupid, so I should tell you about how\nto perform other simple calculations using R. But first, some more\nterminology. Addition is an example of an “operation” that you can\nperform (specifically, an arithmetic operation), and the operator that\nperforms it is +. To people with a programming or mathematics\nbackground, this terminology probably feels pretty natural, but to other\npeople it might feel like I’m trying to make something very simple\n(addition) sound more complicated than it is (by calling it an\narithmetic operation). To some extent, that’s true: if addition was the\nonly operation that we were interested in, it’d be a bit silly to\nintroduce all this extra terminology. However, as we go along, we’ll\nstart using more and more different kinds of operations, so it’s\nprobably a good idea to get the language straight now, while we’re still\ntalking about very familiar concepts like addition!\nArithmetic operations\nSo, now that we have the terminology, let’s learn how to perform some\narithmetic operations. R has operators that correspond to the basic\narithmetic we learned in primary school: addition is +, subtraction is\n-, multiplication is * and division ia /. As you can see, R uses fairly\nstandard symbols to denote each of the different operations you might\nwant to perform: if I wanted to find out what 57 times 61 is (and who\nwouldn’t?), I can use R instead of a calculator, like so:\n\n\n57 * 61\n\n\n[1] 3477\n\nSo that’s handy.\nThere are three other arithmetic operations that I should probably\nmention: taking powers, doing integer division, and calculating a\nmodulus. Of the three, the only one that is of any real importance for\nthe purposes of this book is taking powers, so I’ll discuss that one\nhere: the other two are discussed later.\nType in the complete multiplication?\n\n\n5*5*5*5\n\n\n[1] 625\n\nbut that does seem a bit tedious. It would be very annoying indeed if\nyou wanted to calculate \\(5^{15}\\),\nsince the command would end up being quite long. Therefore, to make our\nlives easier, we use the power operator instead. When we do that, our\ncommand to calculate \\(5^{4}\\) goes\nlike this:\n\n\n5^4\n\n\n[1] 625\n\nMuch easier.\nOrder of operations\nOkay. At this point, you know how to take one of the most powerful\npieces of statistical software in the world, and use it as a calculator.\nAnd as a bonus, you’ve learned a few very basic programming concepts.\nThat’s not nothing (you could argue that you’ve just saved yourself) but\non the other hand, it’s not very much either. In order to use R more\neffectively, we need to introduce more programming concepts.\nIn most situations where you would want to use a calculator, you\nmight want to do multiple calculations. R lets you do this, just by\ntyping in longer commands. In fact, we’ve already seen an example of\nthis earlier, when I typed in 5 * 5 * 5 * 5. However, let’s try a\nslightly different example:\n\n\n1 + 2 * 4 \n\n\n[1] 9\n\nClearly, this isn’t a problem for R either. However, it’s worth\nstopping for a second, and thinking about what R just did. Clearly,\nsince it gave us an answer of 9 it must have multiplied 2 * 4 (to get an\ninterim answer of 8) and then added 1 to that. But, suppose it had\ndecided to just go from left to right: if R had decided instead to add\n1+2 (to get an interim answer of 3) and then multiplied by 4, it would\nhave come up with an answer of 12\nTo answer this, you need to know the order of operations that R\nuses.3 If you remember back to your high school maths classes, it’s\nactually the same order that you got taught when you were at school: the\nBEDMAS order. Thatis, first calculate things inside Brackets, then\ncalculate Exponents, then Division and Multiplication, then Addition and\nSubtraction. So, to continue the example above, if we want to force R to\ncalculate the 1 + 2 part before the multiplication, all we would have to\ndo is enclose it in brackets:\n\n\n(1 + 2) * 4\n\n\n[1] 12\n\nThis is a fairly useful thing to be able to do. The only other thing\nI should point out about order of operations is what to expect when you\nhave two operations that have the same priority: that is, how does R\nresolve ties? For instance, multiplication and division are actually the\nsame priority, but what should we expect when we give R a problem like 4\n/ 2 * 3 to solve? If it evaluates the multiplication first and then the\ndivision, it would calculate a value of two-thirds. But if it evaluates\nthe division first it calculates a value of siz. The answer, in this\ncase, is that R goes from left to right, so in this case the division\nstep would come first:\n\n\n4 / 2 * 3\n\n\n[1] 6\n\nAll of the above being said, it’s helpful to remember that brackets\nalways come first. So, if you’re ever unsure about what order R will do\nthings in, an easy solution is to enclose the thing you want it to do\nfirst in brackets. There’s nothing stopping you from typing (4 / 2) * 3.\nBy enclosing the division in brackets we make it clear which thing is\nsupposed to happen first. In this instance you wouldn’t have needed to,\nsince R would have done the division first anyway, but when you’re first\nstarting out it’s better to make sure R does what you want!\nFunctions\nThe symbols +, -, * and so on are examples of operators. As we’ve\nseen, you can do quite a lot of calculations just by using these\noperators. However, in order to do more advanced calculations (and later\non, to do actual statistics), you’re going to need to start using\nfunctions. I’ll talk in more detail about functions and how they work\nlater, but for now let’s just dive in and use a few. To get started,\nsuppose I wanted to take the square root of 225. There’s two ways I\ncould do this using R. Firstly, since the square root of 255 is the same\nthing as raising 225 to the power of 0.5, I could use the power operator\n^, just like we did earlier:\n\n\n225 ^ 0.5\n\n\n[1] 15\n\nHowever, there’s a second way to do this by using square root\nfunction sqrt.\nUsing functions\nTo calculate the square root of 255 using the sqrt function, the\ncommand I type is this:\n\n\nsqrt(225)\n\n\n[1] 15\n\nWhen we use a function to do something, we generally refer to this as\ncalling the function, and the values that we type into the function\n(there can be more than one) are referred to as the arguments of that\nfunction.\nObviously, the sqrt function doesn’t really give us any new\nfunctionality, since we already knew how to do square root calculations\nby using the power operator ^. However, there are lots of other\nfunctions in R: in fact, almost everything of interest that I’ll talk\nabout in this book is an R function of some kind. For example, one\nfunction that comes in handy quite often is the absolute value function.\nCompared to the square root function, it’s extremely simple: it just\nconverts negative numbers to positive numbers, and leaves positive\nnumbers alone. Calculating absolute values in R is pretty easy, since R\nprovides the abs function that you can use for this purpose. For\ninstance:\n\n\nabs(-13)\n\n\n[1] 13\n\nCombining functions\nBefore moving on, it’s worth noting that, in the same way that R\nallows us to put multiple operations together into a longer command\n(like 1 + 2 * 4 for instance), it also lets us put functions together\nand even combine functions with operators if we so desire. For example,\nthe following is a perfectly legitimate command:\n\n\nsqrt(1 + abs(-8))\n\n\n[1] 3\n\nWhen R executes this command, starts out by calculating the value of\nabs(-8), which produces an intermediate value of 8. Having done so, the\ncommand simplifies to sqrt(1 + 8). To solve the square root5 it first\nneeds to add 1 + 8 to get 9, at which point it evaluates sqrt(9), and so\nit finally outputs a value of 3.\nMultiple arguments\nThere’s two more fairly important things that you need to understand\nabout how functions work in R, and that’s the use of “named” arguments,\nand default values” for arguments. Not surprisingly, that’s not to say\nthat this is the last we’ll hear about how functions work, but they are\nthe last things we desperately need to discuss in order to get you\nstarted. To understand what these two concepts are all about, I’ll\nintroduce another function. The round function can be used to round some\nvalue to the nearest whole number. For example, I could type this:\n\n\nround(3.1415)\n\n\n[1] 3\n\nPretty straightforward, really. However, suppose I only wanted to\nround it to two decimal places: that is, I want to get 3.14 as the\noutput. The round function supports this, by allowing you to input a\nsecond argument to the function that specifies the number of decimal\nplaces that you want to round the number to. In other words, I could do\nthis:\n\n\nround(3.14165, 2)\n\n\n[1] 3.14\n\nWhat’s happening here is that I’ve specified two arguments: the first\nargument is the number that needs to be rounded (i.e., 3.1415), the\nsecond argument is the number of decimal places that it should be\nrounded to (i.e., 2), and the two arguments are separated by a\ncomma.\nArgument names\nIn this simple example, it’s not too hard to remember which argument\ncomes first and which one comes second, but as you might imagine it\nstarts to get very difficult once you start using complicated functions\nthat have lots of arguments. Fortunately, most R functions use argument\nnames to make your life a little easier. For the round function, for\nexample the number that needs to be rounded is specified using the x\nargument, and the number of decimal points that you want it rounded to\nis specified using the digits argument. Because we have these names\navailable to us, we can specify the arguments to the function by name.\nWe do so like this:\n\n\nround(x = 3.1415, digits = 2)\n\n\n[1] 3.14\n\nNotice that this is kind of similar in spirit to variable assignment,\nexcept that I used = here, rather than <-. In both cases we’re\nspecifying specific values to be associated with a label. However, there\nare some differences between what I was doing earlier on when creating\nvariables, and what I’m doing here when specifying arguments, and so as\na consequence it’s important that you use = in this context.\nAs you can see, specifying the arguments by name involves a lot more\ntyping, but it’s also a lot easier to read. Because of this, the\ncommands in this book will usually specify arguments by name,6 since\nthat makes it clearer to you what I’m doing. However, one important\nthing to note is that when specifying the arguments using their names,\nit doesn’t matter what order you type them in. But if you don’t use the\nargument names, then you have to input the arguments in the correct\norder. In other words, these three commands all produce the same\noutput…\n\n\nround(3.14165, 2)\n\n\n[1] 3.14\n\nround(x = 3.1415, digits = 2)\n\n\n[1] 3.14\n\nround(digits = 2, x = 3.1415)\n\n\n[1] 3.14\n\nbut this one does not…\n\n\nround(2, 3.14165)\n\n\n[1] 2\n\nDefault values\nOkay, so that’s the first thing I said you’d need to know: argument\nnames. The second thing you need to know about is default values. Notice\nthat the first time I called the round function I didn’t actually\nspecify the digits argument at all, and yet R somehow knew that this\nmeant it should round to the nearest whole number. How did that happen?\nThe answer is that the digits argument has a default value of 0, meaning\nthat if you decide not to specify a value for digits then R will act as\nif you had typed digits = 0. This is quite handy: most of the time when\nyou want to round a number you want to round it to the nearest whole\nnumber, and it would be pretty annoying to have to specify the digits\nargument every single time. On the other hand, sometimes you actually do\nwant to round to something other than the nearest whole number, and it\nwould be even more annoying if R didn’t allow this! Thus, by having\ndigits = 0 as the default value, we get the best of both worlds.\nNext steps\nFunctions and arguments\nFunctions in R execute specific tasks and normally take a number of\narguments (if you’re into linguistics you might want to think as these\nas verbs that require a subject and an object). You can look up all the\narguments that a function takes by using the help documentation by using\nthe format ?function. Some arguments are required, and some are\noptional. Optional arguments will often use a default (normally\nspecified in the help documentation) if you do not enter any value.\nAs an example, let’s look at the help documentation for the function\nrnorm() which randomly generates a set of numbers with a normal\ndistribution.\nActivity 1\nOpen up R Studio and in the console, type the following code:\n\n\n?rnorm\n\n\n\nThe help documentation for rnorm() should appear in the bottom right\nhelp panel. In the usage section, we see that rnorm() takes the\nfollowing form:\n\n\nrnorm(n, mean = 0, sd = 1)\n\n\n\nIn the arguments section, there are explanations for each of the\narguments. n is the number of observations we want to create, mean is\nthe mean of the data points we will create and sd is the standard\ndeviation of the set. In the details section it notes that if no values\nare entered for mean and sd it will use a default of 0 and 1 for these\nvalues. Because there is no default value for n it must be specified\notherwise the code won’t run.\nLet’s try an example and just change the required argument n to ask R\nto produce 5 random numbers.\nActivity 2\nCopy and paste the following code into the console.\n\n\nset.seed(12042016)\nrnorm(n = 5)\n\n\n\nThese numbers have a mean of 0 and an SD of 1. Now we can change the\nadditional arguments to produce a different set of numbers.\n\n\nrnorm(n = 5, mean = 10, sd = 2)\n\n\n\nThis time R has still produced 5 random numbers, but now this set of\nnumbers has a mean of 10 and an sd of 2 as specified. Always remember to\nuse the help documentation to help you understand what arguments a\nfunction requires.\n\n\n\n",
      "last_modified": "2022-06-28T10:18:36+02:00"
    },
    {
      "path": "firststeps_writing.html",
      "title": "First steps",
      "description": "Writing\n",
      "author": [],
      "contents": "\n\nContents\n\n\nR scripts\nSo far you’ve been using the console to run code. That’s a great\nplace to start, but you’ll find it gets cramped pretty quickly as you\ncreate more complex ggplot2 graphics and dplyr pipes. To give yourself\nmore room to work, it’s a great idea to use the script editor. Open it\nup either by clicking the File menu, and selecting New File, then R\nscript, or using the keyboard shortcut\nCmd/Ctrl + Shift + N. Now you’ll see four panes:\n\nThe script editor is a great place to put code you care about. Keep\nexperimenting in the console, but once you have written code that works\nand does what you want, put it in the script editor. RStudio will\nautomatically save the contents of the editor when you quit RStudio, and\nwill automatically load it when you re-open. Nevertheless, it’s a good\nidea to save your scripts regularly and to back them up.\nRunning code\nThe script editor is also a great place to build up complex ggplot2\nplots or long sequences of dplyr manipulations. The key to using the\nscript editor effectively is to memorise one of the most important\nkeyboard shortcuts: Cmd/Ctrl + Enter. This executes the current R\nexpression in the console. For example, take the code below. If your\ncursor is at █, pressing Cmd/Ctrl + Enter will run the\ncomplete command that generates not_cancelled. It will also\nmove the cursor to the next statement (beginning with\nnot_cancelled %>%). That makes it easy to run your\ncomplete script by repeatedly pressing\nCmd/Ctrl + Enter.\n\nlibrary(dplyr)\nlibrary(nycflights13)\n\nnot_cancelled <- flights %>% \n  filter(!is.na(dep_delay)█, !is.na(arr_delay))\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay))Copy\n\nInstead of running expression-by-expression, you can also execute the\ncomplete script in one step: Cmd/Ctrl + Shift + S. Doing\nthis regularly is a great way to check that you’ve captured all the\nimportant parts of your code in the script.\nI recommend that you always start your script with the packages that\nyou need. That way, if you share your code with others, they can easily\nsee what packages they need to install. Note, however, that you should\nnever include install.packages() or setwd() in\na script that you share. It’s very antisocial to change settings on\nsomeone else’s computer!\nWhen working through future chapters, I highly recommend starting in\nthe editor and practicing your keyboard shortcuts. Over time, sending\ncode to the console in this way will become so natural that you won’t\neven think about it.\nRStudio diagnostics\nThe script editor will also highlight syntax errors with a red\nsquiggly line and a cross in the sidebar:\n\nHover over the cross to see what the problem is:\n\nRStudio will also let you know about potential problems:\n\nRMarkdown\nRMarkdown is a simple and easy to use plain text language used to\ncombine your R code, results from your data analysis (including plots\nand tables) and written commentary into a single nicely formatted and\nreproducible document (like a report, publication, thesis chapter or a\nweb page like this one).\nTechnically, RMarkdown is a variant of another language (yet another\nlanguage!) called Markdown and both are a type of ‘markup’ language. A\nmarkup language simply provides a way of creating an easy to read plain\ntext file which can incorporate formatted text, images, headers and\nlinks to other documents. Actually, if it makes you feel any better all\nof you will have been exposed to a markup language before, as most of\nthe internet content you digest every day is underpinned by a markup\nlanguage called HTML (Hypertext Markup Language). Anyway, the main point\nis that RMarkdown is very easy to learn and when used with RStudio it’s\nridiculously easy to integrate into your workflow to produce feature\nrich content (so why wouldn’t you?!).\nWhy use RMarkdown?\nConducting your research in a robust and reproducible manner can have\nthree main advantages:\nimprove the overall quality\nmake the content you produce easily shareable\nfacilitate open science.\nIn a nutshell, open science is about doing all we can to make our\ndata, methods, results and inferences transparent and available to\neveryone. Some of the main tenets of open science are described here and\ninclude:\nTransparency in methodology, observation, collection of data and\nanalytical methods.\nPublic availability and re-usability of scientific data\nPublic accessibility and transparency of scientific\ncommunication\nUsing web-based tools to facilitate scientific collaboration\nSoon, all of you will (hopefully) be using R to explore and analyze\ndata. Thus, you’ll already be on the road to making your analysis more\nreproducible, transparent and shareable. However, perhaps your current\nworkflow looks something like this:\nInstalling RMarkdown\nTo use RMarkdown you will first need to install the\nrmarkdown package in RStudio (or in the R console if you’re\nnot using RStudio) and any package dependencies.\n\n\ninstall.packages(\"RMarkdown\")\n\n\n\nIf you would like to create pdf documents (or MS Word documents) from\nyour RMarkdown file you will also need to install a version of\nLaTeX on your computer. If you’ve not installed LaTeX\nbefore, I recommend that you install TinyTeX. Instructions on\nhow to do this can be found here.\nOpen and save a new\nRMarkdown document\nTo open a new RMarkdown document click File ->\nNew File -> R Markdown.... You will be\nprompted to give it a title. Once you’ve opened that new document be\nsure to save it by clicking File ->\nSave as.\n\nYou will notice that when your new RMarkdown document is created it\nincludes some example RMarkdown code. Normally you would just highlight\nand delete everything in the document except the information at the top\nbetween the — delimiters (this is called the YAML header which we will\ndiscuss in a bit) and then start writing your own code. However, just\nfor now we will use this document to practice converting RMarkdown to\nboth html and pdf formats and check everything is working.\n\nOnce you’ve created your RMarkdown document it’s good practice to\nsave this file somewhere convenient. You can do this by selecting\nFile -> Save from RStudio menu (or use the\nkeyboard shortcut ctrl + s on Windows or\ncmd + s on a Mac) and enter an appropriate file name.\nNotice the file extension of your new RMarkdown file is .Rmd.\nNow, to convert your .Rmd file to a HTML document click on the little\nblack triangle next to the Knit icon at the top of the\nsource window and select knit to HTML.\n RStudio will now ‘knit’ (or\nrender) your .Rmd file into a HTML file. Notice that there is a new\nRMarkdown tab in your console window which provides you with information\non the rendering process and will also display any errors if something\ngoes wrong.\nIf everything went smoothly a new HTML file will have been created\nand saved in the same directory as your .Rmd file. To view this document\nsimply double click on the file to open in a browser (like Chrome or\nFirefox) to display the rendered content. RStudio will also display a\npreview of the rendered file in a new window for you to check out (your\nwindow might look slightly different if you’re using a Windows\ncomputer).\nGreat, you’ve just rendered your first RMarkdown document. If you\nwant to knit your .Rmd file to a pdf document then all you need to do is\nchoose knit to PDF instead of knit to HTML when you click on the knit\nicon. This will create a file which you can double click to open. Give\nit a go!\nYou can also knit an .Rmd file using the command line in the console\nrather than by clicking on the knit icon. To do this, just use the\nrender() function from the rmarkdown package\nas shown below. Again, you can change the output format using the\noutput_format = argument as well as many other options.\n\n\n\n\n",
      "last_modified": "2022-06-28T10:18:38+02:00"
    },
    {
      "path": "firststeps.html",
      "title": "First steps",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\nIntroduction\nTo get started, we will cover how to use R and\nRStudio as well as some basic programming concepts and\nterminology. We will discuss common pitfalls and, most importantly,\nwhere to get help. Those of you who have no programming experience\nshould find this chapter particularly helpful, however, even if you’ve\nused R (or other languages) before there may be some (I hope)\nhelpful stuff here.\nKeep in mind: You are not expected to memorise all the information -\njust make sure you know what help is available and where you can look up\nthe information you need!\nGetting a feel for R and\nRStudio\nAn important distinction to remember is between the R programming\nlanguage itself, and the software you use to interact with R. You could\nchoose to interact with R directly from the terminal, but that’s\npainful, so most people use an integrated development environment (IDE),\nwhich takes care of a lot of boring tasks for you. We’ll use the popular\nRstudio IDE. To get started, make sure you have both R and RStudio\ninstalled on your computer. Both are free and open source, and for most\npeople they should be straightforward to install.\nInstalling R: Go to the R\nwebsite and download the installer file. Once downloaded, open the\ninstaller file and follow the instructions.\nInstalling RStudio: Go to the RStudio website, and follow the links\nto download RStudio. The version you want is the “RStudio Desktop”. Once\nthe installer is downloaded, open it and follow the\ninstructions.\nR Studio has a console that you can try out code in (appearing as the\nbottom left window in Figure below), there is a script editor (top\nleft), a window showing functions and objects you have created in the\n“environment” tab (top right window in the figure), and a window that\nshows plots, files packages, and help documentation (bottom right).\n\nHelp\nGetting good at programming really means getting good trying stuff\nout, searching for help online, and finding examples of code to copy.\nLearning to problem-solve effectively is a key skill that you need to\ndevelop over time.\nUse the help documentation. If you’re struggling to understand how a\nfunction works, remember the ?function command. If you get\nan error message, copy and paste it in to Google - it’s very likely\nsomeone else has had the same problem.\nIn addition to these course materials there are a number of excellent\nresources for learning R:\nR Cookbook\nStackOverflow\nR for Data Science\nSearch or use the #rstats hashtag on Twitter\nDebugging tips\nA large part of coding is trying to figure why your code doesn’t\nwork and this is true whether you are a novice or an expert. As you\nprogress you should keep a record of mistakes you make and how you fixed\nthem.\nHave you loaded the correct packages for the functions you are\ntrying to use? One very common mistake is to write the code to load the\npackage, e.g., library(tidyverse) but then forget to run it.\nHave you made a typo? Remember data is not the same as DATA and\nt.test is not the same as t_test.\nIs there a package conflict? Have you tried specifying the\npackage and function with package::function?\nIs it definitely an error? Not all red text in R means an error -\nsometimes it is just giving you a message with information.\n\n\n\n",
      "last_modified": "2022-06-28T10:18:39+02:00"
    },
    {
      "path": "index.html",
      "title": "How R You?",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet\nThis page is still under construction. May take some time…\n\n\n\n\n",
      "last_modified": "2022-06-28T10:18:41+02:00"
    },
    {
      "path": "programming_functions.html",
      "title": "How R You?",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet\nThis page is still under construction. May take some time…\n\n\n\n\n",
      "last_modified": "2022-06-28T10:18:43+02:00"
    },
    {
      "path": "programming_git.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:45+02:00"
    },
    {
      "path": "programming_ifelse.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:46+02:00"
    },
    {
      "path": "programming_loops.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:47+02:00"
    },
    {
      "path": "programming.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:49+02:00"
    },
    {
      "path": "viz_advanced.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:50+02:00"
    },
    {
      "path": "viz_customization.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:18:52+02:00"
    },
    {
      "path": "viz_grammar.html",
      "title": "Visualisation",
      "description": "ggplot2: grammar of graphics\n",
      "author": [],
      "contents": "\n\nContents\n\n\n\n\n\nIntroduction\nBeing able to visualize our variables, and relationships between our\nvariables, is a very useful skill. Before we do any statistical analyses\nor present any summary statistics, we should visualize our data as it\nis:\nA quick and easy way to check our data make sense, and to\nidentify any unusual trends.\nA way to honestly present the features of our data to anyone who\nreads our research.\nggplot() builds plots by combining layers (see below).\nIf you’re used to making plots in Excel this might seem a bit odd at\nfirst, however, it means that you can customize each layer and R is\ncapable of making very complex and beautiful figures (this website gives you a good sense\nof what’s possible).\n\nPreparing the data\nAs we will again use the data from the positive psychology\nreplication study, we´ll quickly re-run import,\ninner_join() and selection of variables of interest.\n\n\ndat <- read_csv (\"datasets/positive_psychology/ahi-cesd.csv\") # import intervention data\npinfo <- read_csv(\"datasets/positive_psychology/participant-info.csv\") # import participant data\n\nall_dat <- inner_join(x = dat,                      # the first table you want to join\n                      y = pinfo,                    # the second table you want to join\n                      by = c(\"id\", \"intervention\")) # columns the two tables have in common\n\nsummarydata <- select(.data = all_dat, \n                      ahiTotal, \n                      cesdTotal, \n                      sex, \n                      age, \n                      educ, \n                      income, \n                      occasion,\n                      elapsed.days)\n\n\n\nBefore we go any further we need to perform an additional step of\ndata processing that we have glossed over up until this point. First,\nrun the below code to look at the structure of the dataset:\n\n\nhead(summarydata)\n\n\n# A tibble: 6 × 8\n  ahiTotal cesdTotal   sex   age  educ income occasion elapsed.days\n     <dbl>     <dbl> <dbl> <dbl> <dbl>  <dbl>    <dbl>        <dbl>\n1       63        14     2    35     5      3        0         0   \n2       73         6     2    35     5      3        1        11.8 \n3       73         7     1    59     1      1        0         0   \n4       89        10     1    59     1      1        1         8.02\n5       89        13     1    59     1      1        2        14.3 \n6       93         8     1    59     1      1        3        32.0 \n\nstr(summarydata)\n\n\ntibble [992 × 8] (S3: tbl_df/tbl/data.frame)\n $ ahiTotal    : num [1:992] 63 73 73 89 89 93 80 77 77 85 ...\n $ cesdTotal   : num [1:992] 14 6 7 10 13 8 15 12 3 5 ...\n $ sex         : num [1:992] 2 2 1 1 1 1 1 1 1 1 ...\n $ age         : num [1:992] 35 35 59 59 59 59 59 59 51 51 ...\n $ educ        : num [1:992] 5 5 1 1 1 1 1 1 4 4 ...\n $ income      : num [1:992] 3 3 1 1 1 1 1 1 3 3 ...\n $ occasion    : num [1:992] 0 1 0 1 2 3 4 5 0 2 ...\n $ elapsed.days: num [1:992] 0 11.77 0 8.02 14.3 ...\n\nR assumes that all the variables are numeric (represented by\nnum) and this is going to be a problem because whilst\nsex, educ, and income are\nrepresented by numerical codes, they aren’t actually numbers, they’re\ncategories, or factors.\nWe need to tell R that these variables are factors and\nwe can use mutate() to do this by overriding the original\nvariable with the same data but classified as a factor.\n\n\nsummarydata <- summarydata %>%\n  mutate(sex = as.factor(sex),\n         educ = as.factor(educ),\n         income = as.factor(income))\n\n\n\nYou can read this code as “overwrite the data that is in the column\nsex with sex as a factor”.\nRemember this. It’s a really important step and if your graphs\nare looking weird this might be the reason.\nExample: Bar plot\nFor our first example we will recreate the bar plot showing the\nnumber of male and female participants from loading data by showing you\nhow the layers of code build up.\nThe first line (or layer) sets up the base of the graph: the data\nto use and the aesthetics (what will go on the x and y axis, how the\nplot will be grouped).\naes() can take both an x and\ny argument, however, with a bar plot you are just asking R\nto count the number of data points in each group so you don’t need to\nspecify this.\n\n\nggplot(data = summarydata, aes(x = sex))\n\n\n\n\nThe next layer adds a geom or a shape, in\nthis case we use geom_bar() as we want to draw a bar\nplot.\n\n\nggplot(summarydata, aes(x = sex)) +\n  geom_bar()\n\n\n\n\nAdding fill to the first layer will separate the data\ninto each level of the grouping variable and give it a\ndifferent colour. In this case, there is a different colored bar for\neach level of sex.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar()\n\n\n\n\nfill() has also produced a plot legend to the right of\nthe graph. When you have multiple grouping variables you need this to\nknow which groups each bit of the plot is referring to, but in this case\nit is redundant because it doesn’t tell us anything that the axis labels\ndon’t already. We can get rid of it by adding\nshow.legend = FALSE to the geom_bar()\ncode.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE)\n\n\n\n\nWe might want to tidy up our plot to make it look a bit nicer. First\nwe can edit the axis labels to be more informative. The most common\nfunctions you will use are:\nscale_x_continuous() for adjusting the x-axis for a\ncontinuous variable\nscale_y_continuous() for adjusting the y-axis for a\ncontinuous variable\nscale_x_discrete() for adjusting the x-axis for a\ndiscrete/categorical variable\nscale_y_discrete() for adjusting the y-axis for a\ndiscrete/categorical variable\nAnd in those functions the two most common arguments you will use\nare:\nname which controls the name of each axis\nlabels which controls the names of the break points\non the axis\nThere are lots more ways you can customise your axes but we’ll stick\nwith these for now. Copy, paste, and run the below code to change the\naxis labels and change the numeric sex codes into words.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\",\n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\")\n\n\n\n\nSecond, you might want to adjust the colors and the visual style of\nthe plot. ggplot2 comes with built in themes. Below, we’ll\nuse theme_minimal() but try typing theme_ into\na code chunk and try all the options that come up to see which one you\nlike best.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal()\n\n\n\n\nThere are various options to adjust the colors but a good way to be\ninclusive is to use a color-blind friendly palette that can\nalso be read if printed in black-and-white. To do this, we can add on\nthe function scale_fill_viridis_d(). This function has 5\ncolour options, A, B, C, D, and E. I prefer E but you can play around\nwith them and choose the one you prefer.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")\n\n\n\n\nFinally, you can also adjust the transparency of the bars by adding\nalpha to geom_bar(). Play around with the\nvalue and see what value you prefer.\n\n\nggplot(summarydata, aes(x = sex, fill = sex)) +\n  geom_bar(show.legend = FALSE, alpha = .8) +\n  scale_x_discrete(name = \"Participant Sex\", \n                   labels = c(\"Female\", \"Male\")) +\n  scale_y_continuous(name = \"Number of participants\") +\n  theme_minimal() +\n  scale_fill_viridis_d(option = \"E\")\n\n\n\n\nShort note: In R terms, ggplot2 is a fairly old\npackage. As a result, the use of pipes wasn’t included when it was\noriginally written. As you can see in the code above, the layers of the\ncode are separated by + rather than %>%. In\nthis case, + is doing essentially the same job as a pipe -\nbe careful not to confuse them.\nWhen checking out other peoples´ work, you may however find different\napproaches using pipes along the way (but NOT inside the plotting\nitself):\n\n\nsummarydata %>%\n  ggplot(aes(x = sex, fill = sex)) +\n    geom_bar(show.legend = FALSE, alpha = .8) +\n    scale_x_discrete(name = \"Participant Sex\", \n                     labels = c(\"Female\", \"Male\")) +\n    scale_y_continuous(name = \"Number of participants\") +\n    theme_minimal() +\n    scale_fill_viridis_d(option = \"E\")\n\n\n\n\nMore complex example: Violin\nplots\nViolin plots are a nice way to visualize distributions, and\npotentially include different moments. I´ll later show you some nice\nways to actually tweak them by cutting them in half. But first things\nfirst.\nWe add a y argument to the first layer because we\nwanted to represent two variables, not just a count.\ngeom_violin() has an additional argument\ntrim. Try setting this to TRUE to see what\nhappens.\ngeom_boxplot() has an additional argument\nwidth. Try adjusting the value of this and see what\nhappens.\n\n\nggplot(summarydata, aes(x = income, y = ahiTotal, fill = income)) +\n  geom_violin(trim = FALSE, show.legend = FALSE, alpha = .4) +\n  geom_boxplot(width = .2, show.legend = FALSE, alpha = .7)+\n  geom_jitter() +\n  scale_x_discrete(name = \"Income\",\n                   labels = c(\"Below Average\", \"Average\", \"Above Average\")) +\n  scale_y_continuous(name = \"Authentic Happiness Inventory Score\")+\n  theme_minimal() +\n  scale_fill_viridis_d()\n\n\n\n\nActivity 6: Scatterplot\nWe will now work with our data to generate a scatterplot of\ntwo variables.\nFirst, we want to look at whether there seems to be a relationship\nbetween happiness and depression scores across all\nparticipants.\nIn order to visualize two continuous variables, we can use a\nscatterplot. Using the ggplot code you learned\nbefore, try and recreate the below plot.\nA few hints:\nUse the summarydata data.\nPut ahiTotal on the x-axis and cesdTotal on the y-axis.\nRather than using geom_bar(), geom_violin(), or geom_boxplot(),\nfor a scatterplot you need to use geom_point().\nRather than using scale_fill_viridis_d() to change the colour,\nadd the argument colour = “red” to geom_point (except\nreplace “red” with whatever colour you’d prefer).\nRemember to edit the axis names.\n\n\n\nAdding a line for best fit\nScatterplots are very useful but it can often help to add a line of\nbest fit to help interpretation. Add the below layer to your scatterplot\ncode:\nThis code uses the function geom_smooth() to draw\nthe line. There are several different methods but we want a straight, or\nlinear, line so we specify method = \"lm\".\nThis line is really a regression line, which you’ll learn more\nabout later.\nBy default the regression line will be extended, beyond the\noriginal y-axis limits, if you want to change this so that your plots\nlooks like the below, add limits = c(0,60) to\nscale_y_continuous()\n\n\ngeom_smooth(method = \"lm\")\n\n\n\n\n\n\nIt seems fairly obvious that there might be a negative relationship\nbetween happiness and depression, so instead we want to look at whether\nthis relationship changes depending on different demographic\nvariables.\nGrouped scatterplots\nWe can now use our factor variables (e.g., sex) to\ndisplay the data in the scatterplots for each group.\nRather than adding colour to\ngeom_point() which sets the colour for all the\ndata points, instead we add colour = sex to the\naesthetic mapping on the first line. This tells\nggplot() to produce different colours for each\nlevel (or group) in the variable sex.\nscale_color_viridis_d() works exactly like the other\ncolour blind friendly scale functions you have used, so we can use name\nand labels to adjust the legend.\n\n\nggplot(summarydata, aes(x = ahiTotal , y = cesdTotal, colour = sex)) + \n  geom_point() +\n  scale_x_continuous(name = \"Happiness Score\") +\n  scale_y_continuous(name = \"Depression Score\",\n                     limits = c(0,60)) +\n  theme_minimal() +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_d(name = \"Participant sex\", \n                       labels = c(\"Male\", \"Female\"),\n                       option = \"E\")\n\n\n\n\nIt looks like the relationship between happiness and depression is\nabout the same for male and female participants.\nCreate another scatterplot that shows the relationship between\nhappiness and depression grouped by educ. Make sure you\nupdate the legend labels.\nActivity 7: Group by a new variable\nSo, let’s be honest, there’s not much going on with any of the\ndemographic variables - the relationship between depression and anxiety\nis pretty much the same for all of the groups. A reasonable hypothesis\nmight be that rather than being connected to any demographic variables,\nthe relationship between happiness and depression changes depending\nupon your general happiness level.\nUsing mutate, create a new variable named\nhappiness in summarydata that evaluates\nwhether a participant’s happiness score is equal to or higher than the\nmedian ahiTotal score.\nThis is not an easy task as it’s not something I’ve explicitly\nshown you how to do but, do a bit of trial and error and you´ll do\nit.\nIf you’ve done it right, summarydata should contain a column\nnamed happiness with the value TRUE if\nahiTotal is above the overall median and FALSE\nif it is below.\n\n\n\nDeveloping plots (and\nquestions)\nA big advantage of ggplot2 over Base R plotting functions is the\nrelative ease you can developing your plotting approach with. It´s\nfairly easy to add or change smaller details or provide additional depth\nof analysis within just a few lines of code.\nTo show how such a “process” might look, we´ll start with a very\nsimplistic view of the world: The western world (Western Europe and\nNorth America) is characterized by long life spans and small families,\nopposed to the “developing” world (Africa, Asia, and Latin America)\nwhich is characterized by short life spans and large families. But do\nthe data support this dichotomous view? (And, should we talk about\nAustralia, maybe?)\nThe necessary data to answer this question is also available in the\ngapminder package. Using our newly learned data\nvisualization skills, we will be able to tackle this challenge.\nIn order to analyze this view, our first plot is a scatterplot of\nlife expectancy versus fertility rates (average number of children per\nwoman). We start by looking at data from about 60 years ago, when\nperhaps this view was first cemented in our minds.\n\n\ngapminder <- read_csv(\"datasets/gapminder_extended.csv\")\n\ngapminder %>%\n  filter(year == 1962) %>%\n  ggplot(aes(fertility, life_expectancy)) +\n  geom_point()\n\n\n\n\nMost points fall into two rather distinct categories:\nLife expectancy around 70 years and 3 or fewer children per\nfamily.\nLife expectancy lower than 65 years and more than 5 children per\nfamily.\nTo confirm that indeed these countries are from the regions the\nproposed world view predicts, we can use color to represent\ncontinent.\n\n\ngapminder %>%\n  filter(year == 1962) %>%\n  ggplot( aes(fertility, life_expectancy, color = continent)) +\n  geom_point() \n\n\n\n\nIn 1962, “the West / developing world” might have been grounded in\nsome reality. How does the case look about 50 years later? (This is due\nto missing data for the last decade, of course.)\nFaceting\nWe could easily plot the 2012 data in the same way we did for 1962.\nTo make comparisons, however, side by side plots would be neat. In\nggplot2, we can achieve this by faceting variables: we stratify the data\nby some variable and make the same plot for each.\nTo achieve faceting, we add a layer with the function\nfacet_grid, which automatically separates the plots. This\nfunction lets you facet by up to two variables using columns to\nrepresent one variable and rows to represent the other. The function\nexpects the row and column variables to be\nseparated by a ~. Here is an example of a scatterplot with\nfacet_grid added as the last layer:\n\n\ngapminder %>%\n  filter(year %in% c(1962, 2012)) %>%\n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point() +\n  facet_grid(continent ~ year)\n\n\n\n\nWe see a plot for each continent/year pair. However, this is just an\nexample and more than what we want, which is simply to compare 1962 and\n2012. In this case, there is just one variable and we use .\nto let facet know that we are not using one of the\nvariables:\n\n\ngapminder %>%\n  filter(year %in% c(1962, 2012)) %>%\n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point() +\n  facet_grid(. ~ year)\n\n\n\n\nThis plot hints that the majority of countries have moved towards the\nhigher life expectancy / lower fertility cluster. In 2012, the western /\ndeveloping world view no longer makes sense - if it ever made. This is\nparticularly clear when comparing Europe to Asia, the latter of which\nincludes several countries that have changed dramatically.\nfacet_wrap\nTo explore how this transformation happened through the years, we can\nmake the plot for several years. For example, we can add 1970, 1980,\n1990, and 2000. If we do this, we will not want all the plots on the\nsame row, the default behavior of facet_grid, since they\nwill become too thin to show the data. Instead, we will want to use\nmultiple rows and columns. facet_wrap permits us to do this\nby automatically wrapping the series of plots so that each display has\nviewable dimensions:\n\n\nyears <- c(1962, 1980, 1990, 2000, 2012)\ncontinents <- c(\"Europe\", \"Asia\")\n\ngapminder %>% \n  filter(year %in% years & continent %in% continents) %>%\n  ggplot( aes(fertility, life_expectancy, color = continent)) +\n  geom_point() +\n  facet_wrap(. ~year) \n\n\n\n\nThis plot clearly shows how most Asian countries have improved at a\nmuch faster rate than European ones.\nFixed scales for better\ncomparisons\nThe default choice of the range of the axes is important. When not\nusing facet, this range is determined by the data shown in\nthe plot. When using facet, this range is determined by the\ndata shown in all plots and therefore kept fixed across plots.\nThis makes comparisons across plots much easier. For example, in the\nabove plot, we can see that life expectancy has increased and the\nfertility has decreased across most countries. We see this because the\ncloud of points moves. This is not the case if we adjust the scales:\n\n\ngapminder %>%\n  filter(year %in% c(1962, 2012)) %>%\n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point() +\n  facet_wrap(. ~ year, scales = \"free\")\n\n\n\n\nIn the plot above, we have to pay special attention to the range to\nnotice that the plot on the right has a larger life expectancy.\nAfter fixing the scales again, we should now provide some finishing\ntouches:\n\n\ngapminder %>% \n  filter(year %in% years & continent %in% continents) %>%\n  ggplot(aes(fertility, life_expectancy, color = continent)) +\n  geom_point() +\n  scale_x_continuous(name = \"Fertility\") +\n  scale_y_continuous(name = \"Life Expectancy\") +\n  facet_wrap(. ~ year) +\n  theme_minimal() +\n  scale_color_viridis_d(option = \"E\",\n                        name = \"Continent\") +\n  labs(title = \"Life Expectancy and Fertility since 1962\")\n\n\n\n\nMore plotting options\nggplot2 provides us with a host of different plotting\noptions for different types of data. I´ll provide a brief overview of\nthe most common ones under the Types / overview-tab\nsoonish. For now, you should definitely check out this awesome website.\n\n\n\n",
      "last_modified": "2022-06-28T10:19:09+02:00"
    },
    {
      "path": "viz_tables.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:11+02:00"
    },
    {
      "path": "viz_types.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:12+02:00"
    },
    {
      "path": "viz.html",
      "title": "Visualisation",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:14+02:00"
    },
    {
      "path": "wrangling_datasets.html",
      "title": "Wrangling",
      "description": "Datasets\n",
      "author": [],
      "contents": "\n\nContents\n\n\nDataset 1 & 2: Gapminder\n& Babynames\nFor most data visualization purposes, we´ll use the\ngampinder and babynames datasets that come\nwith the gapminder and babynames packages.\n\n\ninstall.packages(\"gapminder\")\nlibrary(gapminder)\n\ninstall.packages(\"babynames\")\nlibrary(babynames)\n\n\n\nNote: The gapminder package exists for the\npurpose of teaching and making code examples. It is an excerpt of data\nfound in specific spreadsheets on Gapminder.org circa 2010. It is not a\ndefinitive source of socioeconomic data and is not updated regularly.\nHowever, its easy implementation and clean structure make it good\nservant to demonstrate data wrangling and visualization techniques.\nFor more info on the gapminder data visit the website.\nFor more info on the babynames data visit GitHub.\nDataset 3: Surgery Timing\nThis open data set contains 32,001 elective general surgical\npatients. Age, gender, race, BMI, several comorbidities, several\nsurgical risk indices, the surgical timing predictors (hour, day of\nweek, month,moon phase) and the outcomes (30-day mortality and\nin-hosptial complication) are provided. The dataset is cleaned and\ncomplete (no missing data except for BMI). There are no outliers or data\nproblems. These are data from a study by Sessier et al. “Operation\nTiming and 30-Day Mortality After Elective General Surgery”. Anesth\nAnalg 2011; 113: 1423-8\nTo further explore this dataset just follow these links:\nGeneral\ninformation and study overview\nData\ndictionary\nDataset 4: Positive\nPsychology Intervention\nThese two open datasets have been collected for replication purposes\n(so yes, this is a real study). The authors collected from 295\nparticipants between 5 January 2012 and 22 August 2012 to replicate the\neffects of a positive psychology intervention. The main finding was\ncontrary to that of the original study by Seligman et al.: All\ninterventions, including the theoretically-neutral placebo, led to\nsignificant increases in happiness and to significant reductions in\ndepression. The effects of the positive-psychology interventions were\nstatistically indistinguishable from those of the placebo.\nThe first dataset comprises 992 point-in-time records of\nself-reported happiness and depression in 295 participants, each\nassigned to one of four intervention groups, in a study of the effect of\nweb-based positive-psychology interventions. Each point-in-time\nmeasurement consists of a participant’s responses to the 24 items of the\nAuthentic Happiness Inventory and to the 20 items of the Center for\nEpidemiological Studies Depression (CES-D) scale. Measurements were\nsought at the time of each participant’s enrollment in the study and on\nfive subsequent occasions, the last being approximately 189 days after\nenrolment. The second dataset contains basic demographic\ninformation about each participant.\nWoodworth,\nR.J., O’Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from,\n‘Web-based Positive Psychology Interventions: A Reexamination of\nEffectiveness’. Journal of Open Psychology Data, 6(1).\n\n\n\n",
      "last_modified": "2022-06-28T10:19:15+02:00"
    },
    {
      "path": "wrangling_explore.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:19+02:00"
    },
    {
      "path": "wrangling_export.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:20+02:00"
    },
    {
      "path": "wrangling_import.html",
      "title": "Wrangling",
      "description": "Importing data\n",
      "author": [],
      "contents": "\n\nContents\n\n\n\n\n\nSpreadsheets\nSpreadsheets are an incredibly common format in which data are\nstored. If you’ve ever worked in Microsoft Excel or Google Sheets,\nyou’ve worked with spreadsheets. By definition, spreadsheets require\nthat information be stored in a grid utilizing rows and columns.\nExcel files\nMicrosoft Excel files, which typically have the file extension .xls\nor .xlsx, store information in a workbook. Each workbook is made up of\none or more spreadsheet. Within these spreadsheets, information is\nstored in the format of values and formatting (colors, conditional\nformatting, font size, etc.). While this may be a format you’ve worked\nwith before and are familiar, we note that Excel files can only be\nviewed in specific pieces of software (like Microsoft Excel), and thus\nare generally less flexible than many of the other formats we’ll discuss\nin this course. Additionally, Excel has certain defaults that make\nworking with Excel data difficult outside of Excel. For example, Excel\nhas a habit of aggressively changing data types. For example if you type\n1/2, to mean 0.5 or one-half, Excel assumes that this is a date and\nconverts this information to January 2nd. If you are unfamiliar with\nthese defaults, your spreadsheet can sometimes store information other\nthan what you or whoever entered the data into the Excel spreadsheet may\nhave intended. Thus, it’s important to understand the quirks of how\nExcel handles data. Nevertheless, many people do save their data in\nExcel, so it’s important to know how to work with them in R.\nReading Excel files into R\nReading spreadsheets from Excel into R is made possible thanks to the\nreadxl package. This should be installed with the\ntidyverse package, but, rather surprisingly, you´ll have to\nload it separately:\n\n\nlibrary(readxl)\n\n\n\nThe function read_excel() is particularly helpful\nwhenever you want read an Excel file into your R Environment. The only\nrequired argument of this function is the path to the Excel file on your\ncomputer. In the following example, read_excel() would look\nfor the file filename.xlsx in your current working\ndirectory (in our case, the one of your R project). If the file were\nlocated somewhere else on your computer, you would have to provide the\npath to that file.\n\n\ndf_excel <- read_excel(\"filename.xlsx\")\n\n\n\nWithin the readxl package there are a number of example\ndatasets that we can use to demonstrate the packages functionality. To\nread the example dataset in, we’ll use the readxl_example()\nfunction.\n\n\nexample <- readxl_example(\"datasets.xlsx\") # read example dataset\n\ndf <- read_excel(example) # read into tibble from source\ndf\n\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\nNote that the information stored in df is a tibble.\nThat´s good, we like that.\nFurther, by default, read_excel() converts blank cells\nto missing data (NA). This behavior can be changed by specifying the\nna argument within this function. There are a number of\nadditional helpful arguments within this function. They can all be seen\nusing ?read_excel, but we’ll highlight a few here:\nsheet - argument specifies the name of the sheet\nfrom the workbook you’d like to read in (string) or the integer of the\nsheet from the workbook.\ncol_names - specifies whether the first row of the\nspreadsheet should be used as column names (default: TRUE).\nAdditionally, if a character vector is passed, this will rename the\ncolumns explicitly at time of import.\nskip - specifies the number of rows to skip before\nreading information from the file into R. Often blank rows or\ninformation about the data are stored at the top of the spreadsheet that\nyou want R to ignore.\nFor example, we are able to change the column names directly by\npassing a character string to the col_names argument:\n\n\nread_excel(example, col_names = LETTERS[1:5])\n\n\n# A tibble: 151 × 5\n   A            B           C            D           E      \n   <chr>        <chr>       <chr>        <chr>       <chr>  \n 1 Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n 2 5.1          3.5         1.4          0.2         setosa \n 3 4.9          3           1.4          0.2         setosa \n 4 4.7          3.2         1.3          0.2         setosa \n 5 4.6          3.1         1.5          0.2         setosa \n 6 5            3.6         1.4          0.2         setosa \n 7 5.4          3.9         1.7          0.4         setosa \n 8 4.6          3.4         1.4          0.3         setosa \n 9 5            3.4         1.5          0.2         setosa \n10 4.4          2.9         1.4          0.2         setosa \n# … with 141 more rows\n\nTo take this a step further let’s discuss one of the lesser-known\narguments of the read_excel() function:\n.name_repair. This argument allows for further fine-tuning\nand handling of column names.\nThe default for this argument is\n.name_repair = \"unique\". This checks to make sure that each\ncolumn of the imported file has a unique name. If TRUE,\nreadxl leaves them as is, as you see in the example\nhere:\n\n\nread_excel(\n  readxl_example(\"deaths.xlsx\"),\n  range = \"A5:F8\",\n  .name_repair = \"unique\" # read example file into R using .name_repair default\n)\n\n\n# A tibble: 3 × 6\n  Name          Profession   Age `Has kids` `Date of birth`    \n  <chr>         <chr>      <dbl> <lgl>      <dttm>             \n1 David Bowie   musician      69 TRUE       1947-01-08 00:00:00\n2 Carrie Fisher actor         60 TRUE       1956-10-21 00:00:00\n3 Chuck Berry   musician      90 TRUE       1926-10-18 00:00:00\n# … with 1 more variable: `Date of death` <dttm>\n\nAnother option for this argument is\n.name_repair = \"universal\". This ensures that column names\ndon’t contain any forbidden characters or reserved words. It’s often a\ngood idea to use this option if you plan to use these data with other\npackages downstream. This ensures that all the column names will work,\nregardless of the R package being used.\n\n\nread_excel(\n  readxl_example(\"deaths.xlsx\"),\n  range = \"A5:F8\",\n  .name_repair = \"universal\" # require use of universal naming conventions\n)\n\n\n# A tibble: 3 × 6\n  Name          Profession   Age Has.kids Date.of.birth      \n  <chr>         <chr>      <dbl> <lgl>    <dttm>             \n1 David Bowie   musician      69 TRUE     1947-01-08 00:00:00\n2 Carrie Fisher actor         60 TRUE     1956-10-21 00:00:00\n3 Chuck Berry   musician      90 TRUE     1926-10-18 00:00:00\n# … with 1 more variable: Date.of.death <dttm>\n\nNote that when using .name_repair = \"universal\", you’ll\nget a readout about which column names have been changed. Here you see\nthat column names with a space in them have been changed to periods for\nword separation.\nAside from these options, functions can be passed to\n.name_repair. For example, if you want all of your column\nnames to be lowercase, you would use the following:\n\n\nread_excel(\n  readxl_example(\"deaths.xlsx\"),\n  range = \"arts!A5:F8\",\n  .name_repair = tolower # pass function for column naming\n)\n\n\n# A tibble: 3 × 6\n  name          profession   age `has kids` `date of birth`    \n  <chr>         <chr>      <dbl> <lgl>      <dttm>             \n1 David Bowie   musician      69 TRUE       1947-01-08 00:00:00\n2 Carrie Fisher actor         60 TRUE       1956-10-21 00:00:00\n3 Chuck Berry   musician      90 TRUE       1926-10-18 00:00:00\n# … with 1 more variable: `date of death` <dttm>\n\nNotice that the function is passed directly to the argument. It does\nnot have quotes around it, as we want this to be interpreted as the\ntolower() function.\nHere we’ve really only focused on a single function\n(read_excel()) from the readxl package. This\nis because some of the best packages do a single thing and do that\nsingle thing well. The readxl package has a single, slick\nfunction that covers most of what you’ll need when reading in files from\nExcel. That is not to say that the package doesn’t have other useful\nfunctions (it does!), but this function will cover your needs most of\nthe time.\nGoogle Sheets\nSimilar to Microsoft Excel, Google Sheets is another place in which\nspreadsheet information is stored. Google Sheets also stores information\nin spreadsheets within workbooks. Like Excel, it allows for cell\nformatting and has defaults during data entry that could get you into\ntrouble if you’re not familiar with the program.\nUnlike Excel files, however, Google Sheets live on the Internet,\nrather than your computer. This makes sharing and updating Google Sheets\namong people working on the same project much quicker. This also makes\nthe process for reading them into R slightly different. Accordingly, it\nrequires the use of a different, but also very helpful package,\ngooglesheets4!\nAs Google Sheets are stored on the Internet and not on your computer,\nthe googlesheets4 package does not require you to download\nthe file to your computer before reading it into R. Instead, it reads\nthe data into R directly from Google Sheets. Note that if the data\nhosted on Google Sheets changes, every time the file is read into R, the\nmost updated version of the file will be utilized. This can be very\nhelpful if you’re collecting data over time; however, it could lead to\nunexpected changes in results if you’re not aware that the data in the\nGoogle Sheet is changing.\nTo see exactly what we mean, let’s look at a specific example.\nImagine you’ve sent out a survey to your friends asking about how they\nspend their day. Let’s say you’re mostly interested in knowing the hours\nspent on work, leisure, sleep, eating, socializing, and other\nactivities. So in your Google Sheet you add these six items as columns\nand one column asking for your friends names. To collect this data, you\nthen share the link with your friends, giving them the ability to edit\nthe Google Sheet.\n\nYour friends will then one-by-one complete the survey. And, because\nit’s a Google Sheet, everyone will be able to update the Google Sheet,\nregardless of whether or not someone else is also looking at the Sheet\nat the same time. As they do, you’ll be able to pull the data and import\nit to R for analysis at any point. You won’t have to wait for everyone\nto respond. You’ll be able to analyze the results in real-time by\ndirectly reading it into R from Google Sheets, avoiding the need to\ndownload it each time you do so.\nIn other words, every time you import the data from the Google Sheets\nlink using the googlesheets4 package, the most updated data will be\nimported. Let’s say, after waiting for a week, your friends’ data look\nsomething like this:\n\nYou’d be able to analyze these updated data using R and the\ngooglesheets4 package!\nThe googlesheets4\npackage\nThe googlesheets4 package allows R users to take\nadvantage of the Google Sheets Application Programming Interface (API).\nVery generally, APIs allow different applications to communicate with\none another. In this case, Google has released an API that allows other\nsoftware to communicate with Google Sheets and retrieve data and\ninformation directly from Google Sheets. The googlesheets4\npackage enables R users to easily access the Google Sheets API and\nretrieve your Google Sheets data.\nIn addition to the ability of pulling data, you can also edit a\nGoogle Sheet or create new sheets.\nThe googlesheets4 package is tidyverse-adjacent, so it\nrequires its own installation. It also requires that you load it into R\nbefore it can be used.\n\n\ninstall.packages(\"googlesheets4\")\n\nlibrary(googlesheets4)\n\n\n\nNow, let’s get to importing your survey data into R. Every time you\nstart a new session, you need to authenticate the use of the\ngooglesheets4 package with your Google account. This is a\ngreat feature as it ensures that you want to allow access to your Google\nSheets and allows the Google Sheets API to make sure that you should\nhave access to the files you’re going to try to access.\nThe command gs4_auth() will open a new page in your\nbrowser that asks you which Google account you’d like to have access to.\nClick on the appropriate Google user to provide\ngooglesheets4 access to the Google Sheets API.\n\nAfter you click “ALLOW,” giving permission for the\ngooglesheets4 package to connect to your Google account,\nyou will likely be shown a screen where you will be asked to copy an\nauthentication code. Copy this authentication code and paste it into\nR.\n\nOnce authenticated, you can use the command gs4_find()\nto list all your worksheets on Google Sheets as a table. Note that this\nwill ask for authorization of the googledrive package. We\nwill discuss more about googledrive later (after I´ve finished writing\nthat page…).\n\nIn order to ultimately access the information a specific Google\nSheet, you can use the read_sheets() function by typing in the id listed\nfor your Google Sheet of interest when using\ngs4_find().\n\n\nread_sheet(\"2cdw-678dSPLfdID__LIt8eEFZPasdebgIGwHk\") # note this is not a real fake id. surprise there.\n\n\n\nYou can also navigate to your own sheets or to other people’s sheets\nusing a URL. We will now read one into R using the URL:\n\n\nsurvey_sheet <- read_sheet(\"https://docs.google.com/spreadsheets/d/1FN7VVKzJJyifZFY5POdz_LalGTBYaC4SLB-X9vyDnbY/\")\n\n\n\nNote that we assign the information stored in this Google Sheet to\nthe object survey_sheet so that we can use it again\nshortly.\n\nNote that by default the data on the first sheet will be read into R.\nIf you wanted the data on a particular sheet you could specify with the\nsheet argument, like so:\n\n\nsurvey_sheet <- read_sheet(\"https://docs.google.com/spreadsheets/d/1FN7VVKzJJyifZFY5POdz_LalGTBYaC4SLB-X9vyDnbY/\", sheet = 2)\n\n\n\nIf the sheet was named something in particular you would use this\ninstead of the number 2.\nThere are other additional (optional) arguments to\nread_sheet(), some are similar to those in\nread_csv() and read_excel(), while others are\nmore specific to reading in Google Sheets:\nskip = 1: will skip the first row of the Google Sheet\ncol_names = FALSE`: specifies that the first row is not column\nnames\nrange = “A1:G5”: specifies the range of cells that we like to\nimport is A1 to G5\nn_max = 100: specifies the maximum number of rows that we want to\nimport is 100\nIn summary, to read in data from a Google Sheet in\ngooglesheets4, you must first know the id, the name or the\nURL of the Google Sheet and have access to it.\nSee this\nwebsite for a list of additional functions in the\ngooglesheets4 package.\nCSVs\nLike Excel Spreadsheets and Google Sheets, Comma-separated values\n(CSV) files allow us to store tabular data; however, it does this in a\nmuch simple format. CSVs are plain-text files, which means that all the\nimportant information in the file is represented by text (where text is\nnumbers, letters, and symbols you can type on your keyboard). This means\nthat there are no workbooks or metadata making it difficult to open\nthese files. CSVs are flexible files and are thus the preferred storage\nmethod for tabular data for many data scientists.\nFor example, consider a dataset that includes information about the\nheights and blood types of three individuals. You could make a table\nthat has three columns (names, heights, and blood types) and three rows\n(one for each person) in Google Docs or Microsoft Word. However, there\nis a better way of storing this data in plain text without needing to\nput them in table format. CSVs are a perfect way to store these data. In\nthe CSV format, the values of each column for each person in the data\nare separated by commas and each row (each person in our case) is\nseparated by a new line. This means your data would be stored in the\nfollowing format:\n\nNotice that CSV files have a .csv extension at the end. You can see\nthis above at the top of the file. One of the advantages of CSV files is\ntheir simplicity. Because of this, they are one of the most common file\nformats used to store tabular data. Additionally, because they are plain\ntext, they are compatible with many different types of software. CSVs\ncan be read by most programs. Specifically, for our purposes, these\nfiles can be easily read into R (or Google Sheets, or Excel), where they\ncan be better understood by the human eye.\nAs with any file type, CSVs do have their limitations. Specifically,\nCSV files are best used for data that have a consistent number of\nvariables across observations. In our example, there are three variables\nfor each observation: “name,” “height,” and “blood_type.” If, however,\nyou had eye color and weight for the second observation, but not for the\nother rows, you’d have a different number of variables for the second\nobservation than the other two. This type of data is not best suited for\nCSVs (although NA values could be used to make the data rectangular).\nWhenever you have information with the same number of variables across\nall observations, CSVs are a good bet!\nReading CSVs into R\nNow that you have a CSV file, let’s discuss how to get it into R! The\nbest way to accomplish this is using the function\nread_csv() from the readr package. (Note, if you haven’t\ninstalled the readr package, you’ll have to do that first.)\nInside the parentheses of the function, write the name of the file in\nquotes, including the file extension (.csv). Make sure you type the\nexact file name. Save the imported data in a data frame called\ndf_csv. Your data will now be imported into R\nenvironment.\n\n\nlibrary(readr)\n\ndf_csv <- read_csv(\"example.csv\") ## read CSV into R\n\n\n\nAbove, you see the simplest way to import a CSV file. However, as\nwith many functions, there are other arguments that you can set to\nspecify how to import your specific CSV file, a few of which are listed\nbelow. However, as usual, to see all the arguments for this function,\nuse ?read_csv within R.\ncol_names = FALSE to specify that the first row does\nNOT contain column names.\nskip = 2 will skip the first 2 rows. You can set the\nnumber to any number you want. This is helpful if there is additional\ninformation in the first few rows of your data frame that are not\nactually part of the table.\nn_max = 100 will only read in the first 100 rows.\nYou can set the number to any number you want. This is helpful if you’re\nnot sure how big a file is and just want to see part of it.\nBy default, read_csv() converts blank cells to missing\ndata (NA).\nWe have introduced the function read_csv here and\nrecommend that you use it, as it is the simplest and fastest way to read\nCSV files into R. However, we note that there is a function\nread.csv() which is available by default in R. You will\nlikely see this function in others’ code, so we just want to make sure\nyou’re aware of it.\nOther file types\nR can read in data files in many different formats. The\nhaven package is part of the tidyverse. It\nprovides functions to read files created in a variety of commercial\nsoftware packages. R can also talk directly to databases. If your\ndataset is a Stata .dta file, for instance, you can use the\nread_dta() function in much the same way as we used\nread_csv() above.\nBeyond that, R has lots of other importing capabilities - I´ll cover\nsome of those here soonish, too. You could, for example, import TSVs,\ndifferent delimited files, scrape data from web or social media\nplatforms.\n\n\n\n",
      "last_modified": "2022-06-28T10:19:25+02:00"
    },
    {
      "path": "wrangling_imputation.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:27+02:00"
    },
    {
      "path": "wrangling_manipulate.html",
      "title": "Wrangling",
      "description": "Manipulating data\n",
      "author": [],
      "contents": "\n\nContents\n\n\nData manipulation and\npreparation\nData comes in lots of different formats. As we´ve discussed before,\none of the most common formats is that of a two-dimensional table (the\ntwo dimensions being rows and columns). Usually, each row stands for a\nseparate observation (e.g. a subject), and each column stands for a\ndifferent variable (e.g. a response, category, or group). A key benefit\nof tabular data is that it allows you to store different types of\ndata-numerical measurements, alphanumeric labels, categorical\ndescriptors - all in one place.\nWe (or is that just me?) actually spend far more time cleaning and\npreparing data than I spend actually analyzing it. This means completing\ntasks such as cleaning up bad values, changing the structure of tables,\nmerging information stored in separate tables, reducing the data down to\na subset of observations, and producing data summaries. Some have\nestimated that up to 80% of time spent on data analysis involves such\ndata preparation tasks (Dasu & Johnson, 2003)!\nIn the (dark dark) past, the only option for data cleaning was the\npainstaking and time-consuming cutting and pasting of data within a\nspreadsheet program like Excel. We have witnessed students and\ncolleagues waste days, weeks, and even months manually transforming\ntheir data in Excel, cutting, copying, and pasting data. Fixing up your\ndata by hand is not only a terrible use of your time (ok, it is), but it\nis error-prone and not reproducible. Additionally, in this age where we\ncan easily collect massive datasets online, you will not be able to\norganize, clean, and prepare these by hand.\nIn short, you will not thrive if you do not learn some key data\nwrangling skills. Although every dataset presents unique challenges,\nthere are some systematic principles you should follow that will make\nyour analyses easier, less error-prone, more efficient, and more\nreproducible.\nWe will thus explore how data science skills will allow you to\nefficiently get answers to nearly any question you might want to ask\nabout your data.\nTidyverse\nTidyverse (https://www.tidyverse.org/) is a collection of R\npackages created by world-famous data scientist Hadley Wickham.\nTidyverse contains six core packages: dplyr, tidyr, readr, purrr,\nggplot2, and tibble.When you typed library(tidyverse) into\nR, you will have seen that it loads in all of these packages in one go.\nWithin these six core packages, you should be able to find everything\nyou need to wrangle and visualize your data.\nTo to all its magic, the tidyverse proposes a working model /\nworkflow with different packages providing functions for different tasks\nalong the way:\n\nFor simplicity and efficiency, we are going to focus on the dplyr\npackage, which contains six important functions:\nselect() Include or exclude certain variables\n(columns)\nfilter() Include or exclude certain observations\n(rows)\nmutate() Create new variables (columns)\narrange() Change the order of observations\n(rows)\ngroup_by() Organize the observations into\ngroups\nsummarise() Derive aggregate variables for groups of\nobservations\nThese six functions are known as ’single table verbs’ because they\nonly operate on one table at a time. Although the operations of these\nfunctions may seem very simplistic, it’s amazing what you can accomplish\nwhen you string them together: Hadley Wickham has claimed that 90% of\ndata analysis can be reduced to the operations described by these six\nfunctions.\nAgain, I don’t expect you to remember everything here - the important\nthing is that you know where to come and look for help when you need to\ndo particular tasks. Being good at coding really is just being good at\nknowing what to copy and paste.\nThe babynames database\nTo demonstrate the power of the six dplyr verbs, we will\nuse them to work with the babynames data from the\nbabynames package. The babynames dataset has historical\ninformation about births of babies in the U.S.\n\n\nlibrary(tidyverse)\nlibrary(babynames)\n\n\n\nHave a first glance\nThe package babynames contains an object of the same\nname that contains all the data about babynames.\n\n\nhead(babynames)\n\n\n# A tibble: 6 × 5\n   year sex   name          n   prop\n  <dbl> <chr> <chr>     <int>  <dbl>\n1  1880 F     Mary       7065 0.0724\n2  1880 F     Anna       2604 0.0267\n3  1880 F     Emma       2003 0.0205\n4  1880 F     Elizabeth  1939 0.0199\n5  1880 F     Minnie     1746 0.0179\n6  1880 F     Margaret   1578 0.0162\n\nEach row in the table represents data about births for a given name\nand sex in a given year. The variables are:\nvariable\ntype\ndescription\nyear\ndouble (numeric)\nyear of birth\nsex\ncharacter\nrecorded sex of baby (F = female, M = male)\nname\ncharacter\nforename given to baby\nn\ninteger\nnumber of babies given that name\nprop\ndouble (numeric)\nproportion of all babies of that sex\nThe first row of the table tells us that in the year 1880, there were\n7065 baby girls born in the U.S. who were given the name Mary, and this\naccounted for about 7% of all baby girls.\nSelecting variables of\ninterest\nThere are two numeric measurements of name popularity,\nprop (the proportion of all babies with each name) is\nprobably more useful than n (total number of babies with\nthat name), because it takes into account that different numbers of\nbabies are born in different years.\nIf we wanted to create a dataset that only includes certain\nvariables, we can use the select() function from the\ndplyr package. Run the below code to only select the\ncolumns year, sex, name and prop.\n\n\nselect(.data = babynames,     # the object you want to select variables from\n       year, sex, name, prop) # the variables you want to select\n\n\n\nIf you get an error message when using select that says unused\nargument it means that it is trying to use the wrong version of the\nselect() function. There are two solutions to this, first,\nsave you work and then restart the R session (click session -restart R)\nand then run all your code above again from the start, or replace select\nwith dplyr::select() which tells R exactly which version of\nthe select function to use. I’d recommend restarting the session because\nthis will get you in the habit and it’s a useful thing to try for a\nrange of problems\nAlternatively, you can also tell R which variables you don’t want, in\nthis case, rather than telling R to select year, sex, name and prop, we\ncan simply tell it to drop the column n using the minus sign - before\nthe variable name.\n\n\nselect(.data = babynames, -n)\n\n\n\nNote that select() does not change the original tibble,\nbut makes a new tibble with the specified columns. If you don’t save\nthis new tibble to an object, it won’t be saved. If you\nwant to keep this new dataset, create a new object. When you run this\ncode, you will see your new tibble appear in the environment pane.\n\n\nnew_dat <- select(.data = babynames, -n)\n\n\n\nArranging the data\nThe function arrange() will sort the rows in the table\naccording to the columns you supply. Try running the following code:\n\n\narrange(.data = babynames, # the data you want to sort\n        name)              # the variable you want to sort by\n\n\n\nThe data are now sorted in ascending alphabetical order by name. The\ndefault is to sort in ascending order. If you want it descending, wrap\nthe name of the variable in the desc() function. For\ninstance, to sort by year in descending order, run the following\ncode:\n\n\narrange(babynames,desc(year)) \n\n\n\nYou can also sort by more than one column. What do you think the\nfollowing code will do?\n\n\narrange(babynames, desc(year), desc(sex), desc(prop)) \n\n\n\nUsing filter to select\nobservations\nWe have previously used select() to select certain\nvariables or columns, however, frequently you will also want to select\nonly certain observations or rows, for example, only babies born after\n1999, or only babies named “Mary”. You do this using the verb\nfilter(). The filter() function is a bit more\ninvolved than the other verbs, and requires more detailed explanation,\nbut this is because it is also extremely powerful.\nHere is an example of filter, can you guess what it will do?\n\n\nfilter(.data = babynames, year > 2000)\n\n\n\nThe first part of the code tells the function to use the object\nbabynames. The second argument,\nyear > 2000, is what is known as a Boolean\nexpression: an expression whose evaluation results in a value of\nTRUE or FALSE. What filter() does\nis include any observations (rows) for which the expression evaluates to\nTRUE, and exclude any for which it evaluates to\nFALSE. So in effect, behind the scenes,\nfilter() goes through the entire set of 1.8 million\nobservations, row by row, checking the value of year for each row,\nkeeping it if the value is greater than 2000, and rejecting it\nif it is less than 2000. To see how a boolean expression works, consider\nthe code below:\n\n\nyears <- 1996:2005\nyears\n\n\n [1] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005\n\nyears > 2000\n\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nYou can see that the expression years > 2000 returns\na logical vector (a vector of TRUE and\nFALSE values), where each element represents whether the\nexpression is true or false for that element. For the first five\nelements (1996 to 2000) it is false, and for the last five elements\n(2001 to 2005) it is true.\nHere are the most commonly used Boolean expressions.\nOperator\nName\nis TRUE if and only if\nA < B\nless than\nA is less than B\nA <= B\nless than or equal\nA is less than or equal to B\nA > B\ngreater than\nA is greater than B\nA >= B\ngreater than or equal\nA is greater than or equal to B\nA == B\nequivalence\nA exactly equals B\nA != B\nnot equal\nA does not exactly equal B\nA %in% B\nin\nA is an element of vector B\nIf you want only those observations for a specific name (e.g., Mary),\nyou use the equivalence operator ==. Note that you\nuse double equal signs, not a single equal sign.\n\n\nfilter(babynames, name == \"Mary\")\n\n\n\nIf you wanted all the names except Mary, you use the ‘not equals’\noperator:\n\n\nfilter(babynames, name!=\"Mary\") \n\n\n\nAnd if you wanted names from a defined set - e.g., names of British\nqueens - you can use %in%:\n\n\nfilter(babynames, name %in% c(\"Mary\",\"Elizabeth\",\"Victoria\"))\n\n\n\nThis gives you data for the names in the vector on the right hand\nside of %in%. You can always invert an expression to get\nits opposite. So, for instance, if you instead wanted to get rid of all\nMarys, Elizabeths, and Victorias you would use the following:\n\n\nfilter(babynames, !(name %in% c(\"Mary\",\"Elizabeth\",\"Victoria\")))\n\n\n\nYou can include as many expressions as you like as additional\narguments to filter() and it will only pull out the rows\nfor which all of the expressions for that row evaluate to\nTRUE. For instance,\n\n\nfilter(babynames, year > 2000, prop > .01) \n\n\n\nwill pull out only those observations beyond the year 2000\nthat represent greater than 1% of the names for a given sex;\nany observation where either expression is false will be excluded. This\nability to string together criteria makes filter() a very\npowerful member of the Wickham Six.\nRemember that this section exists. It will contain a lot of the\nanswers to problems you face when wrangling data!\nCreating new variables\nSometimes we need to create a new variable that doesn’t exist in our\ndataset. For instance, we might want to figure out what decade a\nparticular year belongs to. To create new variables, we use the function\nmutate(). Note that if you want to save this new column,\nyou need to save it to an object. Here, you are mutating a\nnew column and attaching it to the new_dat object you\ncreated before.\n\n\nnew_dat <- mutate(.data = babynames,           # the tibble you want to add a column to\n                  decade = floor(year/10) *10) # new column name = what you want it to contain\nhead(new_dat)\n\n\n# A tibble: 6 × 6\n   year sex   name          n   prop decade\n  <dbl> <chr> <chr>     <int>  <dbl>  <dbl>\n1  1880 F     Mary       7065 0.0724   1880\n2  1880 F     Anna       2604 0.0267   1880\n3  1880 F     Emma       2003 0.0205   1880\n4  1880 F     Elizabeth  1939 0.0199   1880\n5  1880 F     Minnie     1746 0.0179   1880\n6  1880 F     Margaret   1578 0.0162   1880\n\nIn this case, you are creating a new column decade which has the\ndecade each year appears in. This is calculated using the command\ndecade = floor(year/10)*10.\nGrouping and summarizing\nMost quantitative analyses will require you to summarize your data\nsomehow, for example, by calculating the mean,\nmedian or a sum total of your data. You can\nperform all of these operations using the function\nsummarise().\nFirst, let’s use a new object dat that just has the data\nfor the four girls names, Alexandra, Beverly, Emily, and Kathleen. To\nstart off, we’re simply going to calculate the total number of\nbabies across all years that were given one of these four\nnames.\n\n\ndat <- filter(babynames, name %in% c(\"Alexandra\",\"Beverly\",\"Emily\",\"Kathleen\"))\n\n\n\nIt’s useful to get in the habit of translating your code into full\nsentences to make it easier to figure out what’s happening. You can read\nthe below code as “run the function summarise using the\ndata in the object dat to create a new variable named\ntotal that is the result of adding up all the numbers in the column\nn”.\n\n\nsummarise(.data = dat,    # the data you want to use\n          total = sum(n)) # result = operation\n\n\n# A tibble: 1 × 1\n    total\n    <int>\n1 2170302\n\nsummarise() becomes even more powerful when combined\nwith the final dplyr function, group_by(). Quite often, you\nwill want to produce your summary statistics broken down by groups, for\nexamples, the scores of participants in different conditions, or the\nreading time for native and non-native speakers.\nThere are two ways you can use group_by(). First, you\ncan create a new, grouped object.\n\n\ngroup_dat <- group_by(.data = dat, # the data you want to group\n                      name)        # the variable you want to group by\n\n\n\nIf you look at this object in the viewer, it won’t look any different\nto the original dat, however, the underlying structure has\nchanged. Let’s run the above summarise code again, but now\nusing the grouped data.\n\n\nsummarise(.data = group_dat, \n          total = sum(n)) \n\n\n# A tibble: 4 × 2\n  name       total\n  <chr>      <int>\n1 Alexandra 232223\n2 Beverly   381547\n3 Emily     843235\n4 Kathleen  713297\n\nsummarise() has performed exactly the same operation as\nbefore - adding up the total number in the column n - but\nthis time it has done is separately for each group, which in\nthis case was the variable name.\nYou can request multiple summary calculations to be performed in the\nsame function. For example, the following code calculates the\nmean and median number of babies given each\nname every year.\n\n\nsummarise(group_dat,\n          mean_year = mean(n),\n          median_year = median(n))\n\n\n# A tibble: 4 × 3\n  name      mean_year median_year\n  <chr>         <dbl>       <dbl>\n1 Alexandra     1451.         65 \n2 Beverly       1688.        102.\n3 Emily         3978.        986.\n4 Kathleen      3397.        436 \n\nYou can also add multiple grouping variables. For example,\nthe following code groups new_dat by sex and\ndecade and then calculates the summary statistics to give\nus the mean and median number of male and\nfemale babies in each decade.\n\n\ngroup_new_dat <- group_by(new_dat, sex, decade)\nsummarise(group_new_dat,\n          mean_year = mean(n),\n          median_year = median(n))\n\n\n# A tibble: 28 × 4\n# Groups:   sex [2]\n   sex   decade mean_year median_year\n   <chr>  <dbl>     <dbl>       <dbl>\n 1 F       1880      111.          13\n 2 F       1890      128.          13\n 3 F       1900      131.          12\n 4 F       1910      187.          12\n 5 F       1920      211.          12\n 6 F       1930      214.          12\n 7 F       1940      262.          12\n 8 F       1950      288.          13\n 9 F       1960      235.          12\n10 F       1970      147.          11\n# … with 18 more rows\n\nsummarise() has grouped output by ‘sex’. You can\noverride using the .groups argument.\nIf you get what looks like an error that says summarise() ungrouping\noutput (override with .groups argument) don’t worry, this\nisn’t an error it’s just R telling you what it’s done.\nPipes\nThe final activity for this chapter essentially repeats what we’ve\nalready covered but in a slightly different way. In the previous\nactivities, you created new objects with new variables or groupings and\nthen you called summarise() on those new objects in\nseparate lines of code. As a result, you had multiple objects in your\nenvironment pane and you need to make sure that you keep track of the\ndifferent names. No fun, I promise.\nInstead, you can use pipes. Pipes are written as\n%>% and they should be read as “and then”. Pipes allow\nyou to string together ‘sentences’ of code into ‘paragraphs’ so that you\ndon’t need to create intermediary objects. Again, it is easier\nto show than tell.\nI´ll first try visually, and then via code.\nNo-pipe-approach\n ### Love-pipe-approach \nNot only do I really like these visuals, but I think they do a very\ngood job of depicting how the process and its logic change.\nNow, the below code does exactly the same as all the code we wrote\nabove but it only creates one object.\n\n\npipe_summary <- mutate(babynames, decade = floor(year/10) *10) %>%\n  filter(name %in% c(\"Emily\",\"Kathleen\",\"Alexandra\",\"Beverly\"), sex==\"F\") %>%\n  group_by(name, decade) %>%\n  summarise(mean_decade = mean(n))\n\n\n\nThe reason that this function is called a pipe is because it\n‘pipes’ the data through to the next function. When you wrote the code\npreviously, the first argument of each function was the dataset you\nwanted to work on. When you use pipes it will automatically take the\ndata from the previous line of code so you don’t need to specify it\nagain.\nSome people find pipes a bit tricky to understand from a conceptual\npoint of view, however, it’s well worth learning to use them as when\nyour code starts getting longer they are much more efficient and mean\nyou have to write less code which is always a good thing!\nCombining / joining datasets\nFor this short demo and to introduce a new function, we are going to\nbe using a real dataset. Click the below link to see some\nadditional info.\nWoodworth,\nR.J., O’Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from,\n‘Web-based Positive Psychology Interventions: A Reexamination of\nEffectiveness’. Journal of Open Psychology Data, 6(1).\nReading in the data\nNow we can read in the data. To do this we will use the function\nread_csv().\nFirst, we will create an object called dat that contains\nthe data in the ahi-cesd.csv file. Then, we will create an\nobject called info that contains the data in the\nparticipant-info.csv.\n\n\nlibrary(readr)\n\ndat <- read_csv (\"datasets/positive_psychology/ahi-cesd.csv\")\npinfo <- read_csv(\"datasets/positive_psychology/participant-info.csv\")\n\n\n\nKeep in mind: There is also a function called\nread.csv(). Be very careful NOT to use this function\ninstead of read_csv() as they have different ways of naming\ncolumns.\nCheck your data\nYou should now see that the objects dat and\npinfo have appeared in the environment pane. Whenever you\nread data into R you should always do an initial check to see that your\ndata looks like you expected. There are several ways you can do this,\ntry them all out to see how the results differ.\nIn the environment pane, click on dat and\npinfo. This will open the data to give you a\nspreadsheet-like view (although you can’t edit it like in Excel) In the\nenvironment pane, click the small blue play button to the left of\ndat and pinfo. This will show you the\nstructure of the object information including the names of all the\nvariables in that object and what type they are (also see\nstr(pinfo)).\nUse summary(pinfo)\nUse head(pinfo)\nJust type the name of the object you want to view, e.g.,\ndat.\nJoin the files together\nWe have two files, dat and info but what we really want\nis a single file that has both the data and the demographic\ninformation about the participants. R makes this very easy by using\nthe function inner_join().\nRemember to use the help function ?inner_join if you\nwant more information about how to use a function and to use tab\nauto-complete to help you write your code.\nThe below code will create a new object all_dat that has\nthe data from both dat and pinfo and it will\nuse the columns id and intervention to match\nthe participants’ data. If you want to join tables that have multiple\ncolumns in common, you need to use c() to list them all (I\nthink of it as c for combine, or collection).\n\n\nall_dat <- inner_join(x = dat,                      # the first table you want to join\n                      y = pinfo,                    # the second table you want to join\n                      by = c(\"id\", \"intervention\")) # columns the two tables have in common\n\n\n\nPull out variables of\ninterest\nOur final step is to pull our variables of interest. Very frequently,\ndatasets will have more variables and data than you actually want to use\nand it can make life easier to create a new object with just the data\nyou need.\nIn this case, the file contains the responses to each individual\nquestion on both the AHI scale and the CESD scale as well as the total\nscore (i.e., the sum of all the individual responses). For our analysis,\nall we care about is the total scores, as well as the demographic\ninformation about participants.\nTo do this we use the select() function to create a new\nobject named summarydata.\n\n\nsummarydata <- select(.data = all_dat, # name of the object to take data from\n                      ahiTotal, cesdTotal, sex, age, educ, income, occasion,elapsed.days)\n\n\n\nIf you get an error message when using select that says\nunused argument it means that it is trying to use the wrong\nversion of the select function. There are two solutions to\nthis, first, save you work and then restart the R session (click session\n-restart R) and then run all your code above again from the\nstart, or replace select with dplyr::select which tells R\nexactly which version of the select function to use. We’d\nrecommend restarting the session because this will get you in the habit\nand it’s a useful thing to try for a range of problems\n\n\n\n",
      "last_modified": "2022-06-28T10:19:33+02:00"
    },
    {
      "path": "wrangling_simulation.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:35+02:00"
    },
    {
      "path": "wrangling.html",
      "title": "Soon",
      "description": "fostering R data skills for health science\n",
      "author": [],
      "contents": "\n\nContents\n\n\n… not yet there\nThis page is still under construction. May take some time…\nBER picture\n\n\n\n",
      "last_modified": "2022-06-28T10:19:36+02:00"
    }
  ],
  "collections": []
}
